<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
     <!-- 피드경로 명시하자-->
    <channel>
        <title> 차곡차곡 쌓자  </title>
        <description>차곡차곡 쌓아가는 기술블로그입니다. 되도록 정확한 자료만을 정리하려고 노력합니다.</description>
        <link>https://east-rain.github.io/</link>
        <atom:link href="https://east-rain.github.io/feed.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Thu, 29 Oct 2020 10:14:05 +0900</pubDate>
        <lastBuildDate>Thu, 29 Oct 2020 10:14:05 +0900</lastBuildDate>
        <generator>Jekyll v3.8.6</generator>
        
        <item>
            <title></title>
            <description>&lt;style type=&quot;text/css&quot; media=&quot;screen&quot;&gt;
  .container {
    margin: 10px auto;
    max-width: 600px;
    text-align: center;
  }
  h1 {
    margin: 30px 0;
    font-size: 4em;
    line-height: 1;
    letter-spacing: -1px;
  }
&lt;/style&gt;

&lt;div class=&quot;container&quot;&gt;
  &lt;h1&gt;404&lt;/h1&gt;

  &lt;p&gt;&lt;strong&gt;Page not found :(&lt;/strong&gt;&lt;/p&gt;
  &lt;p&gt;The requested page could not be found.&lt;/p&gt;
&lt;/div&gt;
</description>
            <pubDate>Mon, 20 Apr 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/404.html</link>
            <guid isPermaLink="true">https://east-rain.github.io/404.html</guid>
            
            
        </item>
        
        <item>
            <title>deepercut(base model)</title>
            <description>&lt;h1 id=&quot;deepercut-논문-요약&quot;&gt;Deepercut 논문 요약&lt;/h1&gt;
&lt;p&gt;Bottom-up 방식의 pose estimation 모델이다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Bottom-up 
 영상에서 키포인트를 먼저 찾은다음, 그 키포인트들 간에 관계를 분석하여 자세를 추정한다.
 정확도는 Top-down 방식에 비교해서 떨어지지만 처음에 사람을 detect 하는 과정을 생략하기 때문에 빠르다는 장점이 있고,
 Real-time에 사용하는 모델들에 주로 사용되는 방식이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Top-down
 영상에서 먼저 사람의 위치를 detect하고, detect 되어진 bounding box 내부에서 자세를 추정하는 방식이다.
 정확도는 Bottom-up에 비해서 높지만, Multi-person일 경우에 detectin 된 사람마다 자세를 추정하기 때문에 느리다는 단점이 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;과거의 pose 추정 모델들이 사람의 구조적 특성에 집중하였다면, 요즘의 pose 추정 모델들은 강력해진 CNN 모델들을 이용하여
신체 자체를 측정하는데 좀 더 초점을 둔다.&lt;/p&gt;

&lt;p&gt;먼저 학습 된 CNN 모델을 통해서 신체부위의 클래스를 예측한 후에 SVM이 신체 포인트 쌍의 관계 점수를 매기는데 사용된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SVM - 서포트 백터 머신
서포트 벡터 머신(support vector machine, SVM)은 기계 학습의 분야 중 하나로 패턴 인식, 자료 분석을 위한 지도 학습 모델이며, 주로 분류와 회귀 분석을 위해 사용한다. 두 카테고리 중 어느 하나에 속한 데이터의 집합이 주어졌을 때, SVM 알고리즘은 주어진 데이터 집합을 바탕으로 하여 새로운 데이터가 어느 카테고리에 속할지 판단하는 비확률적 분류 모델을 만든다.
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/1920px-Kernel_Machine.svg.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DeeperCut을 이해하려면 먼저 DeepCut을 알아야 한다. DeepCut은 multi-persion pose estimation의 state-of-the-art이다.&lt;/p&gt;

&lt;p&gt;집합 D = 신체 부위 후보군들(주어진 이미지에서 예측되는 신체 부위들)
집합 C = 신체 부위 클래스들(머리, 어깨, 무릎 등의 신체 부위들)&lt;/p&gt;

&lt;p&gt;집한 D는 일반적으로 body part detector에 의해 생성되며, 각각의 후보자 d ∈ D는 모든 body part class c ∈ C 에 대응하는 점수를 가지고 있다. 이 점수에 기반하여 DeepCut은 각각의 d가 c에 대해 가지는 점수 세트 αdc ∈ R를 만든다. 추가적으로 모든&lt;/p&gt;

&lt;p&gt;최근 이미지넷으로 훈련시킨 Resnet이 인간의 판단 능력을 넘어섰다. 그러한 Resnet을 이용하였으며 152개의 layer를 쌓았다.&lt;/p&gt;
</description>
            <pubDate>Wed, 27 May 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/deeplabcut/deepercut.html</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/deeplabcut/deepercut.html</guid>
            
            
        </item>
        
        <item>
            <title>환경설정</title>
            <description>&lt;h1 id=&quot;환경설정&quot;&gt;환경설정&lt;/h1&gt;
&lt;p&gt;Ubuntu 16.04 환경에서 작업하였다.&lt;/p&gt;

&lt;h3 id=&quot;requirements&quot;&gt;Requirements&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CMake &amp;gt;= 3.12

$ sudo apt install cmake
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CUDA &amp;gt;= 10.0

network 형태로 설치(이 형태로 하면 패치를 받을 필요가 없어서 편하다)
$ wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-ubuntu1604.pin
$ sudo mv cuda-ubuntu1604.pin /etc/apt/preferences.d/cuda-repository-pin-600
$ sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub
$ sudo add-apt-repository &quot;deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/ /&quot;
$ sudo apt-get update
$ sudo apt-get -y install cuda
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;OpenCV &amp;gt;= 2.4

$ sudo apt-get install build-essential cmake pkg-config libjpeg-dev libtiff5-dev libjasper-dev libpng12-dev libavcodec-dev libavformat-dev libswscale-dev libxvidcore-dev libx264-dev libxine2-dev libv4l-dev v4l-utils libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libqt4-dev mesa-utils libgl1-mesa-dri libqt4-opengl-dev libatlas-base-dev gfortran libeigen3-dev python2.7-dev python3-dev python-numpy python3-numpy
임시 폴더 생성
$ wget -O opencv.zip https://github.com/Itseez/opencv/archive/3.2.0.zip
$ unzip opencv.zip
$ wget -O opencv_contrib.zip https://github.com/Itseez/opencv_contrib/archive/3.2.0.zip
$ unzip opencv_contrib.zip
$ cd opencv-3.2.0
$ mkdir build
$ cd build
$ make -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D WITH_TBB=OFF -D WITH_IPP=OFF -D WITH_1394=OFF -D BUILD_WITH_DEBUG_INFO=OFF -D BUILD_DOCS=OFF -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D BUILD_EXAMPLES=OFF -D BUILD_TESTS=OFF -D BUILD_PERF_TESTS=OFF -D ENABLE_NEON=ON -D WITH_QT=ON -D WITH_OPENGL=ON -D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib-3.2.0/modules -D WITH_V4L=ON -D WITH_FFMPEG=ON -D WITH_XINE=ON -D BUILD_NEW_PYTHON_SUPPORT=ON -D PYTHON_INCLUDE_DIR=/usr/include/python2.7 -D PYTHON_INCLUDE_DIR2=/usr/include/x86_64-linux-gnu/python2.7 -D PYTHON_LIBRARY=/usr/lib/x86_64-linux-gnu/libpython2.7.so ../
$ make -j8 (cpu 코어 수에 따라서 숫자 조절)
$ sudo make install
설치확인
$ pkg-config --modversion opencv
$ pkg-config --libs --cflags opencv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;opencv.png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cuDNN &amp;gt;= 7.0

1. https://developer.nvidia.com/cudnn 이동
2. Download cuDNN 클릭
3. Login
4. https://developer.nvidia.com/rdp/cudnn-archive 이동
5. Download cuDNN v7.6.5 (November 18th, 2019), for CUDA 10.2 선택
6. cuDNN Runtime Library for Ubuntu16.04 (Deb) 다운로드
7. sudo dpkg -i 다운받은deb파일
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;darknet-설치&quot;&gt;darknet 설치&lt;/h3&gt;
&lt;p&gt;위의 requirements가 모두 설치되었다는 가정하에 진행한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://github.com/AlexeyAB/darknet.git 받고 해당 폴더 이동&lt;/li&gt;
  &lt;li&gt;Makefile 열어서 수정(아래 그림 파일 참조)
&lt;img src=&quot;makefile.png&quot; /&gt;
본인의 경우에는 현재 PC의 상황에 맞게 다음과 같이 설정하였다.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;GPU=1
CUDNN=1
CUDNN_HALF=0
OPENCV=1
AVX=0
OPENMP=0
LIBSO=1
ZED_CAMERA=0
ZED_CAMERA_v2_8=0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;$ make 명령어를 통해 컴파일&lt;/li&gt;
  &lt;li&gt;컴파일 도중 NVCC 에러가 날 수도 있다. 이때는 다시 Makefile을 열어서 본인의 nvcc 경로를 변경해줘야한다.
일반적으로 위에서 cuda를 제대로 설치했으면 /usr/local/cuda/bin/nvcc 일 것이다.&lt;/li&gt;
  &lt;li&gt;./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights 명령어를 통해 올바르게 컴파일이 되었는지 확인을 해본다. 보통 makefile이나 실행 도중 에러가 나면 에러메시지를 잘 확인해 보길 바란다. cuda version 문제 또는 cudnn version문제 또는 opencv 관련 문제일 것이다.&lt;/li&gt;
&lt;/ul&gt;

</description>
            <pubDate>Wed, 28 Oct 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/Yolo(darknet)/environment.html</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/Yolo(darknet)/environment.html</guid>
            
            
        </item>
        
        <item>
            <title>텐서플로우 이미지 분류하기</title>
            <description># 이미지 분류
이 학습서는 이미지에서 고양이 또는 개를 분류하는 방법을 보여줍니다. tf.keras.Sequential 모델을 사용하여 이미지 분류기를 작성하고 tf.keras.preprocessing.image.ImageDataGenerator를 사용하여 데이터를 로드합니다.

다음과 같은 개념에 대한 실용적인 경험을 쌓고 직관력을 키울 수 있습니다.
* tf.keras.preprocessing.image.ImageDataGenerator를 사용한 data input 파이프라인을 구축하고 디스크에 있는 데이터를 모델과 함께 효율적으로 컨트롤 할 수 있습니다.
* Overfitting(과적합) - 식별하고 예방하는 방법
* 데이터 확대 및 드롭 아웃 - 컴퓨터 비전 task를 데이터 파이프 라인 그리고 이미지 분류 모델에 포함하여 Overfitting을 방지하는 중요한 기술

이 튜토리얼은 일반적인 머신러닝 워크플로우를 따릅니다.
1. 데이터 검사 및 이해
2. 입력 파이프 라인 구축
3. 모델 구축
4. 모델 훈련
5. 모델 테스트
6. 모델 개선 및 프로세스 반복

## 패키지 가져오기
필요한 패키지를 가져와 시작하겠습니다. os패키지는 파일 및 디렉토리 구조를 판독하는데 사용됩니다. NumPy는 파이썬 list 형태를 numpy array 형태로 변환하는데 사용되고, 요구되는 매트릭스 연산을 수행합니다. matplotlib.pyplot은 학습 그리고 검증 그래프를 보여줍니다.

모델을 구성하는 데 필요한 Tensorflow 및 Keras 클래스를 가져옵니다.

```
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator

import os
import numpy as np
import matplotlib.pyplot as plt
```

## 데이터 로드
데이터 세트를 다운로드하여 시작하십시오. 이 튜토리얼은 Kaggle 의 필터링 된 버전의 Dogs vs Cats 데이터 세트를 사용합니다. 데이터 세트의 아카이브 버전을 다운로드하여 &quot;/tmp/&quot;디렉토리에 저장하십시오.
```
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')
```

데이터셋은 다음과 같은 디렉토리 구조를 가집니다.
```
cats_and_dogs_filtered
|__ train
    |______ cats: [cat.0.jpg, cat.1.jpg, cat.2.jpg ....]
    |______ dogs: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...]
|__ validation
    |______ cats: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ....]
    |______ dogs: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]
```

압축을 해제한 후 학습 및 검증 세트에 적합한 파일 경로로 변수를 지정하십시오.
```
train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

train_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures
train_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures
validation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures
validation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures
```

## 데이터 이해
훈련 및 검증 디렉토리에 고양이와 개 이미지가 몇 개인 지 살펴 보겠습니다.

```
num_cats_tr = len(os.listdir(train_cats_dir))
num_dogs_tr = len(os.listdir(train_dogs_dir))

num_cats_val = len(os.listdir(validation_cats_dir))
num_dogs_val = len(os.listdir(validation_dogs_dir))

total_train = num_cats_tr + num_dogs_tr
total_val = num_cats_val + num_dogs_val
```
```
print('total training cat images:', num_cats_tr)
print('total training dog images:', num_dogs_tr)

print('total validation cat images:', num_cats_val)
print('total validation dog images:', num_dogs_val)
print(&quot;--&quot;)
print(&quot;Total training images:&quot;, total_train)
print(&quot;Total validation images:&quot;, total_val)
```
```
total training cat images: 1000
total training dog images: 1000
total validation cat images: 500
total validation dog images: 500
--
Total training images: 2000
Total validation images: 1000
```

편의를 위해 데이터 세트를 사전 처리하고 네트워크를 훈련하는 동안 사용할 변수를 설정하십시오.

```
batch_size = 128
epochs = 15
IMG_HEIGHT = 150
IMG_WIDTH = 150
```

## 데이터 준비
네트워크에 데이터를 공급하기 전에 이미지를 부동 소수점 tensor로 변환해야 합니다

1. 디스크에서 이미지를 읽습니다.
2. 이 이미지의 내용을 디코딩하여 RGB 내용에 따라 적절한 grid 형식으로 변환합니다.
3. 이 값들을 부동 소수점 tensor로 변환합니다.
4. 신경망은 작은 입력 값을 처리하는 것을 선호하므로 텐서를 0에서 255 사이의 값에서 0에서 1 사이의 값으로 재조정하십시오.

다행히 이러한 모든 작업은 tf.keras에 의해 제공되는 ImageDataGenerator 클래스에서 수행 할 수 있습니다. 디스크에서 이미지를 읽고 적절한 텐서로 사전 처리 할 수 ​​있습니다. 또한 이 이미지들을 텐서의 배치로 변환하여 네트워크 학습에 도움을 줄 수 있습니다.

```
train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data
validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data
```

학습 그리고 검증 이미지를 위해 생성기를 정의하고 난 뒤 flow_from_directory 메서드는 디스크로부터 이미지를 불러오고, rescaling을 적용하고, 요구되는 면적에 맞게 이미지를 resize 합니다.

```
train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,
                                                           directory=train_dir,
                                                           shuffle=True,
                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                           class_mode='binary')
```
```
val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,
                                                              directory=validation_dir,
                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                              class_mode='binary')
```

### 학습 이미지 시각화
트레이닝 생성기로부터 이미지의 배치를 추출하고 시각화한다
```
sample_training_images, _ = next(train_data_gen)
```
next 함수는 데이터셋의 배치를 반환한다. next 함수의 반환값은 (x_train, y_train) 형태이다. 여기서 x_train은 학습 특징들이고 y_train은 라벨이다. 학습 데이터만 보여주기 위해서 라벨은 숨긴다
```
# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.
def plotImages(images_arr):
    fig, axes = plt.subplots(1, 5, figsize=(20,20))
    axes = axes.flatten()
    for img, ax in zip( images_arr, axes):
        ax.imshow(img)
        ax.axis('off')
    plt.tight_layout()
    plt.show()
```
```
plotImages(sample_training_images[:5])
```
&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/classification_files/output_d_VVg_gEVrWW_0.png&quot;/&gt;&lt;br/&gt;

## 모델 생성하기
모델에는 max pool layer를 가지고 있는 3개의 convolution block이 존재한다. 512개의 유닛이 존재하는 fully connected layer가 가장 상단에 존재하고 활성화 함수로 relu 함수가 사용된다.
```
model = Sequential([
    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),
    MaxPooling2D(),
    Conv2D(32, 3, padding='same', activation='relu'),
    MaxPooling2D(),
    Conv2D(64, 3, padding='same', activation='relu'),
    MaxPooling2D(),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(1)
])
```

### 모델 컴파일하기
이 튜토리얼에서는 ADAM 옵티마이저와 cross entropy 손실 함수를 선택하였다. 학습 그리고 검증 정확도를 보기 위해 각각의 학습 에폭에 metrics 아규먼트를 넘겨준다.
```
model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
```

### 모델 요약
summary 메서드를 이용하여 네트워크의 모든 레이어를 관찰할 수 있다.
```
model.summary()
```
```
Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 150, 150, 16)      448       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 75, 75, 16)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 75, 75, 32)        4640      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 37, 37, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 37, 37, 64)        18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 18, 18, 64)        0         
_________________________________________________________________
flatten (Flatten)            (None, 20736)             0         
_________________________________________________________________
dense (Dense)                (None, 512)               10617344  
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 513       
=================================================================
Total params: 10,641,441
Trainable params: 10,641,441
Non-trainable params: 0
_________________________________________________________________
```

### 모델 학습시키기
**ImageDataGenerater** 클래스의 **fit_generatoer** 메서드를 이용해서 네트워크를 학습시킨다.
```
history = model.fit_generator(
    train_data_gen,
    steps_per_epoch=total_train // batch_size,
    epochs=epochs,
    validation_data=val_data_gen,
    validation_steps=total_val // batch_size
)
```

### 트레이닝 결과 시각화하기
```
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss=history.history['loss']
val_loss=history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()
```

&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/classification_files/output_K6oA77ADVrWp_0.png&quot;/&gt;&lt;br/&gt;

그림에서 알 수 있듯이 트레이닝 정확도와 밸리데이션 정확도는 큰 폭으로 증가하였습니다. 그리고 밸리데이션 정확도는 70% 정도를 달성하였습니다.
무엇이 잘못되었는지 살펴보고 모델의 전반적인 성능을 향상 시켜보겠습니다.

## 과적합(Overfitting)
위의 그림에서 학습 정확도는 시간이 지날수록 선형적으로 증가하지만, 검증 정확도는 70%를 넘지 못합니다. 또한 학습데이터와 검증데이터의 정확도 차이는 과적합의 명확한 신호입니다.

적은 개수의 학습 데이터에서, 학습 데이터의 원치않는 디테일이나 노이즈를 모델이 학습해버리는 경우가 있습니다. 이것은 새로운 데이터를 인식하는 모델의 정확도에 않좋은 영향을 끼칩니다. 이 현상은 과적합이라고 알려져있습니다. 이 의미는 모델이 새로운 데이터셋이 들어왔을때 일반화하지 못한다는 의미입니다.

오버피팅을 방지하는 여러가지 방법들이 학습 단계에 존재합니다. 이 튜토리얼에서 우리는 *데이터 보강* 방법과 드롭아웃 기법을 사용할 것입니다.

## 데이터 보강
과적합은 일반적으로 트레이닝 데이터가 부족할 때 발생합니다. 이 문제를 해결하는 한 방법은 데이터를 보강하여 충분한 개수의 트레이닝 데이터를 얻는 것입니다. 데이터 보강은 존재하는 학습 데이터에서 그럴듯하게 보이는 이미지를 랜덤하게 생성해내는 방법이다. 목표는 모델은 절대 두번이상 같은 데이터를 학습하지 않는 것이다. 이것은 모델이 더 다양한 데이터를 접하고 일반화하게 만든다.

tf.keras의 **ImageDataGenerator** 클래스를 사용한다. 데이터셋의 다양한 변화를 시도한다.

### 데이터 보강과 시각화
랜덤 수평 flip 보강을 시작하고, 각각의 이미지들의 변화 후 모습을 보자

### 수평 flip 적용하기
ImageDataGeneraotr 클래스에 인자로 horizontal_flip 을 넘겨준다 그리고 True 값을 줘서 이 인자를 적용시킨다.
```
image_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)

train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,
                                               directory=train_dir,
                                               shuffle=True,
                                               target_size=(IMG_HEIGHT, IMG_WIDTH))
```

트레이닝 에제로부터 하나의 샘플 이미지를 가져오고 5번 반복한다, 즉 같은 이미지에 대해서 5번 증강을 시도한다
```
augmented_images = [train_data_gen[0][0][0] for i in range(5)]
```
```
# Re-use the same custom plotting function defined and used
# above to visualize the training images
plotImages(augmented_images)
```
&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/classification_files/output_EvBZoQ9xVrW9_0.png&quot;/&gt;&lt;br/&gt;

### 이미지를 랜덤하게 회전하기
회전이라 불리는 증강방법을 살펴보자 그리고 트레이닝 예제들을 45도 회전해보자 랜덤하게

```
image_gen = ImageDataGenerator(rescale=1./255, rotation_range=45)
```
```
train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,
                                               directory=train_dir,
                                               shuffle=True,
                                               target_size=(IMG_HEIGHT, IMG_WIDTH))

augmented_images = [train_data_gen[0][0][0] for i in range(5)]
```
```
plotImages(augmented_images)
```
&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/classification_files/output_wmBx8NhrVrXK_0.png&quot;/&gt;&lt;br/&gt;

### 확대 증강 적용하기
이미지를 랜덤하게 50프로 확대하는 증강법을 알아보자
```
# zoom_range from 0 - 1 where 1 = 100%.
image_gen = ImageDataGenerator(rescale=1./255, zoom_range=0.5) # 
```
```
train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,
                                               directory=train_dir,
                                               shuffle=True,
                                               target_size=(IMG_HEIGHT, IMG_WIDTH))

augmented_images = [train_data_gen[0][0][0] for i in range(5)]
```
```
plotImages(augmented_images)
```
&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/classification_files/output_-KQWw8IZVrXZ_0.png&quot;/&gt;&lt;br/&gt;

### 모두 적용해보기
이전의 증강법을 모두 적용해보자. 이 방법으로 너는 rescale, 45도 회전, 가로 쉬프트, 세로 쉬프트, 수평 플립 그리고 확대 증강법을 사용할 수 있다.
```
image_gen_train = ImageDataGenerator(
                    rescale=1./255,
                    rotation_range=45,
                    width_shift_range=.15,
                    height_shift_range=.15,
                    horizontal_flip=True,
                    zoom_range=0.5
                    )
```
```
train_data_gen = image_gen_train.flow_from_directory(batch_size=batch_size,
                                                     directory=train_dir,
                                                     shuffle=True,
                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                     class_mode='binary')
```
하나의 이미지에 대해서 5번 이 증강법을 랜덤하게 적용했을 때 어떤 변화가 일어나는지 확인해보자
```
augmented_images = [train_data_gen[0][0][0] for i in range(5)]
plotImages(augmented_images)
```
&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/classification_files/output_z2m68eMhVrXm_0.png&quot;/&gt;&lt;br/&gt;

### 검증 데이터 생성기 만들기
일반적으로 데이터 증강은 훈련 데이터에 대해서만 시행한다. 이번에는, 검증 데이터를 단순히 rescale하고 ImageDataGenerator를 사용하여 검증 데이터를 배치로 변환하는 것을 해보겠다.
```
image_gen_val = ImageDataGenerator(rescale=1./255)
```
```
val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size,
                                                 directory=validation_dir,
                                                 target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                 class_mode='binary')
```

## 드롭아웃 - Dropout
과적합을 방지하는 다른 테크닉은 네트워크에 드롭아웃을 적용하는 것이다. 이것은 정규화의 한 방법으로써 네트워크에서 작은 부분의 가중치만 선택하게 함으로써 가중치를 분산시키고 작은 훈련 데이터에서 과적합을 방지하는 것이다.

드롭아웃을 레이어에 적용할 때, 훈련 과정동안 랜덤하게 뉴런의 일정부분을 꺼놓는다. 드롭아웃은 0.1, 0.2, 0.4와 같이 아주 작은 인풋을 받는다. 이 의미는 10%, 20%, 40%의 뉴런에 대해서 랜덤하게 드롭아웃을 적용하라는 의미다.

어떤 특정한 레이어에 대해 0.1 드롭아웃을 적용하면, 이것은 각 에폭마다 랜덤하게 10%의 뉴런을 끄고 학습을 진행한다.

이 드롭아웃 기법을 컨볼루션과 fully-connected layers에 적용하여 새롭게 모델을 설계해라

## 드롭아웃을 사용하여 새로운 네트워크 생성하기
여기, 너는 첫번째와 마지막 max pool 레이어에 드롭아웃을 적용한다. 드롭아웃은 각 훈련 에폭동안 20%의 뉴런을 제외하고 학습을 진행한다. 이것은 트레이닝 데이터셋에서 오버피팅을 방지하는데 도움을 준다
```
model_new = Sequential([
    Conv2D(16, 3, padding='same', activation='relu', 
           input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),
    MaxPooling2D(),
    Dropout(0.2),
    Conv2D(32, 3, padding='same', activation='relu'),
    MaxPooling2D(),
    Conv2D(64, 3, padding='same', activation='relu'),
    MaxPooling2D(),
    Dropout(0.2),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(1)
])
```

### 모델 컴파일하기
네트워크에 드롭아웃을 적용한 다음에 모델을 컴파일하고 레이어의 서머리를 확인해보자
```
model_new.compile(optimizer='adam',
                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                  metrics=['accuracy'])

model_new.summary()
```
```
Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_3 (Conv2D)            (None, 150, 150, 16)      448       
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 75, 75, 16)        0         
_________________________________________________________________
dropout (Dropout)            (None, 75, 75, 16)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 75, 75, 32)        4640      
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 37, 37, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 37, 37, 64)        18496     
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 18, 18, 64)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 18, 18, 64)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 20736)             0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               10617344  
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 513       
=================================================================
Total params: 10,641,441
Trainable params: 10,641,441
Non-trainable params: 0
_________________________________________________________________
```

### 모델 학습하기
성공적으로 데이터 증강을 학습 데이터들에 적용하고 네트워크에 드롭아웃을 적용하고 이 새로운 네트워크를 학습시켜보자!
```
history = model_new.fit_generator(
    train_data_gen,
    steps_per_epoch=total_train // batch_size,
    epochs=epochs,
    validation_data=val_data_gen,
    validation_steps=total_val // batch_size
)
```

### 모델 시각화하기
훈련 후에 새로운 모델을 시각화한다. 너는 상당히 과적합이 줄어든 것을 확인할 수 있다. 정확도는 에폭이 증가할 수록 점점 더 증가한다.
```
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()
```
&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/classification_files/output_7BTeMuNAVrYC_0.png&quot;/&gt;&lt;br/&gt;
</description>
            <pubDate>Thu, 23 Apr 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/image_classification.html</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/image_classification.html</guid>
            
            
        </item>
        
        <item>
            <title>텐서플로우 허브 전이학습</title>
            <description># 텐서플로 허브와 전이학습(transfer learning)
텐서플로 허브는 미리 학습된 모델들을 공유하는 곳이다. [텐서플로우 모듈 허브](https://tfhub.dev/)를 보면 미리 학습된 모델들을 찾을 수 있다.
이 튜토리얼은 다음과 같은 과정들을 보여줄 것이다.

1. tf.keras로 텐서플로 허브를 사용하는 방법.
2. 텐서플로 허브를 사용하여 이미지 분류를 하는 방법.
3. 간단한 전이학습을 하는 방법.

## 셋업
```
from __future__ import absolute_import, division, print_function, unicode_literals

import matplotlib.pylab as plt

import tensorflow as tf
```
```
!pip install -q -U tf-hub-nightly
import tensorflow_hub as hub

from tensorflow.keras import layers
```

## ImageNet 분류기(classifier)
### 분류기 다운로드
*hub.module*을 사용하여 mobilenet을 불러오고, *tf.keras.layers.Lambda*로 감싸서 keras 레이어로 만든다. tfhub.dev에서 구한 [어떠한 텐서플로우2의 비교가능한 이미지 분류기 URL](https://tfhub.dev/s?q=tf2&amp;module-type=image-classification)도 여기서 다 작동을 한다.

```
classifier_url =&quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2&quot; #@param {type:&quot;string&quot;}
``` 
```
IMAGE_SHAPE = (224, 224)

classifier = tf.keras.Sequential([
    hub.KerasLayer(classifier_url, input_shape=IMAGE_SHAPE+(3,))
])
```

### 하나의 이미지에 대해 실행해보기
이미지를 하나 받아서 모델에 적용해보자
```
import numpy as np
import PIL.Image as Image

grace_hopper = tf.keras.utils.get_file('image.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg')
grace_hopper = Image.open(grace_hopper).resize(IMAGE_SHAPE)
grace_hopper
```
```
grace_hopper = np.array(grace_hopper)/255.0
grace_hopper.shape
```
```
&gt; (224, 224, 3)
```
배치 차원을 더해주고, 모델에다 이미지를 넣는다
```
result = classifier.predict(grace_hopper[np.newaxis, ...])
result.shape
```
```
&gt; (1, 1001)
```
결과는 logit의 1001 요소 벡터 값, 그리고 이미지의 각 클래스에 대한 확률 값이다.
따라서 가장 높은 확률의 id값은 argmax로 구할 수 있다.
```
predicted_class = np.argmax(result[0], axis=-1)
predicted_class
```
```
&gt; 653
```

### 예측 해독하기
우리는 class ID값을 예측하고, **ImageNet** 라벨을 불러오고, 예측을 해독할 것이다.
```
labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')
imagenet_labels = np.array(open(labels_path).read().splitlines())
```
```
plt.imshow(grace_hopper)
plt.axis('off')
predicted_class_name = imagenet_labels[predicted_class]
_ = plt.title(&quot;Prediction: &quot; + predicted_class_name.title())
```

## 단순한 전이학습(transfer learning)
TF Hub를 사용한다면 우리의 데이터셋의 클래스는 구분하기 위한 모델의 탑 레이어를 재교육하기 쉽다.

### Dataset
이 예제에서 너는 tensorFlow flowers dataset을 이용할 것이다.
```
data_root = tf.keras.utils.get_file(
  'flower_photos','https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
   untar=True)
```
우리 모델에 이 데이터를 불러오는 가장 간단한 방법은 **tf.keras.preprocessing.image.ImageDataGenerator를 사용하는 것이다.

모든 텐서플로우 허브의 이미지 모듈들은 [0, 1]사이의 실수 값을 사용한다. **ImageDataGenerator**의 *rescale* 파라메터를 사용하여 변환해라

```
image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)
image_data = image_generator.flow_from_directory(str(data_root), target_size=IMAGE_SHAPE)
```

결과 객체는 image_batch, label_batch 쌍을 반환하는 iterator이다
```
for image_batch, label_batch in image_data:
  print(&quot;Image batch shape: &quot;, image_batch.shape)
  print(&quot;Label batch shape: &quot;, label_batch.shape)
  break
```
```
&gt; Image batch shape:  (32, 224, 224, 3)
  Label batch shape:  (32, 5)
```

### 이미지의 배치에 대한 분류기(classifier) 실행하기
```
result_batch = classifier.predict(image_batch)
result_batch.shape
```
```
&gt; (32, 1001)
```
```
predicted_class_names = imagenet_labels[np.argmax(result_batch, axis=-1)]
predicted_class_names
```
```
&gt; array(['daisy', 'mushroom', 'Bedlington terrier', 'daisy', 'daisy', 'bee',
         'coral fungus', 'hair slide', 'picket fence', 'daisy', 'pot',
         'mushroom', 'daisy', 'bee', 'rapeseed', 'daisy', 'daisy',
         'water buffalo', 'spider web', 'cardoon', 'daisy', 'daisy', 'bee',
         'daisy', 'vase', 'daisy', 'barn spider', 'slug', 'coral fungus',
         'sea urchin', 'pot', 'coral fungus'], dtype='&lt;U30')
```

이미지와 함께 결과를 확인해보자
```
plt.figure(figsize=(10,9))
plt.subplots_adjust(hspace=0.5)
for n in range(30):
  plt.subplot(6,5,n+1)
  plt.imshow(image_batch[n])
  plt.title(predicted_class_names[n])
  plt.axis('off')
_ = plt.suptitle(&quot;ImageNet predictions&quot;)
```

&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub_files/output_IXTB22SpxDLP_0.png&quot;/&gt;&lt;br/&gt;

결과는 완벽하지 않지만 모델이 데이지를 제외한 클래스들에 대해 학습된게 아니라는 점을 고려해야한다.

### 헤드리스모델 다운로드
텐서플로우 허브는 또한 top classification layer를 제외한 모델을 배포한다. 이것은 전이 학습에 매우 쉽게 사용될 수 있다.

tfhub.dev에서 구한 [어떠한 텐서플로우2의 비교가능한 이미지 특징 벡터 URL](https://tfhub.dev/s?module-type=image-feature-vector&amp;q=tf2)도 여기서 다 작동을 한다.
```
feature_extractor_url = &quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2&quot; #@param {type:&quot;string&quot;}
```
특징 추출기를 만든다
```
feature_extractor_layer = hub.KerasLayer(feature_extractor_url,
                                         input_shape=(224,224,3))
```
각 이미지에 대해 1280 길이의 벡터를 반환한다
```
feature_batch = feature_extractor_layer(image_batch)
print(feature_batch.shape)
```
```
&gt; (32, 1280)
```
특징 추출 레이어(feature extractor layer)의 변수들을 고정한다. 따라서 모델에서 오직 새로운 classifier layer를 훈련하고 추가할 뿐이다.
```
feature_extractor_layer.trainable = False
```

### Classification haed 붙이기
hub layer를 **tf.keras.Sequential** 모델로 감싸고 새로운 classification layer를 추가해라
```
model = tf.keras.Sequential([
  feature_extractor_layer,
  layers.Dense(image_data.num_classes)
])

model.summary()
```
```
Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
keras_layer_1 (KerasLayer)   (None, 1280)              2257984   
_________________________________________________________________
dense (Dense)                (None, 5)                 6405      
=================================================================
Total params: 2,264,389
Trainable params: 6,405
Non-trainable params: 2,257,984
_________________________________________________________________
```
```
predictions = model(image_batch)
predictions.shape
```
```
&gt; TensorShape([32, 5])
```

### 모델 학습시키기
트레이닝 과정을 설정하고 컴파일한다
```
model.compile(
  optimizer=tf.keras.optimizers.Adam(),
  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
  metrics=['acc'])
```

이제 **.fit** 메서드를 사용해서 모델을 학습시킨다

예제를 짧게 유지하기 위해 단지 2 에폭만 학습한다. 트레이닝 과정을 시각화하기위해 에폭의 평균 대신 각 배치 고유의 loss와 accuracy를 남기는 custom callback을 이용한다.
```
class CollectBatchStats(tf.keras.callbacks.Callback):
  def __init__(self):
    self.batch_losses = []
    self.batch_acc = []

  def on_train_batch_end(self, batch, logs=None):
    self.batch_losses.append(logs['loss'])
    self.batch_acc.append(logs['acc'])
    self.model.reset_metrics()
```
```
steps_per_epoch = np.ceil(image_data.samples/image_data.batch_size)

batch_stats_callback = CollectBatchStats()

history = model.fit_generator(image_data, epochs=2,
                              steps_per_epoch=steps_per_epoch,
                              callbacks = [batch_stats_callback])
```
```
&gt; Epoch 1/2
  115/115 [==============================] - 10s 91ms/step - loss: 0.3135 - acc: 0.9375
  Epoch 2/2
  115/115 [==============================] - 10s 89ms/step - loss: 0.2287 - acc: 0.9062
```
적은 학습만으로도 잘 동작한다는걸 확인할 수 있습니다.
```
plt.figure()
plt.ylabel(&quot;Loss&quot;)
plt.xlabel(&quot;Training Steps&quot;)
plt.ylim([0,2])
plt.plot(batch_stats_callback.batch_losses)
```
&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub_files/output_3uvX11avTiDg_1.png&quot;/&gt;&lt;br/&gt;

### 예측(prediction) 확인하기
그림을 다시 그리기 위해 클래스 이름의 정렬된 순서를 가져온다
```
class_names = sorted(image_data.class_indices.items(), key=lambda pair:pair[1])
class_names = np.array([key.title() for key, value in class_names])
class_names
```
```
&gt; array(['Daisy', 'Dandelion', 'Roses', 'Sunflowers', 'Tulips'],
      dtype='&lt;U10')
```

모델에 이미지 배치를 돌리고 클래스 이름이 위치하도록 변환한다
```
predicted_batch = model.predict(image_batch)
predicted_id = np.argmax(predicted_batch, axis=-1)
predicted_label_batch = class_names[predicted_id]
```
결과를 그린다
```
label_id = np.argmax(label_batch, axis=-1)

plt.figure(figsize=(10,9))
plt.subplots_adjust(hspace=0.5)
for n in range(30):
  plt.subplot(6,5,n+1)
  plt.imshow(image_batch[n])
  color = &quot;green&quot; if predicted_id[n] == label_id[n] else &quot;red&quot;
  plt.title(predicted_label_batch[n].title(), color=color)
  plt.axis('off')
_ = plt.suptitle(&quot;Model predictions (green: correct, red: incorrect)&quot;)
```
&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub_files/output_wC_AYRJU9NQe_0.png&quot;/&gt;&lt;br/&gt;

## Export your model
훈련시킨 모델을 저장하고 내보낸다
```
import time
t = time.time()

export_path = &quot;/home/testopia-01/workspace/saved_models/{}&quot;.format(int(t))
model.save(export_path, save_format='tf')
```
이제 우리는 모델을 불러오고 같은 결과를 기대할 수 있다.
```
reloaded = tf.keras.models.load_model(export_path)

result_batch = model.predict(image_batch)
reloaded_result_batch = reloaded.predict(image_batch)

abs(reloaded_result_batch - result_batch).max()
```
```
&gt; 0.0
```</description>
            <pubDate>Thu, 23 Apr 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/image_transfer_learning_TFHub.html</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/image_transfer_learning_TFHub.html</guid>
            
            
        </item>
        
        <item>
            <title>텐서플로우 전이학습</title>
            <description># 미리 학습된 ConvNet으로 부터 전이 학습
이 튜토리얼에서, 너는 미리 학습된 네트워크로부터 전이학습을 통하여 강아지와 고양이를 분류하는 방법을 배우게 될 것이다.

미리 학습된 모델은 매우 큰 데이터 셋으로 부터 미리 학습되어 저장되어 있다. 사전 훈련 된 모델을 그대로 사용하거나 전이학습을 통하여 모델을 우리가 원하는 방법으로 커스터마이징 할 수 있다.

이미지 classification을 위한 전이학습은 직관적으로 다음과 같이 표현할 수 있다.

만약 모델이 매우 크고 일반적인 데이터셋을 통하여 훈련되었다면, 이 모델은 실제 visual 세상의 일반적인 모델로서 효과적으로 작동한다.

너는 큰 데이터셋에서 큰 모델을 학습하여 처음부터 시작할 필요 없이, 이러한 학습 된 모델을 사용할 수 있다.

이 노트북에서 너는 사전 훈련된 모델을 커스터마이징 하는 두 가지 방법을 시도할 것이다.

1. 특징 추출: 미리 학습된 네트워크에서 배운 표현을 사용하여 새로운 학습데이터에서 의미있는 특징을 추출할 것이다. 미리 학습된 모델에다가 새로운 classifier를 추가하기만 하면 미리 학습된 모델의 특징맵을 재사용 할 수 이싿.

2. 미세 조정(fine-tuning): 새로 추가 할 classifier 레이어와 기존 모델의 마지막 레이어를 함께 훈련시킨다. 이를 통해 기존의 모델에서 고차원 특징 표현을 미세 조정하여 우리가 원하는 작업에 보다 적합하게 만들 수 있다.

너는 일반적 머신 러닝 작업플로우를 따를 것이다.
1. 데이터 검사 및 이해
2. 케라스 ImageDataGenerator를 통하여 input pipeline 빌드
3. 모델 구성
    - 미리 학습된 모델 Load(and 미리 학습된 가중치)
    - classfication layer 쌓기
4. 모델 학습
5. 모델 검증
```
import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
```

## Data preprocessing
### Data download

강아지와 고양이 데이터셋을 로드하기 위해 TensorFlow Datasets를 사용한다

이 **tfds** 패키지는 사전 정의 된 데이터를 로드하는 가장 쉬운 방법이다.

만약 너의 고유한 데이터셋을 가지고 있고 사용하고 싶다면 loading image data 를 찹조해라
```
import tensorflow_datasets as tfds
tfds.disable_progress_bar()
```
**tfds.load** 메서드는 데이터를 다운로드 및 캐슁하고, **tf.data.Dataset** 객체를 리턴한다.

이 객체는 데이터를 조작하고 너의 모델로 공급하는 강력하고 효율적인 방법을 제공한다

&quot;cats_vs_dogs&quot;는 표준 분할을 정희하지 않았기 때문에 80%(학습), 10%(검증), 10%(테스트) 비율로 데이터를 나눈다.

```
(raw_train, raw_validation, raw_test), metadata = tfds.load(
    'cats_vs_dogs',
    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
    with_info=True,
    as_supervised=True,
)
```
 
**tf.data.Dataset** 객체의 결과는 (image, label) 쌍이고, 이미지는 3개 채널의 변수 shape이고 label은 스칼라 형태이다.

```
print(raw_train)
print(raw_validation)
print(raw_test)
```
```
&gt; &lt;DatasetV1Adapter shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)&gt;
&gt; &lt;DatasetV1Adapter shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)&gt;
&gt; &lt;DatasetV1Adapter shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)&gt;
```
 
훈련 세트에서 처음 두 개의 이미지와 레이블을 보여줍니다.

```
get_label_name = metadata.features['label'].int2str

for image, label in raw_train.take(2):
  plt.figure()
  plt.imshow(image)
  plt.title(get_label_name(label))
```

&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/transfer_learning_files/output_K5BeQyKThC_Y_0.png&quot;/&gt;&lt;br/&gt;

&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/transfer_learning_files/output_K5BeQyKThC_Y_1.png&quot;/&gt;&lt;br/&gt;

### 데이터 구성 방식
데이터를 구성하기 위해 **tf.image** 모듈을 사용한다.
이미지를 고정된 인풋 사이즈로 resize하고, [-1, 1] 범위의 값으로 rescale한다

```
IMG_SIZE = 160 # All images will be resized to 160x160

def format_example(image, label):
  image = tf.cast(image, tf.float32)
  image = (image/127.5) - 1
  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
  return image, label
```
map 함수를 이용하여 각각의 데이터 셋에 적용한다
```
train = raw_train.map(format_example)
validation = raw_validation.map(format_example)
test = raw_test.map(format_example)
```
셔플과 배치 값을 정한다
```
BATCH_SIZE = 32
SHUFFLE_BUFFER_SIZE = 1000
train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
validation_batches = validation.batch(BATCH_SIZE)
test_batches = test.batch(BATCH_SIZE)
```
데이터의 배치를 검사한다
```
for image_batch, label_batch in train_batches.take(1):
   pass

image_batch.shape
```
```
&gt; TensorShape([32, 160, 160, 3])
```

## 미리 학습된 convnets로 부터 base model 생성하기

너는 구글에서 개발 된 **MobileNet V2** 모델로부터 base model을 만들 수 있을 것이다. 140만개의 이미지와 1000개의 클래스로 이루어진 이미지넷 데이터 셋으로 부터 미리 훈련된 모델이다. ImageNet은 jackfruit와 주사기 같은 넓은 범위의 카테고리를 가진 연구용 학습 데이터 셋이다. 이 지식의 기반은 우리의 데이터셋으로 부터 고양이와 강아지를 구분하는데 도움을 줄 것이다.

먼저, 특징 추출을 위해 사용할 MobileNet V2의 레이어를 선택해야 한다. 대부분의 머신러닝 모델 다이어그램은 bottom에서 top으로 가기에 마지막 classification 레이어(top)은 유용하지 않다. 대신 너는 일반적인 관례에 따라 평탄화 작업(flatten operation) 이전의 마지막 레이어를 사용할 수 있다. 이 레이어는 &quot;bottleneck layer&quot;라고 불리운다. bottleneck 레이어는 final/top 레이어에 비해서 더 일반적인 특징들을 가지고 있고 분류할 수 있다. 

### 일반적인 Convolution Network 구성도
&lt;img src=&quot;https://cdn-images-1.medium.com/fit/t/1600/480/1*vkQ0hXDaQv57sALXAJquxA.jpeg&quot;/&gt;&lt;br/&gt;

### MobileNet V2 구성도
&lt;img src=&quot;https://1.bp.blogspot.com/-M8UvZJWNW4E/WsKk-tbzp8I/AAAAAAAAChw/OqxBVPbDygMIQWGug4ZnHNDvuyK5FBMcQCLcBGAs/s1600/image5.png&quot;/&gt;&lt;br/&gt;

먼저, ImageNet 데이터 셋으로 부터 학습 된 가중치를 가진 MobileNet V2 모델을 인스턴스화 한다. **include_top=False** 옵션값을 줌으로써, 너는 top에 classification 레이어를 포함하지 않는 특징 추출에 가장 최적화 된 네트워크를 불러올 것이다.

```
IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)

# Create the base model from the pre-trained model MobileNet V2
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
```

이 특징 추출기는 160*160*3 이미지를 5*5*1280의 특징들의 블록으로 변환시킨다. 이미지들의 배치가 어떻게 되는지 밑의 예를 통해 확인해봐라

```
feature_batch = base_model(image_batch)
print(feature_batch.shape)
```

```
&gt; (32, 5, 5, 1280)
```

## 특징 추출

이 스텝에서, 너는 이전 스텝으로부터 만들어진 합성곱(convolutional) base를 고정할 것이다. 추가적으로, 너는 가장 상단에 classifier(분류기)를 추가할 것이고, 높은 수준의 classifier를 학습할 것이다.

### 컨볼루션 베이스 고정
모델을 학습하고 컴파일 하기 전에 컨볼루션 베이스를 고정하는 것은 중요하다. layer.trainable = False 옵션을 통하여 트레이닝 도중에 이 레이어의 가중치가 업데이트 되는 것을 방지한다. MobileNet V2는 많은 레이어를 가지고 있으므로 모든 모델의 학습가능 플레그를 False로 세팅하여 모든 레이어를 고정할 수 있다.
```
base_model.trainable = False
```
```
# Let's take a look at the base model architecture
base_model.summary()
```
```
Model: &quot;mobilenetv2_1.00_160&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 160, 160, 3) 0                                            
__________________________________________________________________________________________________
Conv1_pad (ZeroPadding2D)       (None, 161, 161, 3)  0           input_1[0][0]                    
__________________________________________________________________________________________________
Conv1 (Conv2D)                  (None, 80, 80, 32)   864         Conv1_pad[0][0]                  
__________________________________________________________________________________________________
bn_Conv1 (BatchNormalization)   (None, 80, 80, 32)   128         Conv1[0][0]                      
__________________________________________________________________________________________________
Conv1_relu (ReLU)               (None, 80, 80, 32)   0           bn_Conv1[0][0]                   
__________________________________________________________________________________________________
expanded_conv_depthwise (Depthw (None, 80, 80, 32)   288         Conv1_relu[0][0]                 
__________________________________________________________________________________________________
expanded_conv_depthwise_BN (Bat (None, 80, 80, 32)   128         expanded_conv_depthwise[0][0]    
__________________________________________________________________________________________________
expanded_conv_depthwise_relu (R (None, 80, 80, 32)   0           expanded_conv_depthwise_BN[0][0] 
__________________________________________________________________________________________________
expanded_conv_project (Conv2D)  (None, 80, 80, 16)   512         expanded_conv_depthwise_relu[0][0
__________________________________________________________________________________________________
expanded_conv_project_BN (Batch (None, 80, 80, 16)   64          expanded_conv_project[0][0]      
__________________________________________________________________________________________________
block_1_expand (Conv2D)         (None, 80, 80, 96)   1536        expanded_conv_project_BN[0][0]   
__________________________________________________________________________________________________
block_1_expand_BN (BatchNormali (None, 80, 80, 96)   384         block_1_expand[0][0]             
__________________________________________________________________________________________________
block_1_expand_relu (ReLU)      (None, 80, 80, 96)   0           block_1_expand_BN[0][0]          
__________________________________________________________________________________________________
block_1_pad (ZeroPadding2D)     (None, 81, 81, 96)   0           block_1_expand_relu[0][0]        
__________________________________________________________________________________________________
block_1_depthwise (DepthwiseCon (None, 40, 40, 96)   864         block_1_pad[0][0]                
__________________________________________________________________________________________________
block_1_depthwise_BN (BatchNorm (None, 40, 40, 96)   384         block_1_depthwise[0][0]          
__________________________________________________________________________________________________
block_1_depthwise_relu (ReLU)   (None, 40, 40, 96)   0           block_1_depthwise_BN[0][0]       
__________________________________________________________________________________________________
block_1_project (Conv2D)        (None, 40, 40, 24)   2304        block_1_depthwise_relu[0][0]     
__________________________________________________________________________________________________
block_1_project_BN (BatchNormal (None, 40, 40, 24)   96          block_1_project[0][0]            
__________________________________________________________________________________________________
block_2_expand (Conv2D)         (None, 40, 40, 144)  3456        block_1_project_BN[0][0]         
__________________________________________________________________________________________________
block_2_expand_BN (BatchNormali (None, 40, 40, 144)  576         block_2_expand[0][0]             
__________________________________________________________________________________________________
block_2_expand_relu (ReLU)      (None, 40, 40, 144)  0           block_2_expand_BN[0][0]          
__________________________________________________________________________________________________
block_2_depthwise (DepthwiseCon (None, 40, 40, 144)  1296        block_2_expand_relu[0][0]        
__________________________________________________________________________________________________
block_2_depthwise_BN (BatchNorm (None, 40, 40, 144)  576         block_2_depthwise[0][0]          
__________________________________________________________________________________________________
block_2_depthwise_relu (ReLU)   (None, 40, 40, 144)  0           block_2_depthwise_BN[0][0]       
__________________________________________________________________________________________________
block_2_project (Conv2D)        (None, 40, 40, 24)   3456        block_2_depthwise_relu[0][0]     
__________________________________________________________________________________________________
block_2_project_BN (BatchNormal (None, 40, 40, 24)   96          block_2_project[0][0]            
__________________________________________________________________________________________________
block_2_add (Add)               (None, 40, 40, 24)   0           block_1_project_BN[0][0]         
                                                                 block_2_project_BN[0][0]         
__________________________________________________________________________________________________
block_3_expand (Conv2D)         (None, 40, 40, 144)  3456        block_2_add[0][0]                
__________________________________________________________________________________________________
block_3_expand_BN (BatchNormali (None, 40, 40, 144)  576         block_3_expand[0][0]             
__________________________________________________________________________________________________
block_3_expand_relu (ReLU)      (None, 40, 40, 144)  0           block_3_expand_BN[0][0]          
__________________________________________________________________________________________________
block_3_pad (ZeroPadding2D)     (None, 41, 41, 144)  0           block_3_expand_relu[0][0]        
__________________________________________________________________________________________________
block_3_depthwise (DepthwiseCon (None, 20, 20, 144)  1296        block_3_pad[0][0]                
__________________________________________________________________________________________________
block_3_depthwise_BN (BatchNorm (None, 20, 20, 144)  576         block_3_depthwise[0][0]          
__________________________________________________________________________________________________
block_3_depthwise_relu (ReLU)   (None, 20, 20, 144)  0           block_3_depthwise_BN[0][0]       
__________________________________________________________________________________________________
block_3_project (Conv2D)        (None, 20, 20, 32)   4608        block_3_depthwise_relu[0][0]     
__________________________________________________________________________________________________
block_3_project_BN (BatchNormal (None, 20, 20, 32)   128         block_3_project[0][0]            
__________________________________________________________________________________________________
block_4_expand (Conv2D)         (None, 20, 20, 192)  6144        block_3_project_BN[0][0]         
__________________________________________________________________________________________________
block_4_expand_BN (BatchNormali (None, 20, 20, 192)  768         block_4_expand[0][0]             
__________________________________________________________________________________________________
block_4_expand_relu (ReLU)      (None, 20, 20, 192)  0           block_4_expand_BN[0][0]          
__________________________________________________________________________________________________
block_4_depthwise (DepthwiseCon (None, 20, 20, 192)  1728        block_4_expand_relu[0][0]        
__________________________________________________________________________________________________
block_4_depthwise_BN (BatchNorm (None, 20, 20, 192)  768         block_4_depthwise[0][0]          
__________________________________________________________________________________________________
block_4_depthwise_relu (ReLU)   (None, 20, 20, 192)  0           block_4_depthwise_BN[0][0]       
__________________________________________________________________________________________________
block_4_project (Conv2D)        (None, 20, 20, 32)   6144        block_4_depthwise_relu[0][0]     
__________________________________________________________________________________________________
block_4_project_BN (BatchNormal (None, 20, 20, 32)   128         block_4_project[0][0]            
__________________________________________________________________________________________________
block_4_add (Add)               (None, 20, 20, 32)   0           block_3_project_BN[0][0]         
                                                                 block_4_project_BN[0][0]         
__________________________________________________________________________________________________
block_5_expand (Conv2D)         (None, 20, 20, 192)  6144        block_4_add[0][0]                
__________________________________________________________________________________________________
block_5_expand_BN (BatchNormali (None, 20, 20, 192)  768         block_5_expand[0][0]             
__________________________________________________________________________________________________
block_5_expand_relu (ReLU)      (None, 20, 20, 192)  0           block_5_expand_BN[0][0]          
__________________________________________________________________________________________________
block_5_depthwise (DepthwiseCon (None, 20, 20, 192)  1728        block_5_expand_relu[0][0]        
__________________________________________________________________________________________________
block_5_depthwise_BN (BatchNorm (None, 20, 20, 192)  768         block_5_depthwise[0][0]          
__________________________________________________________________________________________________
block_5_depthwise_relu (ReLU)   (None, 20, 20, 192)  0           block_5_depthwise_BN[0][0]       
__________________________________________________________________________________________________
block_5_project (Conv2D)        (None, 20, 20, 32)   6144        block_5_depthwise_relu[0][0]     
__________________________________________________________________________________________________
block_5_project_BN (BatchNormal (None, 20, 20, 32)   128         block_5_project[0][0]            
__________________________________________________________________________________________________
block_5_add (Add)               (None, 20, 20, 32)   0           block_4_add[0][0]                
                                                                 block_5_project_BN[0][0]         
__________________________________________________________________________________________________
block_6_expand (Conv2D)         (None, 20, 20, 192)  6144        block_5_add[0][0]                
__________________________________________________________________________________________________
block_6_expand_BN (BatchNormali (None, 20, 20, 192)  768         block_6_expand[0][0]             
__________________________________________________________________________________________________
block_6_expand_relu (ReLU)      (None, 20, 20, 192)  0           block_6_expand_BN[0][0]          
__________________________________________________________________________________________________
block_6_pad (ZeroPadding2D)     (None, 21, 21, 192)  0           block_6_expand_relu[0][0]        
__________________________________________________________________________________________________
block_6_depthwise (DepthwiseCon (None, 10, 10, 192)  1728        block_6_pad[0][0]                
__________________________________________________________________________________________________
block_6_depthwise_BN (BatchNorm (None, 10, 10, 192)  768         block_6_depthwise[0][0]          
__________________________________________________________________________________________________
block_6_depthwise_relu (ReLU)   (None, 10, 10, 192)  0           block_6_depthwise_BN[0][0]       
__________________________________________________________________________________________________
block_6_project (Conv2D)        (None, 10, 10, 64)   12288       block_6_depthwise_relu[0][0]     
__________________________________________________________________________________________________
block_6_project_BN (BatchNormal (None, 10, 10, 64)   256         block_6_project[0][0]            
__________________________________________________________________________________________________
block_7_expand (Conv2D)         (None, 10, 10, 384)  24576       block_6_project_BN[0][0]         
__________________________________________________________________________________________________
block_7_expand_BN (BatchNormali (None, 10, 10, 384)  1536        block_7_expand[0][0]             
__________________________________________________________________________________________________
block_7_expand_relu (ReLU)      (None, 10, 10, 384)  0           block_7_expand_BN[0][0]          
__________________________________________________________________________________________________
block_7_depthwise (DepthwiseCon (None, 10, 10, 384)  3456        block_7_expand_relu[0][0]        
__________________________________________________________________________________________________
block_7_depthwise_BN (BatchNorm (None, 10, 10, 384)  1536        block_7_depthwise[0][0]          
__________________________________________________________________________________________________
block_7_depthwise_relu (ReLU)   (None, 10, 10, 384)  0           block_7_depthwise_BN[0][0]       
__________________________________________________________________________________________________
block_7_project (Conv2D)        (None, 10, 10, 64)   24576       block_7_depthwise_relu[0][0]     
__________________________________________________________________________________________________
block_7_project_BN (BatchNormal (None, 10, 10, 64)   256         block_7_project[0][0]            
__________________________________________________________________________________________________
block_7_add (Add)               (None, 10, 10, 64)   0           block_6_project_BN[0][0]         
                                                                 block_7_project_BN[0][0]         
__________________________________________________________________________________________________
block_8_expand (Conv2D)         (None, 10, 10, 384)  24576       block_7_add[0][0]                
__________________________________________________________________________________________________
block_8_expand_BN (BatchNormali (None, 10, 10, 384)  1536        block_8_expand[0][0]             
__________________________________________________________________________________________________
block_8_expand_relu (ReLU)      (None, 10, 10, 384)  0           block_8_expand_BN[0][0]          
__________________________________________________________________________________________________
block_8_depthwise (DepthwiseCon (None, 10, 10, 384)  3456        block_8_expand_relu[0][0]        
__________________________________________________________________________________________________
block_8_depthwise_BN (BatchNorm (None, 10, 10, 384)  1536        block_8_depthwise[0][0]          
__________________________________________________________________________________________________
block_8_depthwise_relu (ReLU)   (None, 10, 10, 384)  0           block_8_depthwise_BN[0][0]       
__________________________________________________________________________________________________
block_8_project (Conv2D)        (None, 10, 10, 64)   24576       block_8_depthwise_relu[0][0]     
__________________________________________________________________________________________________
block_8_project_BN (BatchNormal (None, 10, 10, 64)   256         block_8_project[0][0]            
__________________________________________________________________________________________________
block_8_add (Add)               (None, 10, 10, 64)   0           block_7_add[0][0]                
                                                                 block_8_project_BN[0][0]         
__________________________________________________________________________________________________
block_9_expand (Conv2D)         (None, 10, 10, 384)  24576       block_8_add[0][0]                
__________________________________________________________________________________________________
block_9_expand_BN (BatchNormali (None, 10, 10, 384)  1536        block_9_expand[0][0]             
__________________________________________________________________________________________________
block_9_expand_relu (ReLU)      (None, 10, 10, 384)  0           block_9_expand_BN[0][0]          
__________________________________________________________________________________________________
block_9_depthwise (DepthwiseCon (None, 10, 10, 384)  3456        block_9_expand_relu[0][0]        
__________________________________________________________________________________________________
block_9_depthwise_BN (BatchNorm (None, 10, 10, 384)  1536        block_9_depthwise[0][0]          
__________________________________________________________________________________________________
block_9_depthwise_relu (ReLU)   (None, 10, 10, 384)  0           block_9_depthwise_BN[0][0]       
__________________________________________________________________________________________________
block_9_project (Conv2D)        (None, 10, 10, 64)   24576       block_9_depthwise_relu[0][0]     
__________________________________________________________________________________________________
block_9_project_BN (BatchNormal (None, 10, 10, 64)   256         block_9_project[0][0]            
__________________________________________________________________________________________________
block_9_add (Add)               (None, 10, 10, 64)   0           block_8_add[0][0]                
                                                                 block_9_project_BN[0][0]         
__________________________________________________________________________________________________
block_10_expand (Conv2D)        (None, 10, 10, 384)  24576       block_9_add[0][0]                
__________________________________________________________________________________________________
block_10_expand_BN (BatchNormal (None, 10, 10, 384)  1536        block_10_expand[0][0]            
__________________________________________________________________________________________________
block_10_expand_relu (ReLU)     (None, 10, 10, 384)  0           block_10_expand_BN[0][0]         
__________________________________________________________________________________________________
block_10_depthwise (DepthwiseCo (None, 10, 10, 384)  3456        block_10_expand_relu[0][0]       
__________________________________________________________________________________________________
block_10_depthwise_BN (BatchNor (None, 10, 10, 384)  1536        block_10_depthwise[0][0]         
__________________________________________________________________________________________________
block_10_depthwise_relu (ReLU)  (None, 10, 10, 384)  0           block_10_depthwise_BN[0][0]      
__________________________________________________________________________________________________
block_10_project (Conv2D)       (None, 10, 10, 96)   36864       block_10_depthwise_relu[0][0]    
__________________________________________________________________________________________________
block_10_project_BN (BatchNorma (None, 10, 10, 96)   384         block_10_project[0][0]           
__________________________________________________________________________________________________
block_11_expand (Conv2D)        (None, 10, 10, 576)  55296       block_10_project_BN[0][0]        
__________________________________________________________________________________________________
block_11_expand_BN (BatchNormal (None, 10, 10, 576)  2304        block_11_expand[0][0]            
__________________________________________________________________________________________________
block_11_expand_relu (ReLU)     (None, 10, 10, 576)  0           block_11_expand_BN[0][0]         
__________________________________________________________________________________________________
block_11_depthwise (DepthwiseCo (None, 10, 10, 576)  5184        block_11_expand_relu[0][0]       
__________________________________________________________________________________________________
block_11_depthwise_BN (BatchNor (None, 10, 10, 576)  2304        block_11_depthwise[0][0]         
__________________________________________________________________________________________________
block_11_depthwise_relu (ReLU)  (None, 10, 10, 576)  0           block_11_depthwise_BN[0][0]      
__________________________________________________________________________________________________
block_11_project (Conv2D)       (None, 10, 10, 96)   55296       block_11_depthwise_relu[0][0]    
__________________________________________________________________________________________________
block_11_project_BN (BatchNorma (None, 10, 10, 96)   384         block_11_project[0][0]           
__________________________________________________________________________________________________
block_11_add (Add)              (None, 10, 10, 96)   0           block_10_project_BN[0][0]        
                                                                 block_11_project_BN[0][0]        
__________________________________________________________________________________________________
block_12_expand (Conv2D)        (None, 10, 10, 576)  55296       block_11_add[0][0]               
__________________________________________________________________________________________________
block_12_expand_BN (BatchNormal (None, 10, 10, 576)  2304        block_12_expand[0][0]            
__________________________________________________________________________________________________
block_12_expand_relu (ReLU)     (None, 10, 10, 576)  0           block_12_expand_BN[0][0]         
__________________________________________________________________________________________________
block_12_depthwise (DepthwiseCo (None, 10, 10, 576)  5184        block_12_expand_relu[0][0]       
__________________________________________________________________________________________________
block_12_depthwise_BN (BatchNor (None, 10, 10, 576)  2304        block_12_depthwise[0][0]         
__________________________________________________________________________________________________
block_12_depthwise_relu (ReLU)  (None, 10, 10, 576)  0           block_12_depthwise_BN[0][0]      
__________________________________________________________________________________________________
block_12_project (Conv2D)       (None, 10, 10, 96)   55296       block_12_depthwise_relu[0][0]    
__________________________________________________________________________________________________
block_12_project_BN (BatchNorma (None, 10, 10, 96)   384         block_12_project[0][0]           
__________________________________________________________________________________________________
block_12_add (Add)              (None, 10, 10, 96)   0           block_11_add[0][0]               
                                                                 block_12_project_BN[0][0]        
__________________________________________________________________________________________________
block_13_expand (Conv2D)        (None, 10, 10, 576)  55296       block_12_add[0][0]               
__________________________________________________________________________________________________
block_13_expand_BN (BatchNormal (None, 10, 10, 576)  2304        block_13_expand[0][0]            
__________________________________________________________________________________________________
block_13_expand_relu (ReLU)     (None, 10, 10, 576)  0           block_13_expand_BN[0][0]         
__________________________________________________________________________________________________
block_13_pad (ZeroPadding2D)    (None, 11, 11, 576)  0           block_13_expand_relu[0][0]       
__________________________________________________________________________________________________
block_13_depthwise (DepthwiseCo (None, 5, 5, 576)    5184        block_13_pad[0][0]               
__________________________________________________________________________________________________
block_13_depthwise_BN (BatchNor (None, 5, 5, 576)    2304        block_13_depthwise[0][0]         
__________________________________________________________________________________________________
block_13_depthwise_relu (ReLU)  (None, 5, 5, 576)    0           block_13_depthwise_BN[0][0]      
__________________________________________________________________________________________________
block_13_project (Conv2D)       (None, 5, 5, 160)    92160       block_13_depthwise_relu[0][0]    
__________________________________________________________________________________________________
block_13_project_BN (BatchNorma (None, 5, 5, 160)    640         block_13_project[0][0]           
__________________________________________________________________________________________________
block_14_expand (Conv2D)        (None, 5, 5, 960)    153600      block_13_project_BN[0][0]        
__________________________________________________________________________________________________
block_14_expand_BN (BatchNormal (None, 5, 5, 960)    3840        block_14_expand[0][0]            
__________________________________________________________________________________________________
block_14_expand_relu (ReLU)     (None, 5, 5, 960)    0           block_14_expand_BN[0][0]         
__________________________________________________________________________________________________
block_14_depthwise (DepthwiseCo (None, 5, 5, 960)    8640        block_14_expand_relu[0][0]       
__________________________________________________________________________________________________
block_14_depthwise_BN (BatchNor (None, 5, 5, 960)    3840        block_14_depthwise[0][0]         
__________________________________________________________________________________________________
block_14_depthwise_relu (ReLU)  (None, 5, 5, 960)    0           block_14_depthwise_BN[0][0]      
__________________________________________________________________________________________________
block_14_project (Conv2D)       (None, 5, 5, 160)    153600      block_14_depthwise_relu[0][0]    
__________________________________________________________________________________________________
block_14_project_BN (BatchNorma (None, 5, 5, 160)    640         block_14_project[0][0]           
__________________________________________________________________________________________________
block_14_add (Add)              (None, 5, 5, 160)    0           block_13_project_BN[0][0]        
                                                                 block_14_project_BN[0][0]        
__________________________________________________________________________________________________
block_15_expand (Conv2D)        (None, 5, 5, 960)    153600      block_14_add[0][0]               
__________________________________________________________________________________________________
block_15_expand_BN (BatchNormal (None, 5, 5, 960)    3840        block_15_expand[0][0]            
__________________________________________________________________________________________________
block_15_expand_relu (ReLU)     (None, 5, 5, 960)    0           block_15_expand_BN[0][0]         
__________________________________________________________________________________________________
block_15_depthwise (DepthwiseCo (None, 5, 5, 960)    8640        block_15_expand_relu[0][0]       
__________________________________________________________________________________________________
block_15_depthwise_BN (BatchNor (None, 5, 5, 960)    3840        block_15_depthwise[0][0]         
__________________________________________________________________________________________________
block_15_depthwise_relu (ReLU)  (None, 5, 5, 960)    0           block_15_depthwise_BN[0][0]      
__________________________________________________________________________________________________
block_15_project (Conv2D)       (None, 5, 5, 160)    153600      block_15_depthwise_relu[0][0]    
__________________________________________________________________________________________________
block_15_project_BN (BatchNorma (None, 5, 5, 160)    640         block_15_project[0][0]           
__________________________________________________________________________________________________
block_15_add (Add)              (None, 5, 5, 160)    0           block_14_add[0][0]               
                                                                 block_15_project_BN[0][0]        
__________________________________________________________________________________________________
block_16_expand (Conv2D)        (None, 5, 5, 960)    153600      block_15_add[0][0]               
__________________________________________________________________________________________________
block_16_expand_BN (BatchNormal (None, 5, 5, 960)    3840        block_16_expand[0][0]            
__________________________________________________________________________________________________
block_16_expand_relu (ReLU)     (None, 5, 5, 960)    0           block_16_expand_BN[0][0]         
__________________________________________________________________________________________________
block_16_depthwise (DepthwiseCo (None, 5, 5, 960)    8640        block_16_expand_relu[0][0]       
__________________________________________________________________________________________________
block_16_depthwise_BN (BatchNor (None, 5, 5, 960)    3840        block_16_depthwise[0][0]         
__________________________________________________________________________________________________
block_16_depthwise_relu (ReLU)  (None, 5, 5, 960)    0           block_16_depthwise_BN[0][0]      
__________________________________________________________________________________________________
block_16_project (Conv2D)       (None, 5, 5, 320)    307200      block_16_depthwise_relu[0][0]    
__________________________________________________________________________________________________
block_16_project_BN (BatchNorma (None, 5, 5, 320)    1280        block_16_project[0][0]           
__________________________________________________________________________________________________
Conv_1 (Conv2D)                 (None, 5, 5, 1280)   409600      block_16_project_BN[0][0]        
__________________________________________________________________________________________________
Conv_1_bn (BatchNormalization)  (None, 5, 5, 1280)   5120        Conv_1[0][0]                     
__________________________________________________________________________________________________
out_relu (ReLU)                 (None, 5, 5, 1280)   0           Conv_1_bn[0][0]                  
==================================================================================================
Total params: 2,257,984
Trainable params: 0
Non-trainable params: 2,257,984
```

### 분류 헤드를 추가하기(Add a classification head)
특징 블록으로 부터 예측기를 생성하기 위해서 5*5 공간 위치를 평균화 한다. 평균화를 위하여 **tf.keras.layers.GlobalAveragePooling2D**를 사용하고 이 레이어는 feature들을 이미지 한장당 1280개의 요소를 가지는 벡터로 변환한다.

```
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
feature_batch_average = global_average_layer(feature_batch)
print(feature_batch_average.shape)
```
```
&gt; (32, 1280)
```
이 feature들을 이미지 하나당 하나의 예측으로 변환하기 위하여 **tf.keras.layers.Dense** 를 적용한다. 여기서 활성화 함수를 사용할 필요는 없다 왜냐하면 이 예측은 logit 또는 원시 예측 값을 가지고 있을 것이기 때문이다. 양수는 class 1을 예측하고 음수는 class 0을 예측한다
```
prediction_layer = tf.keras.layers.Dense(1)
prediction_batch = prediction_layer(feature_batch_average)
print(prediction_batch.shape)
```
```
&gt; (32, 1)
```
이제 특징 추출기와 앞의 2 layer를 **tf.keras.Sequential** 모델을 사용하여 쌓는다.
```
model = tf.keras.Sequential([
  base_model,
  global_average_layer,
  prediction_layer
])
```

### 모델 컴파일하기
너는 트레이닝 작업 전에 반드시 컴파일을 해야한다. 이 모델은 linear output(선형 출력)을 제공하기 때문에 from_logits=True 옵션으로 이진 cross-entropy 손실함수를 사용해라.
```
base_learning_rate = 0.0001
model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
```
```
model.summary()
```
```
Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
mobilenetv2_1.00_160 (Model) (None, 5, 5, 1280)        2257984   
_________________________________________________________________
global_average_pooling2d (Gl (None, 1280)              0         
_________________________________________________________________
dense (Dense)                (None, 1)                 1281      
=================================================================
Total params: 2,259,265
Trainable params: 1,281
Non-trainable params: 2,257,984
________________________________________________________________
```

MobileNet의 250만개의 파라메터는 고정되었지만, 1200개의 학습가능한 파라메터가 Dense 레이어에 남아있다. 또한 이것은 가중치와 편향으로 두개의 tf.Variable 객체로 나눌 수 있다.
```
len(model.trainable_variables)
```
```
&gt; 2
```

### 모델 학습하기
10 에폭을 학습하면 너는 96% 이하의 정확도를 얻을 수 있다.
```
initial_epochs = 10
validation_steps=20

loss0,accuracy0 = model.evaluate(validation_batches, steps = validation_steps)
```
```
20/20 [==============================] - 1s 63ms/step - loss: 0.6461 - accuracy: 0.5469
```
```
print(&quot;initial loss: {:.2f}&quot;.format(loss0))
print(&quot;initial accuracy: {:.2f}&quot;.format(accuracy0))
```
```
initial loss: 0.65
initial accuracy: 0.55
```
```
history = model.fit(train_batches,
                    epochs=initial_epochs,
                    validation_data=validation_batches)
```
```
Epoch 1/10
582/582 [==============================] - 16s 27ms/step - loss: 0.3501 - accuracy: 0.8310 - val_loss: 0.1772 - val_accuracy: 0.8964
Epoch 2/10
582/582 [==============================] - 14s 24ms/step - loss: 0.1947 - accuracy: 0.9200 - val_loss: 0.1358 - val_accuracy: 0.9243
Epoch 3/10
582/582 [==============================] - 13s 23ms/step - loss: 0.1623 - accuracy: 0.9320 - val_loss: 0.1295 - val_accuracy: 0.9304
Epoch 4/10
582/582 [==============================] - 13s 22ms/step - loss: 0.1457 - accuracy: 0.9393 - val_loss: 0.1162 - val_accuracy: 0.9368
Epoch 5/10
582/582 [==============================] - 13s 22ms/step - loss: 0.1366 - accuracy: 0.9428 - val_loss: 0.1105 - val_accuracy: 0.9424
Epoch 6/10
582/582 [==============================] - 13s 23ms/step - loss: 0.1343 - accuracy: 0.9429 - val_loss: 0.1065 - val_accuracy: 0.9445
Epoch 7/10
582/582 [==============================] - 14s 24ms/step - loss: 0.1256 - accuracy: 0.9471 - val_loss: 0.1034 - val_accuracy: 0.9475
Epoch 8/10
582/582 [==============================] - 13s 23ms/step - loss: 0.1236 - accuracy: 0.9506 - val_loss: 0.1101 - val_accuracy: 0.9458
Epoch 9/10
582/582 [==============================] - 13s 22ms/step - loss: 0.1227 - accuracy: 0.9474 - val_loss: 0.0998 - val_accuracy: 0.9523
Epoch 10/10
582/582 [==============================] - 13s 23ms/step - loss: 0.1211 - accuracy: 0.9483 - val_loss: 0.1031 - val_accuracy: 0.9514
```

## 미세 조정 ( Fine tuning )
특징 추출 실험에서 너는 MobileNet V2 base model의 상단 레이어 몇개만 학습시켰다. 사전 훈련된 네트워크의 가중치는 학습되지 않았다.

성능을 더욱 향상시키는 한 가지 방법은 우리가 추가 한 분류기의 훈련과 함께 사전 훈련 된 모델의 최상위 레이어 가중치를 훈련 (또는 &quot;미세 조정&quot;)하는 것입니다. 훈련 과정을 통해 가중치는 일반적인(generic) 특징을 포함한 맵에서 훈련하려는 데이터 집합(cats and dogs)과 관련된 기능으로 강제 조정됩니다.

* Note
사전 훈련 된 모델을 훈련 불가능으로 설정하여 최상위 분류기(classifier)를 훈련 한 뒤에만 ​시도해야합니다. 사전 훈련 된 모델 위에 무작위로 초기화 된 분류기를 추가하고 모든 레이어를 공동으로 훈련하려고하면 경사하강법 업데이트의 크기가 너무 클 수 있으며 (분류기-classifier 의 임의 가중치로 인해) 사전 훈련 된 모델은 배운 것을 잊어 버릴 수 있습니다.

또한 전체 MobileNet 모델이 아닌 소수의 최상위 계층을 미세 조정해야합니다. 대부분의 convolution 네트워크에서 계층이 높을수록 계층이 더 전문화됩니다. 처음 몇 층은 거의 모든 유형의 이미지로 일반화되는 매우 간단하고 일반적인 기능을 학습합니다. 더 높은 수준으로 올라가면 기능이 모델이 훈련 된 데이터 세트에 점점 더 구체적이됩니다. 미세 조정(fine tuning)의 목표는 이러한 특수 기능을 일반 학습을 덮어 쓰지 않고 새 데이터 세트와 함께 사용할 수 있도록 조정하는 것입니다.

### 모델의 상단 레이어 고정 해제
base_model 고정을 해제하고 bottom 레이어를 훈련 할 수 없도록 설정하기 만하면됩니다. 그런 다음 모델을 다시 컴파일하고 훈련을 다시 시작해야합니다.
```
base_model.trainable = True
```
```
# base model에 얼마나 많은 layer가 존재하는지 확인한다
print(&quot;Number of layers in the base model: &quot;, len(base_model.layers))

# 미세조정을 시작할 레이어 위치를 정한다
fine_tune_at = 100

# 미세조정을 시작하기 전 레이어들은 다 고정한다
for layer in base_model.layers[:fine_tune_at]:
  layer.trainable =  False
```
```
&gt; Number of layers in the base model:  155
```

### 모델 컴파일하기
더 낮은 학습률(learning rate)를 적용하여 모델을 컴파일한다
```
model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate/10),
              metrics=['accuracy'])
```
```
model.summary()
```
```
Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
mobilenetv2_1.00_160 (Model) (None, 5, 5, 1280)        2257984   
_________________________________________________________________
global_average_pooling2d (Gl (None, 1280)              0         
_________________________________________________________________
dense (Dense)                (None, 1)                 1281      
=================================================================
Total params: 2,259,265
Trainable params: 1,863,873
Non-trainable params: 395,392
_________________________________________________________________
```
```
len(model.trainable_variables)
```
```
58
```

### 계속해서 모델 학습시키기
```
fine_tune_epochs = 10
total_epochs =  initial_epochs + fine_tune_epochs

history_fine = model.fit(train_batches,
                         epochs=total_epochs,
                         initial_epoch =  history.epoch[-1],
                         validation_data=validation_batches)
```

## 요약
* 특징 추출을 위해 사전 훈련 된 모델 사용 : 작은 데이터 세트로 작업 할 때 동일한 도메인에서 더 큰 데이터 세트에 대해 훈련 된 모델에서 학습 한 기능을 활용하는 것이 일반적입니다. 사전 훈련 된 모델을 인스턴스화하고 완전히 연결된 분류기를 맨 위에 추가하면됩니다. 사전 훈련 된 모델은 &quot;동결&quot;되고 분류기의 가중치 만 훈련 중에 업데이트됩니다. 이 경우 컨벌루션베이스는 각 이미지와 관련된 모든 기능을 추출했으며 추출 된 기능 세트가 제공된 이미지 클래스를 결정하는 분류기를 훈련했습니다.

* 사전 훈련 된 모델 미세 조정 : 성능을 더욱 향상시키기 위해 사전 훈련 된 모델의 최상위 계층을 미세 조정을 통해 새로운 데이터 세트로 재사용 할 수 있습니다. 이 경우 모델이 데이터 세트와 관련된 고급 기능을 학습 할 수 있도록 가중치를 조정했습니다. 이 기술은 일반적으로 훈련 데이터 세트가 크고 사전 훈련 된 모델이 훈련 된 원래 데이터 세트와 매우 유사한 경우에 권장됩니다.</description>
            <pubDate>Thu, 23 Apr 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/image_transfer_learning_preTrained.html</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/image_transfer_learning_preTrained.html</guid>
            
            
        </item>
        
        <item>
            <title>About</title>
            <description># 차곡차곡 쌓아가는 기술블로그입니다
&lt;hr/&gt;

## 귀여운 반려묘와 함께 지내고 있습니다
&lt;img src=&quot;/assets/images/java.jpg&quot; width=&quot;40%&quot; height=&quot;30%&quot;/&gt;

### 한쪽만 파기보단 다양한 기술에 관심이 많고 도큐먼트를 좋아합니다
### email : kdwooa@gmail.com
</description>
            <pubDate>Thu, 23 Apr 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/</link>
            <guid isPermaLink="true">https://east-rain.github.io/</guid>
            
            
        </item>
        
        <item>
            <title>PoseNet</title>
            <description></description>
            <pubDate>Wed, 06 May 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/posenet/</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/posenet/</guid>
            
            
        </item>
        
        <item>
            <title>Programming</title>
            <description># Programming</description>
            <pubDate>Thu, 23 Apr 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Programming/</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Programming/</guid>
            
            
        </item>
        
        <item>
            <title>Deep Learning</title>
            <description></description>
            <pubDate>Thu, 23 Apr 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/</guid>
            
            
        </item>
        
        <item>
            <title>Openpose</title>
            <description>https://github.com/ildoonet/tf-pose-estimation.git

해당 repository를 통해 개의 신체 자세를 추정하는 모델을 개발하였다.

https://arvrjourney.com/human-pose-estimation-using-openpose-with-tensorflow-part-2-e78ab9104fc8

그리고 해당 블로그에서 openpose 구현에 대한 많은것을 배울 수 있었다.

본래 Openpose는 human pose를 추정하는 알고리즘이다. CMU의 오리지날 논문 구현에서는 Caffe를 사용했지만, 해당 프로젝트에서는 텐서플로를 이용해 구현하였다.


### Requirements
* python3
* tensorflow 1.4.1+
* opencv3, protobuf, python3-tk
* slidingwindow

### Install
1. 해당 repository를 받는다.
2. pip install -r requirements.txt 를 통해 필요한 패키지를 설치한다
3. tf_pose/pafprocess 로 이동한다
4. swig -python -c++ pafprocess.i &amp;&amp; python3 setup.py build_ext --inplace 로 c++ 코드를 python으로 치환해 준다.


해당 구현의 경우 인간 자세 Coco dataset의 keypoint 라벨링에 맞게 트레이닝을 하고 추론이 되도록 되어있다. 따라서 인간의 keypoint 개수와 keypoint pair를 강아지에 맞개 개수를 변경해줘야 한다. 또한 repository가 관리된지 조금 오래 지나서 commit이 이루어 지지 않고 다른 버전의 tensorflow와 cuda version에서 동작이 안되는 부분이 있어서 부분부분 수정이 필요하다.


수정해야하는 부분을 지금부터 명시하겠다.


coco dataset의 인간의 키포인트는 총 18개이다.(background까지 하면 19개).
내가 정의해준 강아지의 신체포인트는 총 32개이다.(background까지 하면 33개)

```
# common.py

LEye = 0
REye = 1
Nose = 2
LMouth = 3
RMouth = 4
MouthRow = 5
Tongue = 6
LEarStart = 7
LEarMiddle = 8
LEarEnd = 9
REarStart = 10
REarMiddle = 11
REarEnd = 12
Withers = 13
Prosternum = 14
LElbow = 15
LForearm = 16
LForefoot = 17
RElbow = 18
RForearm = 19
RForefoot = 20
LStiffle = 21
LHock = 22
LHindfoot = 23
RStiffle = 24
RHock = 25
RHindfoot = 26
TailStart = 27
TailStartMiddle = 28
TailMiddle = 29
TailMiddleEnd = 30
TailEnd = 31
Background = 32

# 신체 포인트를 이어주는 pair는 다음과 같다. CocoPairRender는 모두 표시해주고 싶어서 원래 있던 [:-2]를 제외하였다.

CocoPairs = [
    (2, 0), (0, 7), (7, 8), (8, 9), (2, 1), (1, 10), (10, 11), (11, 12), (2, 5), (5, 3),
    (5, 4), (5, 6), (2, 13), (2, 14), (13, 15), (14, 15), (15, 16), (16, 17), (13, 18), (14, 18),
    (18, 19), (19, 20), (13, 27), (14, 27), (27, 21), (21, 22), (22, 23), (27, 24), (24, 25), (25, 26),
    (27, 28), (28, 29), (29, 30), (30, 31)
]   # = 34
CocoPairsRender = CocoPairs

CocoColors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [255, 255, 0], [255, 255, 0], [255, 255, 0],
              [170, 255, 0], [170, 255, 0], [170, 255, 0], [85, 255, 0], [85, 255, 0], [85, 255, 0],
              [0, 255, 0], [0, 255, 85], [0, 255, 170], [0, 255, 170], [0, 255, 170],
              [0, 255, 255], [0, 255, 255], [0, 255, 255], [0, 170, 255], [0, 170, 255], [0, 170, 255],
              [0, 85, 255], [0, 85, 255], [0, 85, 255],
              [0, 0, 255], [0, 0, 255], [0, 0, 255], [0, 0, 255], [0, 0, 255], [85, 0, 255], [170, 0, 255]]

```
```
# estimator.py

# 머리 박스 쳐주는 것을 강아지 신체포인트에 맞게 변경
def get_face_box(self, img_w, img_h, mode=0):
    _NOSE = CocoPart.Nose.value
    _NECK = CocoPart.MouthRow.value
    _REye = CocoPart.REye.value
    _LEye = CocoPart.LEye.value
    _REar = CocoPart.REarEnd.value
    _LEar = CocoPart.LEarEnd.value

# 부분부분 숫자가 18로 되어있는 것들을 32로 바꿔줘야 한다. 사람 신체포인트는 18개, 그리고 정의한 강아지 신체포인트는 32개이기 때문.
for part_idx in range(32):

# 그리고 heatMat과 pafMat 부분이 19로 되어있는데 해당 부분은 34로 변경해준다
self.tensor_heatMat = self.tensor_output[:, :, :, :34]
self.tensor_pafMat = self.tensor_output[:, :, :, 34:]
self.upsample_size = tf.placeholder(dtype=tf.int32, shape=(2,), name='upsample_size')
self.tensor_heatMat_up = tf.image.resize_area(self.tensor_output[:, :, :, :34], self.upsample_size,
                                                      align_corners=False, name='upsample_heatmat')
self.tensor_pafMat_up = tf.image.resize_area(self.tensor_output[:, :, :, 34:], self.upsample_size,
                                                     align_corners=False, name='upsample_pafmat')
if trt_bool is True:
    smoother = Smoother({'data': self.tensor_heatMat_up}, 25, 3.0, 34)
else:
    smoother = Smoother({'data': self.tensor_heatMat_up}, 25, 3.0)
```
```
# network_mobilenet_v2.py
# 프로젝트 진행에서 backbone 모델은 가장 만만한 mobilenet_v2를 이용하였다. 해당 부분 또한 신체포인트 개수가 다르기에 노드의 개수를 변경해 줘야 한다.

# 38 -&gt; 68으로
# 19 -&gt; 34로
```
```
# pose_augment.py
트레이닝 시에 데이터 augmentation을 위해서 변경된 신체포인트를 명시해 주자. 일반적인 경우는 상관없지만 flip같은 경우에 신체포인트 좌우가 바뀌어야 한다

# flip meta
    flip_list = [CocoPart.REye, CocoPart.LEye, CocoPart.Nose, CocoPart.RMouth, CocoPart.LMouth, CocoPart.MouthRow, CocoPart.Tongue, CocoPart.REarStart,
                 CocoPart.REarMiddle, CocoPart.REarEnd, CocoPart.LEarStart, CocoPart.LEarMiddle, CocoPart.LEarEnd, CocoPart.Withers,
                 CocoPart.Prosternum, CocoPart.RElbow, CocoPart.RForearm, CocoPart.RForefoot, CocoPart.LElbow, CocoPart.LForearm, CocoPart.LForefoot,
                 CocoPart.RStiffle, CocoPart.RHock, CocoPart.RHindfoot, CocoPart.LStiffle, CocoPart.LHock, CocoPart.LHindfoot, CocoPart.TailStart,
                 CocoPart.TailStartMiddle, CocoPart.TailMiddle, CocoPart.TailMiddleEnd, CocoPart.TailEnd, CocoPart.Background]

```
```
# pose_dataset.py
__coco_parts = 34
__coco_vecs = list(zip(
        [3, 1, 8, 9,  3,  2, 11, 12, 3, 6, 6, 6,  3,  3, 14, 15, 16, 17, 14, 15, 19, 20, 14, 15, 28, 22, 23, 28, 25, 26, 28, 29, 30, 31],
        [1, 8, 9, 10, 2, 11, 12, 13, 6, 4, 5, 7, 14, 15, 16, 16, 17, 18, 19, 19, 20, 21, 28, 28, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32]
    ))

transform = list(zip(
            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32],
            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]
        ))

# 여기서 기본으로 제공되는 데이터셋이 아닌 커스텀 데이터셋 명시해줌
if is_train:
    whole_path = os.path.join(path, 'train.json')
else:
    whole_path = os.path.join(path, 'test.json')
    self.img_path = (img_path if img_path is not None else '') + ('/train/' if is_train else '/test/')

# tensorflow 버전에 따라서 수정해줘야 하는 경우도 있음
ds = MultiThreadMapData(ds, num_thread=16, map_func=read_image_url, buffer_size=1000)
```
```
# train.py

# vectmap_node의 기존 38 -&gt; 68로, heatmap_node는 기존 19 -&gt; 34로
vectmap_node = tf.placeholder(tf.float32, shape=(args.batchsize, output_h, output_w, 68), name='vectmap')
heatmap_node = tf.placeholder(tf.float32, shape=(args.batchsize, output_h, output_w, 34), name='heatmap')

그리고 트레이닝이 일정부분 지속 된 후 validation을 해주는 부분이 있다. 여기서 아마 계속 오류가 발생할 것이다. df_valid.reset_state() 시점에서 에러가 날것이다. 그냥 맘 편하게 세션이 시작하기 전에 validation 이미지들을 메모리에 올려놓는 것이 정신건강에 이롭다.

# 해당 코드를 with tf.Session(config=config) as sess: 위로 올려놓자. 그러면 밑에서 validation_cache가 존재하기 때문에 다시 불러오려고 노력하지 않는다.
for images_test, heatmaps, vectmaps in tqdm(df_valid.get_data()):
        validation_cache.append((images_test, heatmaps, vectmaps))
del df_valid
df_valid = None

pafMat, heatMat = outputMat[:, :, :, 34:], outputMat[:, :, :, :34]
```

거의 다 왔다. 그리고 openpose 논문을 보면 알겠지만 각 지점의 추정 히트맵 부분을 연결해주는 것이 필요하다. 해당 부분은 속도 때문인지 python으로 구현하지 않고 c++ 코드로 구현이 되어있다. 신체포인트와 페어의 개수가 다르므로 이 부분 또한 변경을 해줘야 한다.

tf_pose/pafprocess (해당 부분을 수정 후에는 swig를 통해 build를 다시 해줘야 한다) 에 존재하는 파일들을 수정해주자
```
# pafprocess.cpp
if (found == 1) {
    if (subset[subset_idx1][part_id2] != conns[conn_id].cid2) {
    subset[subset_idx1][part_id2] = conns[conn_id].cid2;
    subset[subset_idx1][33] += 1;
    subset[subset_idx1][32] += peak_infos_line[conns[conn_id].cid2].score + conns[conn_id].score;
    }
} else if (found == 2) {
    int membership=0;
    for (int subset_id = 0; subset_id &lt; 32; subset_id ++) {
        if (subset[subset_idx1][subset_id] &gt; 0 &amp;&amp; subset[subset_idx2][subset_id] &gt; 0) {
            membership = 2;
        }
    }

    if (membership == 0) {
        for (int subset_id = 0; subset_id &lt; 32; subset_id ++) subset[subset_idx1][subset_id] += (subset[subset_idx2][subset_id] + 1);

            subset[subset_idx1][33] += subset[subset_idx2][33];
            subset[subset_idx1][32] += subset[subset_idx2][32];
            subset[subset_idx1][32] += conns[conn_id].score;
            subset.erase(subset.begin() + subset_idx2);
        } else {
            subset[subset_idx1][part_id2] = conns[conn_id].cid2;
            subset[subset_idx1][33] += 1;
            subset[subset_idx1][32] += peak_infos_line[conns[conn_id].cid2].score + conns[conn_id].score;
        }
} else if (found == 0) {
    vector&lt;float&gt; row(34);
    for (int i = 0; i &lt; 34; i ++) row[i] = -1;
    row[part_id1] = conns[conn_id].cid1;
    row[part_id2] = conns[conn_id].cid2;
    row[33] = 2;
    row[32] = peak_infos_line[conns[conn_id].cid1].score + peak_infos_line[conns[conn_id].cid2].score + conns[conn_id].score;
    subset.push_back(row);
}


float get_score(int human_id) {
    return subset[human_id][32] / subset[human_id][33];
}
```
```
# pafprocess.h
const float THRESH_HUMAN_SCORE = 0.1;
const int NUM_PART = 32;

const int STEP_PAF = 10;

const int COCOPAIRS_SIZE = 34;
const int COCOPAIRS_NET[COCOPAIRS_SIZE][2] = {
    {0, 1}, {2, 3}, {4, 5}, {6, 7}, {8, 9}, {10, 11}, {12, 13}, {14, 15}, {16, 17}, {18, 19},
    {20, 21}, {22, 23}, {24, 25}, {26, 27}, {28, 29}, {30, 31}, {32, 33}, {34, 35}, {36, 37}, {38, 39},
    {40, 41}, {42, 43}, {44, 45}, {46, 47}, {48, 49}, {50, 51}, {52, 53}, {54, 55}, {56, 57}, {58, 59},
    {60, 61}, {62, 63}, {64, 65}, {66, 67}
};

const int COCOPAIRS[COCOPAIRS_SIZE][2] = {
    {2, 0}, {0, 7}, {7, 8}, {8, 9}, {2, 1}, {1, 10}, {10, 11}, {11, 12}, {2, 5}, {5, 3},
    {5, 4}, {5, 6}, {2, 13}, {2, 14}, {13, 15}, {14, 15}, {15, 16}, {16, 17}, {13, 18}, {14, 18},
    {18, 19}, {19, 20}, {13, 27}, {14, 27}, {27, 21}, {21, 22}, {22, 23}, {27, 24}, {24, 25}, {25, 26},
    {27, 28}, {28, 29}, {29, 30}, {30, 31}
};
```
</description>
            <pubDate>Wed, 28 Oct 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/Openpose/</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/Openpose/</guid>
            
            
        </item>
        
        <item>
            <title>DeepLabCut</title>
            <description></description>
            <pubDate>Wed, 06 May 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/deeplabcut/</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/deeplabcut/</guid>
            
            
        </item>
        
        <item>
            <title>딥러닝 기초</title>
            <description></description>
            <pubDate>Thu, 23 Apr 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/basic%20deeplearning/</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/basic%20deeplearning/</guid>
            
            
        </item>
        
        <item>
            <title>텐서플로우 튜토리얼</title>
            <description></description>
            <pubDate>Thu, 23 Apr 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/</guid>
            
            
        </item>
        
        <item>
            <title>Yolo(darknet)</title>
            <description></description>
            <pubDate>Wed, 28 Oct 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/Yolo(darknet)/</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/Yolo(darknet)/</guid>
            
            
        </item>
        
        <item>
            <title>이미지 유사도 검색</title>
            <description>https://github.com/facebookresearch/faiss.git

facebook에서 공개한 Faiss를 이용하여 이미지 유사도를 검색하였다. 

### Faiss란?
Faiss는 밀도가 높은 벡터의 효율적인 유사성 검색 및 클러스터링을위한 라이브러리입니다. 여기에는 RAM에 맞지 않을 수있는 최대 크기의 벡터 세트에서 검색하는 알고리즘이 포함되어 있습니다. 평가 및 매개 변수 조정을위한 지원 코드도 포함되어 있습니다. Python / numpy를 위한 완전한 래퍼와 함께 C ++로 작성되었습니다. 가장 유용한 알고리즘 중 일부는 GPU에서 구현됩니다. Facebook AI Research에서 개발했습니다.


다양한 백터 유사도 검색 알고리즘들을 구현해놓고 쓰기 쉽게 말아 놓았다. 그 중에서 HNSW(Hierarchical Navigable Small World graphs)를 이용해서 이미지 유사도를 검색하였다.


백터 그래프를 그리는 코드는 다음과 같다
```
import struct
import glob
import numpy as np
import os
import tensorflow as tf
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
import tensorflow.keras.layers as layers
from tensorflow.keras.models import Model

def preprocess(img_path, input_shape):
    img = tf.io.read_file(img_path)
    img = tf.image.decode_jpeg(img, channels=input_shape[2])
    img = tf.image.resize(img, input_shape[:2])
    img = preprocess_input(img)
    return img

def main():
    batch_size = 100
    input_shape = (224, 224, 3)
    base = tf.keras.applications.MobileNetV2(input_shape=input_shape,
                                             include_top=False,
                                             weights='imagenet')
    base.trainable = False
    model = Model(inputs=base.input, outputs=layers.GlobalAveragePooling2D()(base.output))


    root_dir = &quot;이미지 존재하는 루트 디렉토리&quot;
    images = []
    for(dirpath, dirnames, filenames) in os.walk(root_dir):
        for filename in filenames:
            if &quot;.png&quot; in filename or &quot;.jpg&quot; in filename:
                images.append(dirpath + &quot;/&quot; + filename)
    fnames = images

    list_ds = tf.data.Dataset.from_tensor_slices(fnames)
    ds = list_ds.map(lambda x: preprocess(x, input_shape), num_parallel_calls=-1)
    dataset = ds.batch(batch_size).prefetch(-1)

    with open('fvecs.bin', 'wb') as f:
        for batch in dataset:
            fvecs = model.predict(batch)

            fmt = f'{np.prod(fvecs.shape)}f'
            f.write(struct.pack(fmt, *(fvecs.flatten())))

    with open('fnames.txt', 'w') as f:
        f.write('\n'.join(fnames))

if __name__ == '__main__':
    main()
```

해당 코드를 통해 백터 그래프를 그리면 다음과 같이 3개의 파일이 생성된다. fnames.txt, fvecs.bin, fvecs.bin.hnsw.index

그리고 이제 이 파일들을 이용해서 유사도를 검색할 수 있다.

```
import os
import time
import math
import random
import numpy as np
import json
from sklearn.preprocessing import normalize
import faiss
import csv


def dist2sim(d):
    return 1 - d / 2
    

def get_index(index_type, dim):
    if index_type == 'hnsw':
        m = 48
        index = faiss.IndexHNSWFlat(dim, m)
        index.hnsw.efConstruction = 128
        return index
    elif index_type == 'l2':
        return faiss.IndexFlatL2(dim)
    raise


def populate(index, fvecs, batch_size=1000):
    nloop = math.ceil(fvecs.shape[0] / batch_size)
    for n in range(nloop):
        s = time.time()
        index.add(normalize(fvecs[n * batch_size : min((n + 1) * batch_size, fvecs.shape[0])]))
        print(n * batch_size, time.time() - s)

    return index


def find_file_name(idx):
	if idx == -1:
		return 'None'
	with open('fnames.txt', 'r') as f:
		names = f.readlines()
	return names[idx].strip('\n').strip('\t')

def main():
    dim = 1280
    fvec_file = 'fvecs.bin'
    index_type = 'hnsw'
    #index_type = 'l2'

	# f-string 방식 (python3 이상에서 지원)
    index_file = f'{fvec_file}.{index_type}.index'

    fvecs = np.memmap(fvec_file, dtype='float32', mode='r').view('float32').reshape(-1, dim)

    if os.path.exists(index_file):
        index = faiss.read_index(index_file)
        if index_type == 'hnsw':
            index.hnsw.efSearch = 256
    else:
        index = get_index(index_type, dim)
        index = populate(index, fvecs)
        faiss.write_index(index, index_file)
    print(index.ntotal)

    random.seed(2020)
    
	# random하게 쿼리 인덱스를 생성한다
    # 0부터 데이터 갯수 사이의 인덱스
    q_idx = [random.randint(0, fvecs.shape[0]) for _ in range(100)]
    q_idx = np.arange(0, 1)
    k = 10
    s = time.time()
    
    total = 0
    csv_file = open('result.csv', 'w', encoding='utf-8-sig', newline='')
    wr = csv.writer(csv_file)
    for source in range(index.ntotal):
    	q_idx = [source]
    	dists, idxs = index.search(normalize(fvecs[q_idx]), k)
    	sim_files = []
    	sim_scores = []
    	for i, idx in enumerate(idxs[0]):
    		if dists[0][i] &lt;= 0.3 and idx != source:
    			sim_scores.append(dists[0][i])
    			sim_files.append(find_file_name(idx))
    			total = total + 1

    	if len(sim_files) &gt; 0:
    		wr.writerow([find_file_name(source)])
    		for i, file in enumerate(sim_files):
    			wr.writerow([sim_scores[i], file])

    print(&quot;length = &quot; + str(len(sim_files)))
    csv_file.close()
    print(total)
  
if __name__ == '__main__':
    main()
```

해당 코드를 돌리면 fnames.txt에 존재하는 모든 파일들을 읽어와서 백터 유사도 검색을 해서 이미지의 유사도를 result.csv파일에 모두 뛀궈준다.

그리고 그래프를 그리고 찾는 두개의 코드를 이용해 실시간으로 이미지의 유사도를 검색할 수 있는 시스템을 구성할 수도 있다.

&lt;img src=&quot;similary.png&quot;/&gt;</description>
            <pubDate>Wed, 28 Oct 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/vector%20search/</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/vector%20search/</guid>
            
            
        </item>
        
        <item>
            <title>텐서플로우 설치 - windows</title>
            <description># 텐서플로우 설치 - windows

1. Python 3 설치 - anaconda 사용&lt;br&gt;
https://www.anaconda.com/distribution/ 에서 anaconda installer 다운로드 및 설치

2. NVIDIA GPU driver 설치&lt;br&gt;
https://www.nvidia.co.kr/Download/index.aspx?lang=kr

3. NVIDIA CUDA Toolkit 설치&lt;br&gt;
https://developer.nvidia.com/cuda-downloads

4. cuDDN 설치&lt;br&gt;
https://developer.nvidia.com/cudnn&lt;br&gt;
다운받은 압축파일을 적당한 위치에 푼다.
여기서는 C:\tools/cuda 에 풀겠다.

5. 환경변수 추가&lt;br&gt;
C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\bin;&lt;br&gt;
C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\v10.2\\extras\\CUPTI\\libx64;&lt;br&gt;
C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\include;&lt;br&gt;
C:\\tools\\cuda\\bin;&lt;br&gt;
해당 경로들을 환경변수에 추가한다.

6. 텐서플로우 설치 - pip&lt;br&gt;
```
$ pip install tensorflow  // 텐서플로우2 버전부터는 해당 명령어로 CPU와 GPU를 모두 설치해준다
```

7. 설치 확인&lt;br&gt;
```
$ python3 -c &quot;import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))&quot;
&gt; 2020-04-21 20:36:09.675162: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 tf.Tensor(-342.69507, shape=(), dtype=float32)
```
결과가 나오면 설치가 잘 된것이다.
위에 메시지는 내 cpu는 더 많은 명령어를 지원하지만, pip로 받은 텐서플로우 빌드 버전에서는 해당 명령어를 사용하지 못한다는 의미이다. 만약 해당 메시지가 싫거나 좀 더 빠른 동작을 원한다면 텐서플로우 소스를 받아서 빌드하면 된다.
</description>
            <pubDate>Thu, 23 Apr 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/install_tensorflow.html</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/install_tensorflow.html</guid>
            
            
        </item>
        
        <item>
            <title>개요</title>
            <description># Darknet을 통하여 YoloV4 사용하기
#### https://github.com/AlexeyAB/darknet
해당 문서는 위 repository의 darknet이란 프레임워크를 이용하여 CNN 모델 중 하나인 YoloV4를 학습해보고 실행해보는 것을 목적으로 하고있다.


YoloV4 알고리즘 간단 설명(작성 중)</description>
            <pubDate>Wed, 28 Oct 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/Yolo(darknet)/introduction.html</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/Yolo(darknet)/introduction.html</guid>
            
            
        </item>
        
        <item>
            <title>개요</title>
            <description># DeepLabCut 개요 번역
#### http://www.mousemotorlab.org/deeplabcut

#### DeepLabCut : 동물 포즈 추정을 위한 소프트웨어 패키지

DeepLabCut은 3D 마커가 없는 자세 추정에 효율적인 방법을 제공한다. DeepLabCut은 멋진 결과를 달성한 *deep neural networks*에 기반한 전이학습을 가능하게 해준다. (DeepLabCut에서는 일반적으로 50~200 프레임만 있으면 최소한의 학습 데이터를 마련한 것이다).
DeepLabCut은 다양한 행동에 거쳐 다양한 종들의 다양한 바디 파트를 트랙킹할 수 있다.

이 패키지는 오픈 소스이고, 빠르고, 활동적이고 3D 포즈 측정 계산에 사용할 수 있다. 
원본 논문과 최근 연구 동향을 확인할 수 있다.



</description>
            <pubDate>Thu, 23 Apr 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/deeplabcut/introduction.html</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/deeplabcut/introduction.html</guid>
            
            
        </item>
        
        <item>
            <title>개요</title>
            <description># PoseNet 개요 번역
#### https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5

### TensorFlow.js를 사용하여 브라우저에서 실시간 인간 자세 추정하기

Google Creative Lab과 협업하여, PoseNet의 TensorFlow.js 버전 릴리즈을 알리게되어 흥분된다. 이 머신 러닝 모델은 브라우저에서 실시간 인간 자세 추정을 가능하게 한다.

*그래서 자세 추정(pose estimation)이 뭔데?* 자세 추정은 컴퓨터 비전분야에서 비디오나 이미지의 사람 모양을 탐지하는 것을 나타낸다. 예를들어 누군가의 팔꿈치가 이미지 어디에 위치하는지 나타낸다. 명백하게 이 기술은 이미지에 있는 사람이 누구인지 인식하지 못한다. 이 알고리즘은 단순하게 몸의 위치 정보를 추정할 뿐이다.

*좋아, 이게 왜 흥미진진한데?* 자세 추정은 다양한 쓰임세가 있다. 우리는 더 많은 개발자가 자신의 고유한 프로젝트에 자세 추정을 실험하고 적용할 수 있기를 바란다. 대부분의 자세 추정 시스템이 오픈 소스지만, 시스템 셋업 또는 카메라 같은 몇가지 특별한 설정이 필요하다. TensorFlow.js에서 PoseNet을 실행하여 웹캠이 달린 데스크탑이나 핸드폰에서 곧바로 체험해 볼 수 있다. TensorFlow.js의 PoseNet은 브라우저에서 동작하기 때문에 사용자의 컴퓨터에 어떠한 포즈 데이터도 남기지 않는다.

이 모델을 사용하는 방법에 대해서 깊게 파고들어가기전에, 이 프로젝트를 가능하게 만들어준 분들에게 감사한다. George Papandreou and Tyler Zhu, Google researchers behind the papers Towards Accurate Multi-person Pose Estimation in the Wild and PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model, and Nikhil Thorat and Daniel Smilkov, engineers on the Google Brain team behind the TensorFlow.js library.

### PoseNet 시작하기
PoseNet은 싱글 포즈와 멀티플 포즈를 추측할 수 있다. 이 의미는 알고리즘의 버전에 따라서 한사람만 추정하거나 여러 사람을 추정할 수 있다는 것이다. 왜 버전이 두 개로 나뉘냐? 한사람의 포즈를 디텍트 하는 것이 더 빠르고 간단하다 그러나 이미지에 한가지의 객체만 있어야 한다. 우리는 싱글-포즈 모델을 먼저 알아볼 것이다. 왜냐하면 더 쉬우니까

고수준의 자세 추정은 두개의 페이즈로 나뉜다.
1. convolutional neural network를 통해 RGB 이미지가 공급된다
2. 싱글-포즈 또는 멀티-포즈 디코딩 알고리즘은 다음과 같은 것들을 모델의 아웃풋에서 해독하는데 사용횐다. pose, pose 신뢰 점수, Keypoint 위치 그리고 Keypoint 신뢰 점수

그럼 이러한 키워드가 무엇을 의미하는지 보자
* pose - 가장 높은 레벨에서 PoseNet은 키포인트의 리스트와 각각의 detect 된 사람의 인스턴스 레벨의 신뢰 점수를 포함한 pose object를 리턴한다

&lt;img src=&quot;https://miro.medium.com/max/1400/1*3bg3CO1b4yxqgrjsGaSwBw.png&quot;/&gt;*PoseNet returns confidence values for each person detected as well as each pose keypoint detected. Image Credit: “Microsoft Coco: Common Objects in Context Dataset”, https://cocodataset.org.*

* Pose 신뢰 점수 - 이것은 자세의 추정에서 신뢰 점수 전체를 나타낸다. 이것의 범위는 0.0에서 1.0 까지이다. 이 점수는 충분한 점수를 얻지 못한 포즈를 없애는데 사용될 수 있다.

* KeyPoint - 코, 오른쪽 귀, 왼쪽 무릎, 오른쪽 발 등등 추정된 사람 자세의 부분. PoseNet은 현재 17개의 keypoint를 디텍할 수 있다. 밑에 그 예가 있다.

&lt;img src=&quot;https://miro.medium.com/max/1400/1*7qDyLpIT-3s4ylULsrnz8A.png&quot;/&gt;

* KeyPoint 신뢰 점수 - 이것은 추정한 keypoint 위치가 정확한지를 나타낸다. 이것의 범위는 0.0 부터 1.0 까지이다. 충분한 점수를 얻지 못한 keypoint를 지우는데 사용할 수 있다.

* Keypoint Position - 인풋 이미지에서 키포인트가 디텍 된 x,y 좌표를 나타낸다.



#### Part 1: TensorFlow.js와 PoseNet 라이브러리 임포트하기
모델의 복잡성을 추상화하고 기능을 사용하기 쉬운 방법으로 캡슐화하는 많은 작업이 진행되었다. PoseNet 프로젝트 설정 방법의 기본 사항을 살펴보자.

이 라이브러리는 npm으로 설치할 수 있다
```
npm install @tensorflow-models/posenet
```

그리고 모듈을 가져온다
```
import * as posenet from '@tensorflow-models/posenet';
const net = await posenet.load();
```
또는 페이지에 껴넣는다
```
&lt;html&gt;
  &lt;body&gt;
    &lt;!-- Load TensorFlow.js --&gt;
    &lt;script src=&quot;https://unpkg.com/@tensorflow/tfjs&quot;&gt;&lt;/script&gt;
    &lt;!-- Load Posenet --&gt;
    &lt;script src=&quot;https://unpkg.com/@tensorflow-models/posenet&quot;&gt;
    &lt;/script&gt;
    &lt;script type=&quot;text/javascript&quot;&gt;
      posenet.load().then(function(net) {
        // posenet model loaded
      });
    &lt;/script&gt;
  &lt;/body&gt;
&lt;/html&gt;
```

#### Part 2a: single-person 자세 추정

&lt;img src=&quot;https://miro.medium.com/max/1026/1*SpWPwprVuNYhXs44iTSODg.png&quot;/&gt;*Example single-person pose estimation algorithm applied to an image. Image Credit: “Microsoft Coco: Common Objects in Context Dataset”, https://cocodataset.org.*

시작하기 전에 single-pose 추정 알고리즘은 2개를 추정하는거보다 간단하고 쉽다. 이것은 이미지나 비디오에서 오직 한 사람만이 이미지의 중앙에 위치할 때 가장 이상적이다. 불이익은 만약 많은 사람이 이미지에 존재한다면 각각의 사람들의 키포인트가 섞여서 추정될 수 있다. 만약 많은 사람이 이미지에 등장한다면 multi-pose 추정 알고리즘을 사용해라

single-pose 추정 알고리즘의 inputs을 알아보자
* Input image element - 자세를 추정할 이미지를 가지고 있는 html 요소이다. 보통 img나 video tag를 가지고 있다. 그리고 이미지나 비디오는 사각형 형태여야 한다.

* Image scale factor - 0.2부터 1까지의 값을 가지고 기본값은 0.5이다. 네트워크를 통해 이미지를 공급하기 전에 이 값을 낮게 설정하면 이미지를 축소하고 정확도를 포기하여 네트워크의 속도를 높일 수 있다.

* Flip horizontal - 기본값은 false이다. 만약 포즈를 뒤집거나 대칭시켜야 하는 경우 이 값을 true로 설정하면 된다.

* Output stride - 32,16 또는 8이어야 하고 기본값은 16이다. 내부적으로 이 파라메터는 신경망 안에 레이어의 높이와 넓이에 영향을 준다. 고차원적인 수준에서는 이것은 정확도와 속도에 영향을 준다. 저차원적인 수준에서는 출력값이 낮을수록 속도는 빨라지지만 정확도는 낮아진다.

이제 single-pose 추정 알고리즘의 아웃풋을 알아보자.
* 포즈 신뢰 점수와 17개의 키포인트를 가진 배열
* 각 키포인트에는 키포인트 위치 및 키포인트 신뢰 점수가 포함된다.

해당 코드 블락은 single-pose 추정 알고리즘을 사용하는 방법이다.
```
const imageScaleFactor = 0.50;
const flipHorizontal = false;
const outputStride = 16;
const imageElement = document.getElementById('cat');
// load the posenet model
const net = await posenet.load();
const pose = await net.estimateSinglePose(imageElement, scaleFactor, flipHorizontal, outputStride);
```
다음과 같이 출력이 나온다
```
{
  &quot;score&quot;: 0.32371445304906,
  &quot;keypoints&quot;: [
    { // nose
      &quot;position&quot;: {
        &quot;x&quot;: 301.42237830162,
        &quot;y&quot;: 177.69162777066
      },
      &quot;score&quot;: 0.99799561500549
    },
    { // left eye
      &quot;position&quot;: {
        &quot;x&quot;: 326.05302262306,
        &quot;y&quot;: 122.9596464932
      },
      &quot;score&quot;: 0.99766051769257
    },
    { // right eye
      &quot;position&quot;: {
        &quot;x&quot;: 258.72196650505,
        &quot;y&quot;: 127.51624706388
      },
      &quot;score&quot;: 0.99926537275314
    },
    ...
  ]
}
```
#### Part 2b: Multi-person 자세 추정

&lt;img src=&quot;https://miro.medium.com/max/1026/1*EZOqbMLkIwBgyxrKLuQTHA.png&quot;/&gt;*Example multi-person pose estimation algorithm applied to an image. Image Credit: “Microsoft Coco: Common Objects in Context Dataset”, https://cocodataset.org*

multi-person 자세 추정 알고리즘은 이미지에서 많은 사람과 포즈를 추정할 수 있다. 이것은 single-pose 알고리즘보다 느리지만 사진에 많은 사람들이 있는 경우 이점을 가지고 있다. 그리고 잘못된 자세를 추정하지 않을 확률이 좀 더 높다. 이런 이유로, 한사람의 자세를 디텍해야 하는 경우에도 이 알고리즘을 쓰는게 바람직 할 수도 있다.

게자다가, 이 알고리즘의 특징은 사람수에 따라서 성능이 영향을 받지 않는다는 것이다. 감지를 해야할 사람이 15명이든 5명이든 같은 계산 시간이 걸린다.

inputs을 봐보자
* Input image element - single-pose 추정과 같다
* Image scale factor - single-pose 추정과 같다
* Flip horizontal - single-pose 추정과 같다
* Output stride - single-pose 추정과 같다
* Maximum pose detections - Integer이고 기본값은 5이다. 디텍을 할 최대 포즈 개수이다.
* Pose confidence score threshold - 0.0 ~ 0.1값이고 기본값은 0.5이다. 고차원적 수준에서 이것은 반환되는 포즈의 신뢰 점수를 제어한다
* Non-maximum suppression(NMS) radius - 픽셀의 숫자이다. 고차원적 수준에서는 반환되는 포즈 사이의 최소 거리를 제어한다. 이 값의 기본값은 20이며 대부분의 경우에 괜찮다. 더 작은 정확도의 포즈를 나타내거나 숨기거나 할 때 사용될 수 있지만, 자세 신뢰 점수가 충분히 좋지 않을 때 사용하기에 좋다

출력값을 알아보자
* 자세의 배열을 내는 추정치
* 각 자세에는 single-pose 추정 알고리즘에서 설명한 것과 동일한 정보가 포함되어있다.

이 코드 블록은 multiple-person 추정 알고리즘을 사용하는 방법을 부여준다
```
const imageScaleFactor = 0.50;
const flipHorizontal = false;
const outputStride = 16;
// get up to 5 poses
const maxPoseDetections = 5;
// minimum confidence of the root part of a pose
const scoreThreshold = 0.5;
// minimum distance in pixels between the root parts of poses
const nmsRadius = 20;
const imageElement = document.getElementById('cat');
// load posenet
const net = await posenet.load();
const poses = await net.estimateMultiplePoses(
  imageElement, imageScaleFactor, flipHorizontal, outputStride,    
  maxPoseDetections, scoreThreshold, nmsRadius);
```

출력값은 다음과 같다
```
// array of poses/persons
[ 
  { // pose #1
    &quot;score&quot;: 0.42985695206067,
    &quot;keypoints&quot;: [
      { // nose
        &quot;position&quot;: {
          &quot;x&quot;: 126.09371757507,
          &quot;y&quot;: 97.861720561981
         },
        &quot;score&quot;: 0.99710708856583
      },
      ... 
    ]
  },
  { // pose #2
    &quot;score&quot;: 0.13461434583673,
    &quot;keypositions&quot;: [
      { // nose
        &quot;position&quot;: {
          &quot;x&quot;: 116.58444058895,
          &quot;y&quot;: 99.772533416748
        },
      &quot;score&quot;: 0.9978438615799
      },
      ...
    ]
  },
  ... 
]
```

여기 까지가 PoseNet 데모를 시작하기에 충분하지만 모델 및 구현의 세부 사항이 궁금하다면 계속 읽어보자.

### For Curious Minds: A Technical Deep Dive
이 섹션에서는 single-pose 추정 알고리즘에 대해 좀 더 자세히 설명한다. 고차원적 수준에서 프로세스는 다음과 같다

&lt;img src=&quot;https://miro.medium.com/max/1400/1*ey139jykjnBzUqcknAjHGQ.png&quot;/&gt;*Single person pose detector pipeline using PoseNet*

중요한 디테일은 ResNet과 MobileNet을 사용하여 PoseNet을 구성하였다는 것이다. Resnet 모델이 좀 더 높은 정확도를 가지지만 크기가 크고 많은 레이어를 가지며, 페이지의 로딩시간과 추론 시간이 실시간 어플리케이션에는 적합하지 않다. 여기서는 모바일 장치에서 실행되도록 설계된 MobileNet 모델을 사용하였다.

#### Revisiting the Single-pose Estimation Algorithm
##### Processing Model Inputs: 출력 Strides에 대한 설명

먼저 output strides에 대해 알아봐서 PoseNet 모델의 출력(주로 heatmaps and offset vectors)를 얻는 방법을 보자.

편의성을 위해, PoseNet 모델은 이미지 크기가 변하지 않는다. 이 뜻은 이미지의 축소여부에 관계없이 원본 이미지와 동일한 배율로 포즈 위치를 예측할 수 있다. 즉, 위에서 언급한 output stride를 런타임에 설정하여 성능을 낮추고 더 높은 정확도를 얻을 수 있다.

Output stride는 인풋 이미지 크기를 기준으로 출력 크기를 얼마나 줄일지 결정한다. 이 인자는 모델 출력과 레이어의 크기에 영향을 미친다. 만약 output stride를 높인다면 네트워크안에 레이어와 출력의 해상도가 낮아지고 그에 따라 정확도도 낮아진다. 이 구현에서 output stride는 8,16 또는 32의 값을 가질 수 있다. 다른 말로 32의 값을 가진 output stride는 가장 빠른 퍼포먼스를 보여주지만 낮은 정확도를 보여줄 것이고, 8의 경우 높은 정확도와 낮은 속도를 보여줄 것이다. 우리는 16을 추천한다.

&lt;img src=&quot;https://miro.medium.com/max/1400/1*zXXwR16kprAWLPIOKCrXLw.png&quot;/&gt;*The output stride determines how much we’re scaling down the output relative to the input image size. A higher output stride is faster but results in lower accuracy.*

#### Model Outputs: Heatmaps and Offset Vectors
PoseNet이 이미지를 처리할 때 리턴되는 값은 자세 키포인트와 대응되는 이미지의 가장 높은 신뢰 영역을 발견하기 위한 offset vector과 이를 포함한 heatmap이다.

&lt;img src=&quot;https://miro.medium.com/max/1400/1*mcaovEoLBt_Aj0lwv1-xtA.png&quot;/&gt;*Each of the 17 pose keypoints returned by PoseNet is associated to one heatmap tensor and one offset vector tensor used to determine the exact location of the keypoint.*

이 출력쌍은 해상도라고 언급되는 높이와 너비를 포함한 3D tensor이다. 해상도는 아래의 공식에 따라 입력 이미지 크기와 output stride에 의해 결정된다.

```
Resolution = ((InputImageSize - 1) / OutputStride) + 1
// Example: an input image with a width of 225 pixels and an output
// stride of 16 results in an output resolution of 15
// 15 = ((225 - 1) / 16) + 1
```

#### Heatmaps
각각의 히트맵은 해상도 * 해상도 * 17의 3D 텐서이다. 17은 PoseNet에서 디텍한 키포인트의 개수이다. 예를들언 이미지 크기가 225이고 output stride가 16이면 15*15*17이 될 것이다. 각각의 3번째 차원(17)은 특정한 키포인트와 대응된다. 히트맵의 각각의 포지션은 신뢰 점수를 가지고, 이것은 해당 히트맵 포인트에서 벡터를 따라 이동할 때 키포인트의 정확한 위치를 예측하는데 사용된다. 

#### Offset Vectors
각각의 offset vector는 해상도*해상도*34의 3D Tensor이다. 34는 키포인트 * 2를 의미한다. 이미지 크기가 225이고 output stride가 16이면 이것은, 15*15*34가 될 것이다. 히트맵은 키포인트가 어디 위치하는지를 나타내는 근사치이고 offset vectors는 히트맵 포인트에 해당하며 해당 히트 맵 포인트에서 벡터를 따라 이동할 때 키포인트의 정확한 위치를 예측하는데 사용된다. 

#### 모델의 출력으로 부터의 포즈 추정
이미지가 모델을 통해 공급된 후 몇가지 계산을 수행하여 출력에서 포즈를 추정합니다. 예를 들어, 단일 포즈 추정 알고리즘은 각각 자체 신뢰 점수 및 x, y 위치를 갖는 키포인트 (부품 ID로 색인화 됨)의 배열을 포함하는 포즈 신뢰 점수를 반환합니다.

포즈의 키포인트를 얻는 방법
1. sigmoid 활성화는 점수를 얻을 수 있는 히트맵에서 수행된다
scores = heatmap.sigmoid()

2. argmax2d는 각각의 부위에서 가장 높은 점수를 가진 히트맵의 x y 인덱스를 얻기 위하여 키포인트 신뢰 점수에 대해 수행된다. 이는 본질적으로 부위가 존재할 가능성이 가장 높은 곳입니다. 이렇게하면 크기가 17*2인 텐서가 생성되며 각 행은 각 부위의 점수가 가장 높은 y,x 인덱스 값입니다.
heatmapPositions = scores.argmax(y, x)

3. 각 부품 의 오프셋 벡터 는 해당 부품에 대한 히트 맵의 x 및 y 색인에 해당하는 오프셋에서 x 및 y를 가져 와서 검색됩니다. 그러면 크기가 17x2 인 텐서가 생성되며 각 행은 해당 키포인트의 오프셋 벡터입니다. 예를 들어, 인덱스 k의 부품에서 히트 맵 위치가 y 및 d 인 경우 오프셋 벡터는 다음과 같습니다.
offsetVector = [offsets.get(y, x, k), offsets.get(y, x, 17 + k)]

4. 키포인트를 얻기 위해 각 부위의 히트맵 x와 y에 output stride를 곱한다음 해당 오프셋 벡터에 추가한다
keypointPositions = heatmapPositions * outputStride + offsetVectors

5. 마지막으로 각각의 키포인트 신뢰 점수는 히트맵 위치의 신뢰 점수이다. 포즈 신뢰 점수는 키포인트의 점수의 평균이다.
</description>
            <pubDate>Thu, 23 Apr 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/posenet/introduction.html</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/posenet/introduction.html</guid>
            
            
        </item>
        
        <item>
            <title>최적화 함수들(optimization)</title>
            <description># 최적화 함수들(optimization)
신경망 학습의 목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것이며, 이러한 문제를 푸는 것을 최적화(optimization)이라고 한다.

## 확률적 경사 하강법 - SGD(Stochastic Gradient Descent)
최적의 매개변수 값을 찾는 단서로 매개변수의 기울기(미분)을 이용. 매개변수의 기울기를 구해, 기울어진 방향으로 매개변수 값을 갱신하는 일을 계속 반복한다.

```
W &lt;- W - ( learning rate * dL / dW )

W : 가중치
L : 손실 함수 
```

#### SGD의 단점
&lt;img src=&quot;optimization/sgd-1.png&quot;/&gt;
&lt;img src=&quot;optimization/sgd-2.png&quot;/&gt;

심하게 굽이진 움직임을 보여준다. 따라서 이러한 경우에는 조금 비효율 적이다.

## 모멘텀(Momentum)
```
v &lt;- av - ( learning rate * dL / dW )
W &lt;- W + v

W : 가중치
L : 손실함수
```

SGD와 차이점을 보면 av 값을 더해준게 눈에 띈다. 여기서 a는 고정된 상수값이고(ex 0.9) v는 물체의 속도라고 생각하면 된다.
해당 방향으로 진행할 수록 공이 기울기를 따라 구르듯 힘을 받는다.

&lt;img src=&quot;optimization/momentum-1.png&quot; width=&quot;350&quot;/&gt;

모멘텀의 갱신 경로는 공이 그릇 바닥을 구르듯 움직인다. SGD와 비교했을 때 지그재그 정도가 덜한 것을 알 수 있다.

&lt;img src=&quot;optimization/momentum-2.gif&quot; width=&quot;350&quot;/&gt;

또한 Momentum 방식을 이용할 경우 위의 그림과 같이 local minima를 빠져나오는 효과가 있을 것이라고도 기대할 수 있다. 기존의 SGD를 이용할 경우 좌측의 local minima에 빠지면 gradient가 0이 되어 이동할 수가 없지만, momentum 방식의 경우 기존에 이동했던 방향에 관성이 있어 이 local minima를 빠져나오고 더 좋은 minima로 이동할 것을 기대할 수 있게 된다.

## AdaGrad

학습에서는 Learning Rate가 중요하다. 이 값이 너무 작으면 학습 시간이 너무 길어지고, 반대로 너무 크면 발산하여 학습이 제대로 이뤄지지 않는다.

이러한 학습률을 정하는 효과적 기술로 학습률 감소(learning rate decay)가 있다. 이는 학습을 진행하면서 학습률을 점차 줄여나가는 방법이다.

학습률을 서서히 낮추는 가장 간단한 방법은 전체 학습률 값을 일괄적으로 낮추는 것이지만, 이를 더 발전시킨 것이 AdaGrad이다. AdaGrad는 '각각의' 매개변수에 '맞춤형'값을 만들어준다.

AdaGrad는 개별 매개변수에 각각 다른 학습률을 적용해준다.
&lt;img src=&quot;optimization/adaGrad.png&quot; width=&quot;350&quot;/&gt;

이 수식을 해석하면 매개변수의 원수 중에서 크게 갱신된 원소는 학습률이 낮아진다.

&lt;img src=&quot;optimization/adaGrad-1.png&quot; width=&quot;350&quot;/&gt;

```
RMSProp
AdaGrad는 과거의 기울기를 제곱하여 계속 더해간다. 그래서 학습을 진행할수록 갱신 강도가 
약해지고 계속해서 학습하면 어느 순간 갱신량이 0이 되어 전혀 갱신되지가 않는다. 
이를 개선한 기법이 RMSProp이다. RMSProp은 과거의 모든 기울기를 균일하게 
더해가는 것이 아니라, 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영한다.
```

## Adam
모멘텀 + AdaGrad
정확히 파고들면 다르지만, 직관적으로 해석하면 모멘텀과 AdaGrad를 융합한 듯한 방법이다.

&lt;img src=&quot;optimization/adam.png&quot; width=&quot;350&quot;/&gt;

## Optimizer 한눈으로 보기
&lt;img src=&quot;optimization/optimizer.jpg&quot; width=&quot;550&quot;/&gt;&lt;br&gt;
*출처: 하용호, 자습해도 모르겠던 딥러닝, 머리속에 인스톨 시켜드립니다(https://www.slideshare.net/yongho/ss-79607172)*

## 어느 갱신 방법을 이용할 것인가?
상황마다 다르다. 풀어야 할 문제가 무엇이냐에 따라 달라지고, 하이퍼파라미터를 어떻게 설정하느냐에 따라서 적절한 optimizer가 다르다. </description>
            <pubDate>Thu, 23 Apr 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/basic%20deeplearning/optimization.html</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/basic%20deeplearning/optimization.html</guid>
            
            
        </item>
        
        <item>
            <title>트레이닝</title>
            <description># 환경설정
Ubuntu 16.04 환경에서 작업하였다.

### Requirements
```
CMake &gt;= 3.12

$ sudo apt install cmake
```
```
CUDA &gt;= 10.0

network 형태로 설치(이 형태로 하면 패치를 받을 필요가 없어서 편하다)
$ wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-ubuntu1604.pin
$ sudo mv cuda-ubuntu1604.pin /etc/apt/preferences.d/cuda-repository-pin-600
$ sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub
$ sudo add-apt-repository &quot;deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/ /&quot;
$ sudo apt-get update
$ sudo apt-get -y install cuda
```
```
OpenCV &gt;= 2.4

$ sudo apt-get install build-essential cmake pkg-config libjpeg-dev libtiff5-dev libjasper-dev libpng12-dev libavcodec-dev libavformat-dev libswscale-dev libxvidcore-dev libx264-dev libxine2-dev libv4l-dev v4l-utils libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libqt4-dev mesa-utils libgl1-mesa-dri libqt4-opengl-dev libatlas-base-dev gfortran libeigen3-dev python2.7-dev python3-dev python-numpy python3-numpy
임시 폴더 생성
$ wget -O opencv.zip https://github.com/Itseez/opencv/archive/3.2.0.zip
$ unzip opencv.zip
$ wget -O opencv_contrib.zip https://github.com/Itseez/opencv_contrib/archive/3.2.0.zip
$ unzip opencv_contrib.zip
$ cd opencv-3.2.0
$ mkdir build
$ cd build
$ make -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D WITH_TBB=OFF -D WITH_IPP=OFF -D WITH_1394=OFF -D BUILD_WITH_DEBUG_INFO=OFF -D BUILD_DOCS=OFF -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D BUILD_EXAMPLES=OFF -D BUILD_TESTS=OFF -D BUILD_PERF_TESTS=OFF -D ENABLE_NEON=ON -D WITH_QT=ON -D WITH_OPENGL=ON -D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib-3.2.0/modules -D WITH_V4L=ON -D WITH_FFMPEG=ON -D WITH_XINE=ON -D BUILD_NEW_PYTHON_SUPPORT=ON -D PYTHON_INCLUDE_DIR=/usr/include/python2.7 -D PYTHON_INCLUDE_DIR2=/usr/include/x86_64-linux-gnu/python2.7 -D PYTHON_LIBRARY=/usr/lib/x86_64-linux-gnu/libpython2.7.so ../
$ make -j8 (cpu 코어 수에 따라서 숫자 조절)
$ sudo make install
설치확인
$ pkg-config --modversion opencv
$ pkg-config --libs --cflags opencv
```
&lt;img src=&quot;opencv.png&quot;/&gt;

```
cuDNN &gt;= 7.0

1. https://developer.nvidia.com/cudnn 이동
2. Download cuDNN 클릭
3. Login
4. https://developer.nvidia.com/rdp/cudnn-archive 이동
5. Download cuDNN v7.6.5 (November 18th, 2019), for CUDA 10.2 선택
6. cuDNN Runtime Library for Ubuntu16.04 (Deb) 다운로드
7. sudo dpkg -i 다운받은deb파일
```

### darknet 설치
위의 requirements가 모두 설치되었다는 가정하에 진행한다.

* https://github.com/AlexeyAB/darknet.git 받고 해당 폴더 이동
* Makefile 열어서 수정(아래 그림 파일 참조)
&lt;img src=&quot;makefile.png&quot;/&gt;
본인의 경우에는 현재 PC의 상황에 맞게 다음과 같이 설정하였다.
```
GPU=1
CUDNN=1
CUDNN_HALF=0
OPENCV=1
AVX=0
OPENMP=0
LIBSO=1
ZED_CAMERA=0
ZED_CAMERA_v2_8=0
```
* $ make 명령어를 통해 컴파일
* 컴파일 도중 NVCC 에러가 날 수도 있다. 이때는 다시 Makefile을 열어서 본인의 nvcc 경로를 변경해줘야한다.
일반적으로 위에서 cuda를 제대로 설치했으면 /usr/local/cuda/bin/nvcc 일 것이다.
* ./darknet detector test ./cfg/coco.data ./cfg/yolov4.cfg ./yolov4.weights 명령어를 통해 올바르게 컴파일이 되었는지 확인을 해본다. 보통 makefile이나 실행 도중 에러가 나면 에러메시지를 잘 확인해 보길 바란다. cuda version 문제 또는 cudnn version문제 또는 opencv 관련 문제일 것이다.



</description>
            <pubDate>Wed, 28 Oct 2020 00:00:00 +0900</pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/Yolo(darknet)/training.html</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/Yolo(darknet)/training.html</guid>
            
            
        </item>
        
    </channel>
</rss>