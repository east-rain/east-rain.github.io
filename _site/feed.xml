<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
     <!-- 피드경로 명시하자-->
    <channel>
        <title> 차곡차곡 쌓자  </title>
        <description>차곡차곡 쌓아가는 기술블로그입니다. 되도록 정확한 자료만을 정리하려고 노력합니다.</description>
        <link>https://east-rain.github.io/</link>
        <atom:link href="https://east-rain.github.io/feed.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Thu, 23 Apr 2020 18:57:04 +0900</pubDate>
        <lastBuildDate>Thu, 23 Apr 2020 18:57:04 +0900</lastBuildDate>
        <generator>Jekyll v3.8.6</generator>
        
        <item>
            <title></title>
            <description>&lt;style type=&quot;text/css&quot; media=&quot;screen&quot;&gt;
  .container {
    margin: 10px auto;
    max-width: 600px;
    text-align: center;
  }
  h1 {
    margin: 30px 0;
    font-size: 4em;
    line-height: 1;
    letter-spacing: -1px;
  }
&lt;/style&gt;

&lt;div class=&quot;container&quot;&gt;
  &lt;h1&gt;404&lt;/h1&gt;

  &lt;p&gt;&lt;strong&gt;Page not found :(&lt;/strong&gt;&lt;/p&gt;
  &lt;p&gt;The requested page could not be found.&lt;/p&gt;
&lt;/div&gt;
</description>
            <pubDate></pubDate>
            <link>https://east-rain.github.io/404.html</link>
            <guid isPermaLink="true">https://east-rain.github.io/404.html</guid>
            
            
        </item>
        
        <item>
            <title>텐서플로우 이미지 분류하기</title>
            <description># 이미지 분류
이 학습서는 이미지에서 고양이 또는 개를 분류하는 방법을 보여줍니다. tf.keras.Sequential 모델을 사용하여 이미지 분류기를 작성하고 tf.keras.preprocessing.image.ImageDataGenerator를 사용하여 데이터를 로드합니다.

다음과 같은 개념에 대한 실용적인 경험을 쌓고 직관력을 키울 수 있습니다.
* tf.keras.preprocessing.image.ImageDataGenerator를 사용한 data input 파이프라인을 구축하고 디스크에 있는 데이터를 모델과 함께 효율적으로 컨트롤 할 수 있습니다.
* Overfitting(과적합) - 식별하고 예방하는 방법
* 데이터 확대 및 드롭 아웃 - 컴퓨터 비전 task를 데이터 파이프 라인 그리고 이미지 분류 모델에 포함하여 Overfitting을 방지하는 중요한 기술

이 튜토리얼은 일반적인 머신러닝 워크플로우를 따릅니다.
1. 데이터 검사 및 이해
2. 입력 파이프 라인 구축
3. 모델 구축
4. 모델 훈련
5. 모델 테스트
6. 모델 개선 및 프로세스 반복

## 패키지 가져오기
필요한 패키지를 가져와 시작하겠습니다. os패키지는 파일 및 디렉토리 구조를 판독하는데 사용됩니다. NumPy는 파이썬 list 형태를 numpy array 형태로 변환하는데 사용되고, 요구되는 매트릭스 연산을 수행합니다. matplotlib.pyplot은 학습 그리고 검증 그래프를 보여줍니다.

모델을 구성하는 데 필요한 Tensorflow 및 Keras 클래스를 가져옵니다.

```
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator

import os
import numpy as np
import matplotlib.pyplot as plt
```

## 데이터 로드
데이터 세트를 다운로드하여 시작하십시오. 이 튜토리얼은 Kaggle 의 필터링 된 버전의 Dogs vs Cats 데이터 세트를 사용합니다. 데이터 세트의 아카이브 버전을 다운로드하여 &quot;/tmp/&quot;디렉토리에 저장하십시오.
```
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')
```

데이터셋은 다음과 같은 디렉토리 구조를 가집니다.
```
cats_and_dogs_filtered
|__ train
    |______ cats: [cat.0.jpg, cat.1.jpg, cat.2.jpg ....]
    |______ dogs: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...]
|__ validation
    |______ cats: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ....]
    |______ dogs: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]
```

압축을 해제한 후 학습 및 검증 세트에 적합한 파일 경로로 변수를 지정하십시오.
```
train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

train_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures
train_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures
validation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures
validation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures
```

## 데이터 이해
훈련 및 검증 디렉토리에 고양이와 개 이미지가 몇 개인 지 살펴 보겠습니다.

```
num_cats_tr = len(os.listdir(train_cats_dir))
num_dogs_tr = len(os.listdir(train_dogs_dir))

num_cats_val = len(os.listdir(validation_cats_dir))
num_dogs_val = len(os.listdir(validation_dogs_dir))

total_train = num_cats_tr + num_dogs_tr
total_val = num_cats_val + num_dogs_val
```
```
print('total training cat images:', num_cats_tr)
print('total training dog images:', num_dogs_tr)

print('total validation cat images:', num_cats_val)
print('total validation dog images:', num_dogs_val)
print(&quot;--&quot;)
print(&quot;Total training images:&quot;, total_train)
print(&quot;Total validation images:&quot;, total_val)
```
```
total training cat images: 1000
total training dog images: 1000
total validation cat images: 500
total validation dog images: 500
--
Total training images: 2000
Total validation images: 1000
```

편의를 위해 데이터 세트를 사전 처리하고 네트워크를 훈련하는 동안 사용할 변수를 설정하십시오.

```
batch_size = 128
epochs = 15
IMG_HEIGHT = 150
IMG_WIDTH = 150
```

## 데이터 준비
네트워크에 데이터를 공급하기 전에 이미지를 부동 소수점 tensor로 변환해야 합니다

1. 디스크에서 이미지를 읽습니다.
2. 이 이미지의 내용을 디코딩하여 RGB 내용에 따라 적절한 grid 형식으로 변환합니다.
3. 이 값들을 부동 소수점 tensor로 변환합니다.
4. 신경망은 작은 입력 값을 처리하는 것을 선호하므로 텐서를 0에서 255 사이의 값에서 0에서 1 사이의 값으로 재조정하십시오.

다행히 이러한 모든 작업은 tf.keras에 의해 제공되는 ImageDataGenerator 클래스에서 수행 할 수 있습니다. 디스크에서 이미지를 읽고 적절한 텐서로 사전 처리 할 수 ​​있습니다. 또한 이 이미지들을 텐서의 배치로 변환하여 네트워크 학습에 도움을 줄 수 있습니다.

```
train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data
validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data
```

학습 그리고 검증 이미지를 위해 생성기를 정의하고 난 뒤 flow_from_directory 메서드는 디스크로부터 이미지를 불러오고, rescaling을 적용하고, 요구되는 면적에 맞게 이미지를 resize 합니다.

```
train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,
                                                           directory=train_dir,
                                                           shuffle=True,
                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                           class_mode='binary')
```
```
val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,
                                                              directory=validation_dir,
                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                              class_mode='binary')
```

### 학습 이미지 시각화
트레이닝 생성기로부터 이미지의 배치를 추출하고 시각화한다
```
sample_training_images, _ = next(train_data_gen)
```
next 함수는 데이터셋의 배치를 반환한다. next 함수의 반환값은 (x_train, y_train) 형태이다. 여기서 x_train은 학습 특징들이고 y_train은 라벨이다. 학습 데이터만 보여주기 위해서 라벨은 숨긴다
```
# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.
def plotImages(images_arr):
    fig, axes = plt.subplots(1, 5, figsize=(20,20))
    axes = axes.flatten()
    for img, ax in zip( images_arr, axes):
        ax.imshow(img)
        ax.axis('off')
    plt.tight_layout()
    plt.show()
```
```
plotImages(sample_training_images[:5])
```
&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/classification_files/output_d_VVg_gEVrWW_0.png&quot;/&gt;&lt;br/&gt;

## 모델 생성하기
모델에는 max pool layer를 가지고 있는 3개의 convolution block이 존재한다. 512개의 유닛이 존재하는 fully connected layer가 가장 상단에 존재하고 활성화 함수로 relu 함수가 사용된다.
```
model = Sequential([
    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),
    MaxPooling2D(),
    Conv2D(32, 3, padding='same', activation='relu'),
    MaxPooling2D(),
    Conv2D(64, 3, padding='same', activation='relu'),
    MaxPooling2D(),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(1)
])
```

### 모델 컴파일하기
이 튜토리얼에서는 ADAM 옵티마이저와 cross entropy 손실 함수를 선택하였다. 학습 그리고 검증 정확도를 보기 위해 각각의 학습 에폭에 metrics 아규먼트를 넘겨준다.
```
model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
```

### 모델 요약
summary 메서드를 이용하여 네트워크의 모든 레이어를 관찰할 수 있다.
```
model.summary()
```
```
Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 150, 150, 16)      448       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 75, 75, 16)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 75, 75, 32)        4640      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 37, 37, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 37, 37, 64)        18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 18, 18, 64)        0         
_________________________________________________________________
flatten (Flatten)            (None, 20736)             0         
_________________________________________________________________
dense (Dense)                (None, 512)               10617344  
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 513       
=================================================================
Total params: 10,641,441
Trainable params: 10,641,441
Non-trainable params: 0
_________________________________________________________________
```

### 모델 학습시키기
**ImageDataGenerater** 클래스의 **fit_generatoer** 메서드를 이용해서 네트워크를 학습시킨다.
```
history = model.fit_generator(
    train_data_gen,
    steps_per_epoch=total_train // batch_size,
    epochs=epochs,
    validation_data=val_data_gen,
    validation_steps=total_val // batch_size
)
```

### 트레이닝 결과 시각화하기
```
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss=history.history['loss']
val_loss=history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()
```

&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/classification_files/output_K6oA77ADVrWp_0.png&quot;/&gt;&lt;br/&gt;

그림에서 알 수 있듯이 트레이닝 정확도와 밸리데이션 정확도는 큰 폭으로 증가하였습니다. 그리고 밸리데이션 정확도는 70% 정도를 달성하였습니다.
무엇이 잘못되었는지 살펴보고 모델의 전반적인 성능을 향상 시켜보겠습니다.

## 과적합(Overfitting)
위의 그림에서 학습 정확도는 시간이 지날수록 선형적으로 증가하지만, 검증 정확도는 70%를 넘지 못합니다. 또한 학습데이터와 검증데이터의 정확도 차이는 과적합의 명확한 신호입니다.

적은 개수의 학습 데이터에서, 학습 데이터의 원치않는 디테일이나 노이즈를 모델이 학습해버리는 경우가 있습니다. 이것은 새로운 데이터를 인식하는 모델의 정확도에 않좋은 영향을 끼칩니다. 이 현상은 과적합이라고 알려져있습니다. 이 의미는 모델이 새로운 데이터셋이 들어왔을때 일반화하지 못한다는 의미입니다.

오버피팅을 방지하는 여러가지 방법들이 학습 단계에 존재합니다. 이 튜토리얼에서 우리는 *데이터 보강* 방법과 드롭아웃 기법을 사용할 것입니다.

## 데이터 보강
과적합은 일반적으로 트레이닝 데이터가 부족할 때 발생합니다. 이 문제를 해결하는 한 방법은 데이터를 보강하여 충분한 개수의 트레이닝 데이터를 얻는 것입니다. 데이터 보강은 존재하는 학습 데이터에서 그럴듯하게 보이는 이미지를 랜덤하게 생성해내는 방법이다. 목표는 모델은 절대 두번이상 같은 데이터를 학습하지 않는 것이다. 이것은 모델이 더 다양한 데이터를 접하고 일반화하게 만든다.

tf.keras의 **ImageDataGenerator** 클래스를 사용한다. 데이터셋의 다양한 변화를 시도한다.

### 데이터 보강과 시각화
랜덤 수평 flip 보강을 시작하고, 각각의 이미지들의 변화 후 모습을 보자

### 수평 flip 적용하기
ImageDataGeneraotr 클래스에 인자로 horizontal_flip 을 넘겨준다 그리고 True 값을 줘서 이 인자를 적용시킨다.
```
image_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)

train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,
                                               directory=train_dir,
                                               shuffle=True,
                                               target_size=(IMG_HEIGHT, IMG_WIDTH))
```

트레이닝 에제로부터 하나의 샘플 이미지를 가져오고 5번 반복한다, 즉 같은 이미지에 대해서 5번 증강을 시도한다
```
augmented_images = [train_data_gen[0][0][0] for i in range(5)]
```
```
# Re-use the same custom plotting function defined and used
# above to visualize the training images
plotImages(augmented_images)
```
&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/classification_files/output_EvBZoQ9xVrW9_0.png&quot;/&gt;&lt;br/&gt;

### 이미지를 랜덤하게 회전하기
회전이라 불리는 증강방법을 살펴보자 그리고 트레이닝 예제들을 45도 회전해보자 랜덤하게

```
image_gen = ImageDataGenerator(rescale=1./255, rotation_range=45)
```
```
train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,
                                               directory=train_dir,
                                               shuffle=True,
                                               target_size=(IMG_HEIGHT, IMG_WIDTH))

augmented_images = [train_data_gen[0][0][0] for i in range(5)]
```
```
plotImages(augmented_images)
```
&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/classification_files/output_wmBx8NhrVrXK_0.png&quot;/&gt;&lt;br/&gt;

### 확대 증강 적용하기
이미지를 랜덤하게 50프로 확대하는 증강법을 알아보자
```
# zoom_range from 0 - 1 where 1 = 100%.
image_gen = ImageDataGenerator(rescale=1./255, zoom_range=0.5) # 
```
```
train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,
                                               directory=train_dir,
                                               shuffle=True,
                                               target_size=(IMG_HEIGHT, IMG_WIDTH))

augmented_images = [train_data_gen[0][0][0] for i in range(5)]
```
```
plotImages(augmented_images)
```
&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/classification_files/output_-KQWw8IZVrXZ_0.png&quot;/&gt;&lt;br/&gt;

### 모두 적용해보기
이전의 증강법을 모두 적용해보자. 이 방법으로 너는 rescale, 45도 회전, 가로 쉬프트, 세로 쉬프트, 수평 플립 그리고 확대 증강법을 사용할 수 있다.
```
image_gen_train = ImageDataGenerator(
                    rescale=1./255,
                    rotation_range=45,
                    width_shift_range=.15,
                    height_shift_range=.15,
                    horizontal_flip=True,
                    zoom_range=0.5
                    )
```
```
train_data_gen = image_gen_train.flow_from_directory(batch_size=batch_size,
                                                     directory=train_dir,
                                                     shuffle=True,
                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                     class_mode='binary')
```
하나의 이미지에 대해서 5번 이 증강법을 랜덤하게 적용했을 때 어떤 변화가 일어나는지 확인해보자
```
augmented_images = [train_data_gen[0][0][0] for i in range(5)]
plotImages(augmented_images)
```
&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/classification_files/output_z2m68eMhVrXm_0.png&quot;/&gt;&lt;br/&gt;

### 검증 데이터 생성기 만들기
일반적으로 데이터 증강은 훈련 데이터에 대해서만 시행한다. 이번에는, 검증 데이터를 단순히 rescale하고 ImageDataGenerator를 사용하여 검증 데이터를 배치로 변환하는 것을 해보겠다.
```
image_gen_val = ImageDataGenerator(rescale=1./255)
```
```
val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size,
                                                 directory=validation_dir,
                                                 target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                 class_mode='binary')
```

## 드롭아웃 - Dropout
과적합을 방지하는 다른 테크닉은 네트워크에 드롭아웃을 적용하는 것이다. 이것은 정규화의 한 방법으로써 네트워크에서 작은 부분의 가중치만 선택하게 함으로써 가중치를 분산시키고 작은 훈련 데이터에서 과적합을 방지하는 것이다.

드롭아웃을 레이어에 적용할 때, 훈련 과정동안 랜덤하게 뉴런의 일정부분을 꺼놓는다. 드롭아웃은 0.1, 0.2, 0.4와 같이 아주 작은 인풋을 받는다. 이 의미는 10%, 20%, 40%의 뉴런에 대해서 랜덤하게 드롭아웃을 적용하라는 의미다.

어떤 특정한 레이어에 대해 0.1 드롭아웃을 적용하면, 이것은 각 에폭마다 랜덤하게 10%의 뉴런을 끄고 학습을 진행한다.

이 드롭아웃 기법을 컨볼루션과 fully-connected layers에 적용하여 새롭게 모델을 설계해라

## 드롭아웃을 사용하여 새로운 네트워크 생성하기
여기, 너는 첫번째와 마지막 max pool 레이어에 드롭아웃을 적용한다. 드롭아웃은 각 훈련 에폭동안 20%의 뉴런을 제외하고 학습을 진행한다. 이것은 트레이닝 데이터셋에서 오버피팅을 방지하는데 도움을 준다
```
model_new = Sequential([
    Conv2D(16, 3, padding='same', activation='relu', 
           input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),
    MaxPooling2D(),
    Dropout(0.2),
    Conv2D(32, 3, padding='same', activation='relu'),
    MaxPooling2D(),
    Conv2D(64, 3, padding='same', activation='relu'),
    MaxPooling2D(),
    Dropout(0.2),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(1)
])
```

### 모델 컴파일하기
네트워크에 드롭아웃을 적용한 다음에 모델을 컴파일하고 레이어의 서머리를 확인해보자
```
model_new.compile(optimizer='adam',
                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                  metrics=['accuracy'])

model_new.summary()
```
```
Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_3 (Conv2D)            (None, 150, 150, 16)      448       
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 75, 75, 16)        0         
_________________________________________________________________
dropout (Dropout)            (None, 75, 75, 16)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 75, 75, 32)        4640      
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 37, 37, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 37, 37, 64)        18496     
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 18, 18, 64)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 18, 18, 64)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 20736)             0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               10617344  
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 513       
=================================================================
Total params: 10,641,441
Trainable params: 10,641,441
Non-trainable params: 0
_________________________________________________________________
```

### 모델 학습하기
성공적으로 데이터 증강을 학습 데이터들에 적용하고 네트워크에 드롭아웃을 적용하고 이 새로운 네트워크를 학습시켜보자!
```
history = model_new.fit_generator(
    train_data_gen,
    steps_per_epoch=total_train // batch_size,
    epochs=epochs,
    validation_data=val_data_gen,
    validation_steps=total_val // batch_size
)
```

### 모델 시각화하기
훈련 후에 새로운 모델을 시각화한다. 너는 상당히 과적합이 줄어든 것을 확인할 수 있다. 정확도는 에폭이 증가할 수록 점점 더 증가한다.
```
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()
```
&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/classification_files/output_7BTeMuNAVrYC_0.png&quot;/&gt;&lt;br/&gt;
</description>
            <pubDate></pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/image_classification.html</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/image_classification.html</guid>
            
            
        </item>
        
        <item>
            <title>텐서플로우 허브 전이학습</title>
            <description># 텐서플로 허브와 전이학습(transfer learning)
텐서플로 허브는 미리 학습된 모델들을 공유하는 곳이다. [텐서플로우 모듈 허브](https://tfhub.dev/)를 보면 미리 학습된 모델들을 찾을 수 있다.
이 튜토리얼은 다음과 같은 과정들을 보여줄 것이다.

1. tf.keras로 텐서플로 허브를 사용하는 방법.
2. 텐서플로 허브를 사용하여 이미지 분류를 하는 방법.
3. 간단한 전이학습을 하는 방법.

## 셋업
```
from __future__ import absolute_import, division, print_function, unicode_literals

import matplotlib.pylab as plt

import tensorflow as tf
```
```
!pip install -q -U tf-hub-nightly
import tensorflow_hub as hub

from tensorflow.keras import layers
```

## ImageNet 분류기(classifier)
### 분류기 다운로드
*hub.module*을 사용하여 mobilenet을 불러오고, *tf.keras.layers.Lambda*로 감싸서 keras 레이어로 만든다. tfhub.dev에서 구한 [어떠한 텐서플로우2의 비교가능한 이미지 분류기 URL](https://tfhub.dev/s?q=tf2&amp;module-type=image-classification)도 여기서 다 작동을 한다.

```
classifier_url =&quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2&quot; #@param {type:&quot;string&quot;}
``` 
```
IMAGE_SHAPE = (224, 224)

classifier = tf.keras.Sequential([
    hub.KerasLayer(classifier_url, input_shape=IMAGE_SHAPE+(3,))
])
```

### 하나의 이미지에 대해 실행해보기
이미지를 하나 받아서 모델에 적용해보자
```
import numpy as np
import PIL.Image as Image

grace_hopper = tf.keras.utils.get_file('image.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg')
grace_hopper = Image.open(grace_hopper).resize(IMAGE_SHAPE)
grace_hopper
```
```
grace_hopper = np.array(grace_hopper)/255.0
grace_hopper.shape
```
```
&gt; (224, 224, 3)
```
배치 차원을 더해주고, 모델에다 이미지를 넣는다
```
result = classifier.predict(grace_hopper[np.newaxis, ...])
result.shape
```
```
&gt; (1, 1001)
```
결과는 logit의 1001 요소 벡터 값, 그리고 이미지의 각 클래스에 대한 확률 값이다.
따라서 가장 높은 확률의 id값은 argmax로 구할 수 있다.
```
predicted_class = np.argmax(result[0], axis=-1)
predicted_class
```
```
&gt; 653
```

### 예측 해독하기
우리는 class ID값을 예측하고, **ImageNet** 라벨을 불러오고, 예측을 해독할 것이다.
```
labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')
imagenet_labels = np.array(open(labels_path).read().splitlines())
```
```
plt.imshow(grace_hopper)
plt.axis('off')
predicted_class_name = imagenet_labels[predicted_class]
_ = plt.title(&quot;Prediction: &quot; + predicted_class_name.title())
```

## 단순한 전이학습(transfer learning)
TF Hub를 사용한다면 우리의 데이터셋의 클래스는 구분하기 위한 모델의 탑 레이어를 재교육하기 쉽다.

### Dataset
이 예제에서 너는 tensorFlow flowers dataset을 이용할 것이다.
```
data_root = tf.keras.utils.get_file(
  'flower_photos','https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
   untar=True)
```
우리 모델에 이 데이터를 불러오는 가장 간단한 방법은 **tf.keras.preprocessing.image.ImageDataGenerator를 사용하는 것이다.

모든 텐서플로우 허브의 이미지 모듈들은 [0, 1]사이의 실수 값을 사용한다. **ImageDataGenerator**의 *rescale* 파라메터를 사용하여 변환해라

```
image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)
image_data = image_generator.flow_from_directory(str(data_root), target_size=IMAGE_SHAPE)
```

결과 객체는 image_batch, label_batch 쌍을 반환하는 iterator이다
```
for image_batch, label_batch in image_data:
  print(&quot;Image batch shape: &quot;, image_batch.shape)
  print(&quot;Label batch shape: &quot;, label_batch.shape)
  break
```
```
&gt; Image batch shape:  (32, 224, 224, 3)
  Label batch shape:  (32, 5)
```

### 이미지의 배치에 대한 분류기(classifier) 실행하기
```
result_batch = classifier.predict(image_batch)
result_batch.shape
```
```
&gt; (32, 1001)
```
```
predicted_class_names = imagenet_labels[np.argmax(result_batch, axis=-1)]
predicted_class_names
```
```
&gt; array(['daisy', 'mushroom', 'Bedlington terrier', 'daisy', 'daisy', 'bee',
         'coral fungus', 'hair slide', 'picket fence', 'daisy', 'pot',
         'mushroom', 'daisy', 'bee', 'rapeseed', 'daisy', 'daisy',
         'water buffalo', 'spider web', 'cardoon', 'daisy', 'daisy', 'bee',
         'daisy', 'vase', 'daisy', 'barn spider', 'slug', 'coral fungus',
         'sea urchin', 'pot', 'coral fungus'], dtype='&lt;U30')
```

이미지와 함께 결과를 확인해보자
```
plt.figure(figsize=(10,9))
plt.subplots_adjust(hspace=0.5)
for n in range(30):
  plt.subplot(6,5,n+1)
  plt.imshow(image_batch[n])
  plt.title(predicted_class_names[n])
  plt.axis('off')
_ = plt.suptitle(&quot;ImageNet predictions&quot;)
```

&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub_files/output_IXTB22SpxDLP_0.png&quot;/&gt;&lt;br/&gt;

결과는 완벽하지 않지만 모델이 데이지를 제외한 클래스들에 대해 학습된게 아니라는 점을 고려해야한다.

### 헤드리스모델 다운로드
텐서플로우 허브는 또한 top classification layer를 제외한 모델을 배포한다. 이것은 전이 학습에 매우 쉽게 사용될 수 있다.

tfhub.dev에서 구한 [어떠한 텐서플로우2의 비교가능한 이미지 특징 벡터 URL](https://tfhub.dev/s?module-type=image-feature-vector&amp;q=tf2)도 여기서 다 작동을 한다.
```
feature_extractor_url = &quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2&quot; #@param {type:&quot;string&quot;}
```
특징 추출기를 만든다
```
feature_extractor_layer = hub.KerasLayer(feature_extractor_url,
                                         input_shape=(224,224,3))
```
각 이미지에 대해 1280 길이의 벡터를 반환한다
```
feature_batch = feature_extractor_layer(image_batch)
print(feature_batch.shape)
```
```
&gt; (32, 1280)
```
특징 추출 레이어(feature extractor layer)의 변수들을 고정한다. 따라서 모델에서 오직 새로운 classifier layer를 훈련하고 추가할 뿐이다.
```
feature_extractor_layer.trainable = False
```

### Classification haed 붙이기
hub layer를 **tf.keras.Sequential** 모델로 감싸고 새로운 classification layer를 추가해라
```
model = tf.keras.Sequential([
  feature_extractor_layer,
  layers.Dense(image_data.num_classes)
])

model.summary()
```
```
Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
keras_layer_1 (KerasLayer)   (None, 1280)              2257984   
_________________________________________________________________
dense (Dense)                (None, 5)                 6405      
=================================================================
Total params: 2,264,389
Trainable params: 6,405
Non-trainable params: 2,257,984
_________________________________________________________________
```
```
predictions = model(image_batch)
predictions.shape
```
```
&gt; TensorShape([32, 5])
```

### 모델 학습시키기
트레이닝 과정을 설정하고 컴파일한다
```
model.compile(
  optimizer=tf.keras.optimizers.Adam(),
  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
  metrics=['acc'])
```

이제 **.fit** 메서드를 사용해서 모델을 학습시킨다

예제를 짧게 유지하기 위해 단지 2 에폭만 학습한다. 트레이닝 과정을 시각화하기위해 에폭의 평균 대신 각 배치 고유의 loss와 accuracy를 남기는 custom callback을 이용한다.
```
class CollectBatchStats(tf.keras.callbacks.Callback):
  def __init__(self):
    self.batch_losses = []
    self.batch_acc = []

  def on_train_batch_end(self, batch, logs=None):
    self.batch_losses.append(logs['loss'])
    self.batch_acc.append(logs['acc'])
    self.model.reset_metrics()
```
```
steps_per_epoch = np.ceil(image_data.samples/image_data.batch_size)

batch_stats_callback = CollectBatchStats()

history = model.fit_generator(image_data, epochs=2,
                              steps_per_epoch=steps_per_epoch,
                              callbacks = [batch_stats_callback])
```
```
&gt; Epoch 1/2
  115/115 [==============================] - 10s 91ms/step - loss: 0.3135 - acc: 0.9375
  Epoch 2/2
  115/115 [==============================] - 10s 89ms/step - loss: 0.2287 - acc: 0.9062
```
적은 학습만으로도 잘 동작한다는걸 확인할 수 있습니다.
```
plt.figure()
plt.ylabel(&quot;Loss&quot;)
plt.xlabel(&quot;Training Steps&quot;)
plt.ylim([0,2])
plt.plot(batch_stats_callback.batch_losses)
```
&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub_files/output_3uvX11avTiDg_1.png&quot;/&gt;&lt;br/&gt;

### 예측(prediction) 확인하기
그림을 다시 그리기 위해 클래스 이름의 정렬된 순서를 가져온다
```
class_names = sorted(image_data.class_indices.items(), key=lambda pair:pair[1])
class_names = np.array([key.title() for key, value in class_names])
class_names
```
```
&gt; array(['Daisy', 'Dandelion', 'Roses', 'Sunflowers', 'Tulips'],
      dtype='&lt;U10')
```

모델에 이미지 배치를 돌리고 클래스 이름이 위치하도록 변환한다
```
predicted_batch = model.predict(image_batch)
predicted_id = np.argmax(predicted_batch, axis=-1)
predicted_label_batch = class_names[predicted_id]
```
결과를 그린다
```
label_id = np.argmax(label_batch, axis=-1)

plt.figure(figsize=(10,9))
plt.subplots_adjust(hspace=0.5)
for n in range(30):
  plt.subplot(6,5,n+1)
  plt.imshow(image_batch[n])
  color = &quot;green&quot; if predicted_id[n] == label_id[n] else &quot;red&quot;
  plt.title(predicted_label_batch[n].title(), color=color)
  plt.axis('off')
_ = plt.suptitle(&quot;Model predictions (green: correct, red: incorrect)&quot;)
```
&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub_files/output_wC_AYRJU9NQe_0.png&quot;/&gt;&lt;br/&gt;

## Export your model
훈련시킨 모델을 저장하고 내보낸다
```
import time
t = time.time()

export_path = &quot;/home/testopia-01/workspace/saved_models/{}&quot;.format(int(t))
model.save(export_path, save_format='tf')
```
이제 우리는 모델을 불러오고 같은 결과를 기대할 수 있다.
```
reloaded = tf.keras.models.load_model(export_path)

result_batch = model.predict(image_batch)
reloaded_result_batch = reloaded.predict(image_batch)

abs(reloaded_result_batch - result_batch).max()
```
```
&gt; 0.0
```</description>
            <pubDate></pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/image_transfer_learning_TFHub.html</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/image_transfer_learning_TFHub.html</guid>
            
            
        </item>
        
        <item>
            <title>텐서플로우 전이학습</title>
            <description># 미리 학습된 ConvNet으로 부터 전이 학습
이 튜토리얼에서, 너는 미리 학습된 네트워크로부터 전이학습을 통하여 강아지와 고양이를 분류하는 방법을 배우게 될 것이다.

미리 학습된 모델은 매우 큰 데이터 셋으로 부터 미리 학습되어 저장되어 있다. 사전 훈련 된 모델을 그대로 사용하거나 전이학습을 통하여 모델을 우리가 원하는 방법으로 커스터마이징 할 수 있다.

이미지 classification을 위한 전이학습은 직관적으로 다음과 같이 표현할 수 있다.

만약 모델이 매우 크고 일반적인 데이터셋을 통하여 훈련되었다면, 이 모델은 실제 visual 세상의 일반적인 모델로서 효과적으로 작동한다.

너는 큰 데이터셋에서 큰 모델을 학습하여 처음부터 시작할 필요 없이, 이러한 학습 된 모델을 사용할 수 있다.

이 노트북에서 너는 사전 훈련된 모델을 커스터마이징 하는 두 가지 방법을 시도할 것이다.

1. 특징 추출: 미리 학습된 네트워크에서 배운 표현을 사용하여 새로운 학습데이터에서 의미있는 특징을 추출할 것이다. 미리 학습된 모델에다가 새로운 classifier를 추가하기만 하면 미리 학습된 모델의 특징맵을 재사용 할 수 이싿.

2. 미세 조정(fine-tuning): 새로 추가 할 classifier 레이어와 기존 모델의 마지막 레이어를 함께 훈련시킨다. 이를 통해 기존의 모델에서 고차원 특징 표현을 미세 조정하여 우리가 원하는 작업에 보다 적합하게 만들 수 있다.

너는 일반적 머신 러닝 작업플로우를 따를 것이다.
1. 데이터 검사 및 이해
2. 케라스 ImageDataGenerator를 통하여 input pipeline 빌드
3. 모델 구성
    - 미리 학습된 모델 Load(and 미리 학습된 가중치)
    - classfication layer 쌓기
4. 모델 학습
5. 모델 검증
```
import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
```

## Data preprocessing
### Data download

강아지와 고양이 데이터셋을 로드하기 위해 TensorFlow Datasets를 사용한다

이 **tfds** 패키지는 사전 정의 된 데이터를 로드하는 가장 쉬운 방법이다.

만약 너의 고유한 데이터셋을 가지고 있고 사용하고 싶다면 loading image data 를 찹조해라
```
import tensorflow_datasets as tfds
tfds.disable_progress_bar()
```
**tfds.load** 메서드는 데이터를 다운로드 및 캐슁하고, **tf.data.Dataset** 객체를 리턴한다.

이 객체는 데이터를 조작하고 너의 모델로 공급하는 강력하고 효율적인 방법을 제공한다

&quot;cats_vs_dogs&quot;는 표준 분할을 정희하지 않았기 때문에 80%(학습), 10%(검증), 10%(테스트) 비율로 데이터를 나눈다.

```
(raw_train, raw_validation, raw_test), metadata = tfds.load(
    'cats_vs_dogs',
    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
    with_info=True,
    as_supervised=True,
)
```
 
**tf.data.Dataset** 객체의 결과는 (image, label) 쌍이고, 이미지는 3개 채널의 변수 shape이고 label은 스칼라 형태이다.

```
print(raw_train)
print(raw_validation)
print(raw_test)
```
```
&gt; &lt;DatasetV1Adapter shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)&gt;
&gt; &lt;DatasetV1Adapter shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)&gt;
&gt; &lt;DatasetV1Adapter shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)&gt;
```
 
훈련 세트에서 처음 두 개의 이미지와 레이블을 보여줍니다.

```
get_label_name = metadata.features['label'].int2str

for image, label in raw_train.take(2):
  plt.figure()
  plt.imshow(image)
  plt.title(get_label_name(label))
```

&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/transfer_learning_files/output_K5BeQyKThC_Y_0.png&quot;/&gt;&lt;br/&gt;

&lt;img src=&quot;https://www.tensorflow.org/tutorials/images/transfer_learning_files/output_K5BeQyKThC_Y_1.png&quot;/&gt;&lt;br/&gt;

### 데이터 구성 방식
데이터를 구성하기 위해 **tf.image** 모듈을 사용한다.
이미지를 고정된 인풋 사이즈로 resize하고, [-1, 1] 범위의 값으로 rescale한다

```
IMG_SIZE = 160 # All images will be resized to 160x160

def format_example(image, label):
  image = tf.cast(image, tf.float32)
  image = (image/127.5) - 1
  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
  return image, label
```
map 함수를 이용하여 각각의 데이터 셋에 적용한다
```
train = raw_train.map(format_example)
validation = raw_validation.map(format_example)
test = raw_test.map(format_example)
```
셔플과 배치 값을 정한다
```
BATCH_SIZE = 32
SHUFFLE_BUFFER_SIZE = 1000
train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
validation_batches = validation.batch(BATCH_SIZE)
test_batches = test.batch(BATCH_SIZE)
```
데이터의 배치를 검사한다
```
for image_batch, label_batch in train_batches.take(1):
   pass

image_batch.shape
```
```
&gt; TensorShape([32, 160, 160, 3])
```

## 미리 학습된 convnets로 부터 base model 생성하기

너는 구글에서 개발 된 **MobileNet V2** 모델로부터 base model을 만들 수 있을 것이다. 140만개의 이미지와 1000개의 클래스로 이루어진 이미지넷 데이터 셋으로 부터 미리 훈련된 모델이다. ImageNet은 jackfruit와 주사기 같은 넓은 범위의 카테고리를 가진 연구용 학습 데이터 셋이다. 이 지식의 기반은 우리의 데이터셋으로 부터 고양이와 강아지를 구분하는데 도움을 줄 것이다.

먼저, 특징 추출을 위해 사용할 MobileNet V2의 레이어를 선택해야 한다. 대부분의 머신러닝 모델 다이어그램은 bottom에서 top으로 가기에 마지막 classification 레이어(top)은 유용하지 않다. 대신 너는 일반적인 관례에 따라 평탄화 작업(flatten operation) 이전의 마지막 레이어를 사용할 수 있다. 이 레이어는 &quot;bottleneck layer&quot;라고 불리운다. bottleneck 레이어는 final/top 레이어에 비해서 더 일반적인 특징들을 가지고 있고 분류할 수 있다. 

### 일반적인 Convolution Network 구성도
&lt;img src=&quot;https://cdn-images-1.medium.com/fit/t/1600/480/1*vkQ0hXDaQv57sALXAJquxA.jpeg&quot;/&gt;&lt;br/&gt;

### MobileNet V2 구성도
&lt;img src=&quot;https://1.bp.blogspot.com/-M8UvZJWNW4E/WsKk-tbzp8I/AAAAAAAAChw/OqxBVPbDygMIQWGug4ZnHNDvuyK5FBMcQCLcBGAs/s1600/image5.png&quot;/&gt;&lt;br/&gt;

먼저, ImageNet 데이터 셋으로 부터 학습 된 가중치를 가진 MobileNet V2 모델을 인스턴스화 한다. **include_top=False** 옵션값을 줌으로써, 너는 top에 classification 레이어를 포함하지 않는 특징 추출에 가장 최적화 된 네트워크를 불러올 것이다.

```
IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)

# Create the base model from the pre-trained model MobileNet V2
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
```

이 특징 추출기는 160*160*3 이미지를 5*5*1280의 특징들의 블록으로 변환시킨다. 이미지들의 배치가 어떻게 되는지 밑의 예를 통해 확인해봐라

```
feature_batch = base_model(image_batch)
print(feature_batch.shape)
```

```
&gt; (32, 5, 5, 1280)
```

## 특징 추출

이 스텝에서, 너는 이전 스텝으로부터 만들어진 합성곱(convolutional) base를 고정할 것이다. 추가적으로, 너는 가장 상단에 classifier(분류기)를 추가할 것이고, 높은 수준의 classifier를 학습할 것이다.

### 컨볼루션 베이스 고정
모델을 학습하고 컴파일 하기 전에 컨볼루션 베이스를 고정하는 것은 중요하다. layer.trainable = False 옵션을 통하여 트레이닝 도중에 이 레이어의 가중치가 업데이트 되는 것을 방지한다. MobileNet V2는 많은 레이어를 가지고 있으므로 모든 모델의 학습가능 플레그를 False로 세팅하여 모든 레이어를 고정할 수 있다.
```
base_model.trainable = False
```
```
# Let's take a look at the base model architecture
base_model.summary()
```
```
Model: &quot;mobilenetv2_1.00_160&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 160, 160, 3) 0                                            
__________________________________________________________________________________________________
Conv1_pad (ZeroPadding2D)       (None, 161, 161, 3)  0           input_1[0][0]                    
__________________________________________________________________________________________________
Conv1 (Conv2D)                  (None, 80, 80, 32)   864         Conv1_pad[0][0]                  
__________________________________________________________________________________________________
bn_Conv1 (BatchNormalization)   (None, 80, 80, 32)   128         Conv1[0][0]                      
__________________________________________________________________________________________________
Conv1_relu (ReLU)               (None, 80, 80, 32)   0           bn_Conv1[0][0]                   
__________________________________________________________________________________________________
expanded_conv_depthwise (Depthw (None, 80, 80, 32)   288         Conv1_relu[0][0]                 
__________________________________________________________________________________________________
expanded_conv_depthwise_BN (Bat (None, 80, 80, 32)   128         expanded_conv_depthwise[0][0]    
__________________________________________________________________________________________________
expanded_conv_depthwise_relu (R (None, 80, 80, 32)   0           expanded_conv_depthwise_BN[0][0] 
__________________________________________________________________________________________________
expanded_conv_project (Conv2D)  (None, 80, 80, 16)   512         expanded_conv_depthwise_relu[0][0
__________________________________________________________________________________________________
expanded_conv_project_BN (Batch (None, 80, 80, 16)   64          expanded_conv_project[0][0]      
__________________________________________________________________________________________________
block_1_expand (Conv2D)         (None, 80, 80, 96)   1536        expanded_conv_project_BN[0][0]   
__________________________________________________________________________________________________
block_1_expand_BN (BatchNormali (None, 80, 80, 96)   384         block_1_expand[0][0]             
__________________________________________________________________________________________________
block_1_expand_relu (ReLU)      (None, 80, 80, 96)   0           block_1_expand_BN[0][0]          
__________________________________________________________________________________________________
block_1_pad (ZeroPadding2D)     (None, 81, 81, 96)   0           block_1_expand_relu[0][0]        
__________________________________________________________________________________________________
block_1_depthwise (DepthwiseCon (None, 40, 40, 96)   864         block_1_pad[0][0]                
__________________________________________________________________________________________________
block_1_depthwise_BN (BatchNorm (None, 40, 40, 96)   384         block_1_depthwise[0][0]          
__________________________________________________________________________________________________
block_1_depthwise_relu (ReLU)   (None, 40, 40, 96)   0           block_1_depthwise_BN[0][0]       
__________________________________________________________________________________________________
block_1_project (Conv2D)        (None, 40, 40, 24)   2304        block_1_depthwise_relu[0][0]     
__________________________________________________________________________________________________
block_1_project_BN (BatchNormal (None, 40, 40, 24)   96          block_1_project[0][0]            
__________________________________________________________________________________________________
block_2_expand (Conv2D)         (None, 40, 40, 144)  3456        block_1_project_BN[0][0]         
__________________________________________________________________________________________________
block_2_expand_BN (BatchNormali (None, 40, 40, 144)  576         block_2_expand[0][0]             
__________________________________________________________________________________________________
block_2_expand_relu (ReLU)      (None, 40, 40, 144)  0           block_2_expand_BN[0][0]          
__________________________________________________________________________________________________
block_2_depthwise (DepthwiseCon (None, 40, 40, 144)  1296        block_2_expand_relu[0][0]        
__________________________________________________________________________________________________
block_2_depthwise_BN (BatchNorm (None, 40, 40, 144)  576         block_2_depthwise[0][0]          
__________________________________________________________________________________________________
block_2_depthwise_relu (ReLU)   (None, 40, 40, 144)  0           block_2_depthwise_BN[0][0]       
__________________________________________________________________________________________________
block_2_project (Conv2D)        (None, 40, 40, 24)   3456        block_2_depthwise_relu[0][0]     
__________________________________________________________________________________________________
block_2_project_BN (BatchNormal (None, 40, 40, 24)   96          block_2_project[0][0]            
__________________________________________________________________________________________________
block_2_add (Add)               (None, 40, 40, 24)   0           block_1_project_BN[0][0]         
                                                                 block_2_project_BN[0][0]         
__________________________________________________________________________________________________
block_3_expand (Conv2D)         (None, 40, 40, 144)  3456        block_2_add[0][0]                
__________________________________________________________________________________________________
block_3_expand_BN (BatchNormali (None, 40, 40, 144)  576         block_3_expand[0][0]             
__________________________________________________________________________________________________
block_3_expand_relu (ReLU)      (None, 40, 40, 144)  0           block_3_expand_BN[0][0]          
__________________________________________________________________________________________________
block_3_pad (ZeroPadding2D)     (None, 41, 41, 144)  0           block_3_expand_relu[0][0]        
__________________________________________________________________________________________________
block_3_depthwise (DepthwiseCon (None, 20, 20, 144)  1296        block_3_pad[0][0]                
__________________________________________________________________________________________________
block_3_depthwise_BN (BatchNorm (None, 20, 20, 144)  576         block_3_depthwise[0][0]          
__________________________________________________________________________________________________
block_3_depthwise_relu (ReLU)   (None, 20, 20, 144)  0           block_3_depthwise_BN[0][0]       
__________________________________________________________________________________________________
block_3_project (Conv2D)        (None, 20, 20, 32)   4608        block_3_depthwise_relu[0][0]     
__________________________________________________________________________________________________
block_3_project_BN (BatchNormal (None, 20, 20, 32)   128         block_3_project[0][0]            
__________________________________________________________________________________________________
block_4_expand (Conv2D)         (None, 20, 20, 192)  6144        block_3_project_BN[0][0]         
__________________________________________________________________________________________________
block_4_expand_BN (BatchNormali (None, 20, 20, 192)  768         block_4_expand[0][0]             
__________________________________________________________________________________________________
block_4_expand_relu (ReLU)      (None, 20, 20, 192)  0           block_4_expand_BN[0][0]          
__________________________________________________________________________________________________
block_4_depthwise (DepthwiseCon (None, 20, 20, 192)  1728        block_4_expand_relu[0][0]        
__________________________________________________________________________________________________
block_4_depthwise_BN (BatchNorm (None, 20, 20, 192)  768         block_4_depthwise[0][0]          
__________________________________________________________________________________________________
block_4_depthwise_relu (ReLU)   (None, 20, 20, 192)  0           block_4_depthwise_BN[0][0]       
__________________________________________________________________________________________________
block_4_project (Conv2D)        (None, 20, 20, 32)   6144        block_4_depthwise_relu[0][0]     
__________________________________________________________________________________________________
block_4_project_BN (BatchNormal (None, 20, 20, 32)   128         block_4_project[0][0]            
__________________________________________________________________________________________________
block_4_add (Add)               (None, 20, 20, 32)   0           block_3_project_BN[0][0]         
                                                                 block_4_project_BN[0][0]         
__________________________________________________________________________________________________
block_5_expand (Conv2D)         (None, 20, 20, 192)  6144        block_4_add[0][0]                
__________________________________________________________________________________________________
block_5_expand_BN (BatchNormali (None, 20, 20, 192)  768         block_5_expand[0][0]             
__________________________________________________________________________________________________
block_5_expand_relu (ReLU)      (None, 20, 20, 192)  0           block_5_expand_BN[0][0]          
__________________________________________________________________________________________________
block_5_depthwise (DepthwiseCon (None, 20, 20, 192)  1728        block_5_expand_relu[0][0]        
__________________________________________________________________________________________________
block_5_depthwise_BN (BatchNorm (None, 20, 20, 192)  768         block_5_depthwise[0][0]          
__________________________________________________________________________________________________
block_5_depthwise_relu (ReLU)   (None, 20, 20, 192)  0           block_5_depthwise_BN[0][0]       
__________________________________________________________________________________________________
block_5_project (Conv2D)        (None, 20, 20, 32)   6144        block_5_depthwise_relu[0][0]     
__________________________________________________________________________________________________
block_5_project_BN (BatchNormal (None, 20, 20, 32)   128         block_5_project[0][0]            
__________________________________________________________________________________________________
block_5_add (Add)               (None, 20, 20, 32)   0           block_4_add[0][0]                
                                                                 block_5_project_BN[0][0]         
__________________________________________________________________________________________________
block_6_expand (Conv2D)         (None, 20, 20, 192)  6144        block_5_add[0][0]                
__________________________________________________________________________________________________
block_6_expand_BN (BatchNormali (None, 20, 20, 192)  768         block_6_expand[0][0]             
__________________________________________________________________________________________________
block_6_expand_relu (ReLU)      (None, 20, 20, 192)  0           block_6_expand_BN[0][0]          
__________________________________________________________________________________________________
block_6_pad (ZeroPadding2D)     (None, 21, 21, 192)  0           block_6_expand_relu[0][0]        
__________________________________________________________________________________________________
block_6_depthwise (DepthwiseCon (None, 10, 10, 192)  1728        block_6_pad[0][0]                
__________________________________________________________________________________________________
block_6_depthwise_BN (BatchNorm (None, 10, 10, 192)  768         block_6_depthwise[0][0]          
__________________________________________________________________________________________________
block_6_depthwise_relu (ReLU)   (None, 10, 10, 192)  0           block_6_depthwise_BN[0][0]       
__________________________________________________________________________________________________
block_6_project (Conv2D)        (None, 10, 10, 64)   12288       block_6_depthwise_relu[0][0]     
__________________________________________________________________________________________________
block_6_project_BN (BatchNormal (None, 10, 10, 64)   256         block_6_project[0][0]            
__________________________________________________________________________________________________
block_7_expand (Conv2D)         (None, 10, 10, 384)  24576       block_6_project_BN[0][0]         
__________________________________________________________________________________________________
block_7_expand_BN (BatchNormali (None, 10, 10, 384)  1536        block_7_expand[0][0]             
__________________________________________________________________________________________________
block_7_expand_relu (ReLU)      (None, 10, 10, 384)  0           block_7_expand_BN[0][0]          
__________________________________________________________________________________________________
block_7_depthwise (DepthwiseCon (None, 10, 10, 384)  3456        block_7_expand_relu[0][0]        
__________________________________________________________________________________________________
block_7_depthwise_BN (BatchNorm (None, 10, 10, 384)  1536        block_7_depthwise[0][0]          
__________________________________________________________________________________________________
block_7_depthwise_relu (ReLU)   (None, 10, 10, 384)  0           block_7_depthwise_BN[0][0]       
__________________________________________________________________________________________________
block_7_project (Conv2D)        (None, 10, 10, 64)   24576       block_7_depthwise_relu[0][0]     
__________________________________________________________________________________________________
block_7_project_BN (BatchNormal (None, 10, 10, 64)   256         block_7_project[0][0]            
__________________________________________________________________________________________________
block_7_add (Add)               (None, 10, 10, 64)   0           block_6_project_BN[0][0]         
                                                                 block_7_project_BN[0][0]         
__________________________________________________________________________________________________
block_8_expand (Conv2D)         (None, 10, 10, 384)  24576       block_7_add[0][0]                
__________________________________________________________________________________________________
block_8_expand_BN (BatchNormali (None, 10, 10, 384)  1536        block_8_expand[0][0]             
__________________________________________________________________________________________________
block_8_expand_relu (ReLU)      (None, 10, 10, 384)  0           block_8_expand_BN[0][0]          
__________________________________________________________________________________________________
block_8_depthwise (DepthwiseCon (None, 10, 10, 384)  3456        block_8_expand_relu[0][0]        
__________________________________________________________________________________________________
block_8_depthwise_BN (BatchNorm (None, 10, 10, 384)  1536        block_8_depthwise[0][0]          
__________________________________________________________________________________________________
block_8_depthwise_relu (ReLU)   (None, 10, 10, 384)  0           block_8_depthwise_BN[0][0]       
__________________________________________________________________________________________________
block_8_project (Conv2D)        (None, 10, 10, 64)   24576       block_8_depthwise_relu[0][0]     
__________________________________________________________________________________________________
block_8_project_BN (BatchNormal (None, 10, 10, 64)   256         block_8_project[0][0]            
__________________________________________________________________________________________________
block_8_add (Add)               (None, 10, 10, 64)   0           block_7_add[0][0]                
                                                                 block_8_project_BN[0][0]         
__________________________________________________________________________________________________
block_9_expand (Conv2D)         (None, 10, 10, 384)  24576       block_8_add[0][0]                
__________________________________________________________________________________________________
block_9_expand_BN (BatchNormali (None, 10, 10, 384)  1536        block_9_expand[0][0]             
__________________________________________________________________________________________________
block_9_expand_relu (ReLU)      (None, 10, 10, 384)  0           block_9_expand_BN[0][0]          
__________________________________________________________________________________________________
block_9_depthwise (DepthwiseCon (None, 10, 10, 384)  3456        block_9_expand_relu[0][0]        
__________________________________________________________________________________________________
block_9_depthwise_BN (BatchNorm (None, 10, 10, 384)  1536        block_9_depthwise[0][0]          
__________________________________________________________________________________________________
block_9_depthwise_relu (ReLU)   (None, 10, 10, 384)  0           block_9_depthwise_BN[0][0]       
__________________________________________________________________________________________________
block_9_project (Conv2D)        (None, 10, 10, 64)   24576       block_9_depthwise_relu[0][0]     
__________________________________________________________________________________________________
block_9_project_BN (BatchNormal (None, 10, 10, 64)   256         block_9_project[0][0]            
__________________________________________________________________________________________________
block_9_add (Add)               (None, 10, 10, 64)   0           block_8_add[0][0]                
                                                                 block_9_project_BN[0][0]         
__________________________________________________________________________________________________
block_10_expand (Conv2D)        (None, 10, 10, 384)  24576       block_9_add[0][0]                
__________________________________________________________________________________________________
block_10_expand_BN (BatchNormal (None, 10, 10, 384)  1536        block_10_expand[0][0]            
__________________________________________________________________________________________________
block_10_expand_relu (ReLU)     (None, 10, 10, 384)  0           block_10_expand_BN[0][0]         
__________________________________________________________________________________________________
block_10_depthwise (DepthwiseCo (None, 10, 10, 384)  3456        block_10_expand_relu[0][0]       
__________________________________________________________________________________________________
block_10_depthwise_BN (BatchNor (None, 10, 10, 384)  1536        block_10_depthwise[0][0]         
__________________________________________________________________________________________________
block_10_depthwise_relu (ReLU)  (None, 10, 10, 384)  0           block_10_depthwise_BN[0][0]      
__________________________________________________________________________________________________
block_10_project (Conv2D)       (None, 10, 10, 96)   36864       block_10_depthwise_relu[0][0]    
__________________________________________________________________________________________________
block_10_project_BN (BatchNorma (None, 10, 10, 96)   384         block_10_project[0][0]           
__________________________________________________________________________________________________
block_11_expand (Conv2D)        (None, 10, 10, 576)  55296       block_10_project_BN[0][0]        
__________________________________________________________________________________________________
block_11_expand_BN (BatchNormal (None, 10, 10, 576)  2304        block_11_expand[0][0]            
__________________________________________________________________________________________________
block_11_expand_relu (ReLU)     (None, 10, 10, 576)  0           block_11_expand_BN[0][0]         
__________________________________________________________________________________________________
block_11_depthwise (DepthwiseCo (None, 10, 10, 576)  5184        block_11_expand_relu[0][0]       
__________________________________________________________________________________________________
block_11_depthwise_BN (BatchNor (None, 10, 10, 576)  2304        block_11_depthwise[0][0]         
__________________________________________________________________________________________________
block_11_depthwise_relu (ReLU)  (None, 10, 10, 576)  0           block_11_depthwise_BN[0][0]      
__________________________________________________________________________________________________
block_11_project (Conv2D)       (None, 10, 10, 96)   55296       block_11_depthwise_relu[0][0]    
__________________________________________________________________________________________________
block_11_project_BN (BatchNorma (None, 10, 10, 96)   384         block_11_project[0][0]           
__________________________________________________________________________________________________
block_11_add (Add)              (None, 10, 10, 96)   0           block_10_project_BN[0][0]        
                                                                 block_11_project_BN[0][0]        
__________________________________________________________________________________________________
block_12_expand (Conv2D)        (None, 10, 10, 576)  55296       block_11_add[0][0]               
__________________________________________________________________________________________________
block_12_expand_BN (BatchNormal (None, 10, 10, 576)  2304        block_12_expand[0][0]            
__________________________________________________________________________________________________
block_12_expand_relu (ReLU)     (None, 10, 10, 576)  0           block_12_expand_BN[0][0]         
__________________________________________________________________________________________________
block_12_depthwise (DepthwiseCo (None, 10, 10, 576)  5184        block_12_expand_relu[0][0]       
__________________________________________________________________________________________________
block_12_depthwise_BN (BatchNor (None, 10, 10, 576)  2304        block_12_depthwise[0][0]         
__________________________________________________________________________________________________
block_12_depthwise_relu (ReLU)  (None, 10, 10, 576)  0           block_12_depthwise_BN[0][0]      
__________________________________________________________________________________________________
block_12_project (Conv2D)       (None, 10, 10, 96)   55296       block_12_depthwise_relu[0][0]    
__________________________________________________________________________________________________
block_12_project_BN (BatchNorma (None, 10, 10, 96)   384         block_12_project[0][0]           
__________________________________________________________________________________________________
block_12_add (Add)              (None, 10, 10, 96)   0           block_11_add[0][0]               
                                                                 block_12_project_BN[0][0]        
__________________________________________________________________________________________________
block_13_expand (Conv2D)        (None, 10, 10, 576)  55296       block_12_add[0][0]               
__________________________________________________________________________________________________
block_13_expand_BN (BatchNormal (None, 10, 10, 576)  2304        block_13_expand[0][0]            
__________________________________________________________________________________________________
block_13_expand_relu (ReLU)     (None, 10, 10, 576)  0           block_13_expand_BN[0][0]         
__________________________________________________________________________________________________
block_13_pad (ZeroPadding2D)    (None, 11, 11, 576)  0           block_13_expand_relu[0][0]       
__________________________________________________________________________________________________
block_13_depthwise (DepthwiseCo (None, 5, 5, 576)    5184        block_13_pad[0][0]               
__________________________________________________________________________________________________
block_13_depthwise_BN (BatchNor (None, 5, 5, 576)    2304        block_13_depthwise[0][0]         
__________________________________________________________________________________________________
block_13_depthwise_relu (ReLU)  (None, 5, 5, 576)    0           block_13_depthwise_BN[0][0]      
__________________________________________________________________________________________________
block_13_project (Conv2D)       (None, 5, 5, 160)    92160       block_13_depthwise_relu[0][0]    
__________________________________________________________________________________________________
block_13_project_BN (BatchNorma (None, 5, 5, 160)    640         block_13_project[0][0]           
__________________________________________________________________________________________________
block_14_expand (Conv2D)        (None, 5, 5, 960)    153600      block_13_project_BN[0][0]        
__________________________________________________________________________________________________
block_14_expand_BN (BatchNormal (None, 5, 5, 960)    3840        block_14_expand[0][0]            
__________________________________________________________________________________________________
block_14_expand_relu (ReLU)     (None, 5, 5, 960)    0           block_14_expand_BN[0][0]         
__________________________________________________________________________________________________
block_14_depthwise (DepthwiseCo (None, 5, 5, 960)    8640        block_14_expand_relu[0][0]       
__________________________________________________________________________________________________
block_14_depthwise_BN (BatchNor (None, 5, 5, 960)    3840        block_14_depthwise[0][0]         
__________________________________________________________________________________________________
block_14_depthwise_relu (ReLU)  (None, 5, 5, 960)    0           block_14_depthwise_BN[0][0]      
__________________________________________________________________________________________________
block_14_project (Conv2D)       (None, 5, 5, 160)    153600      block_14_depthwise_relu[0][0]    
__________________________________________________________________________________________________
block_14_project_BN (BatchNorma (None, 5, 5, 160)    640         block_14_project[0][0]           
__________________________________________________________________________________________________
block_14_add (Add)              (None, 5, 5, 160)    0           block_13_project_BN[0][0]        
                                                                 block_14_project_BN[0][0]        
__________________________________________________________________________________________________
block_15_expand (Conv2D)        (None, 5, 5, 960)    153600      block_14_add[0][0]               
__________________________________________________________________________________________________
block_15_expand_BN (BatchNormal (None, 5, 5, 960)    3840        block_15_expand[0][0]            
__________________________________________________________________________________________________
block_15_expand_relu (ReLU)     (None, 5, 5, 960)    0           block_15_expand_BN[0][0]         
__________________________________________________________________________________________________
block_15_depthwise (DepthwiseCo (None, 5, 5, 960)    8640        block_15_expand_relu[0][0]       
__________________________________________________________________________________________________
block_15_depthwise_BN (BatchNor (None, 5, 5, 960)    3840        block_15_depthwise[0][0]         
__________________________________________________________________________________________________
block_15_depthwise_relu (ReLU)  (None, 5, 5, 960)    0           block_15_depthwise_BN[0][0]      
__________________________________________________________________________________________________
block_15_project (Conv2D)       (None, 5, 5, 160)    153600      block_15_depthwise_relu[0][0]    
__________________________________________________________________________________________________
block_15_project_BN (BatchNorma (None, 5, 5, 160)    640         block_15_project[0][0]           
__________________________________________________________________________________________________
block_15_add (Add)              (None, 5, 5, 160)    0           block_14_add[0][0]               
                                                                 block_15_project_BN[0][0]        
__________________________________________________________________________________________________
block_16_expand (Conv2D)        (None, 5, 5, 960)    153600      block_15_add[0][0]               
__________________________________________________________________________________________________
block_16_expand_BN (BatchNormal (None, 5, 5, 960)    3840        block_16_expand[0][0]            
__________________________________________________________________________________________________
block_16_expand_relu (ReLU)     (None, 5, 5, 960)    0           block_16_expand_BN[0][0]         
__________________________________________________________________________________________________
block_16_depthwise (DepthwiseCo (None, 5, 5, 960)    8640        block_16_expand_relu[0][0]       
__________________________________________________________________________________________________
block_16_depthwise_BN (BatchNor (None, 5, 5, 960)    3840        block_16_depthwise[0][0]         
__________________________________________________________________________________________________
block_16_depthwise_relu (ReLU)  (None, 5, 5, 960)    0           block_16_depthwise_BN[0][0]      
__________________________________________________________________________________________________
block_16_project (Conv2D)       (None, 5, 5, 320)    307200      block_16_depthwise_relu[0][0]    
__________________________________________________________________________________________________
block_16_project_BN (BatchNorma (None, 5, 5, 320)    1280        block_16_project[0][0]           
__________________________________________________________________________________________________
Conv_1 (Conv2D)                 (None, 5, 5, 1280)   409600      block_16_project_BN[0][0]        
__________________________________________________________________________________________________
Conv_1_bn (BatchNormalization)  (None, 5, 5, 1280)   5120        Conv_1[0][0]                     
__________________________________________________________________________________________________
out_relu (ReLU)                 (None, 5, 5, 1280)   0           Conv_1_bn[0][0]                  
==================================================================================================
Total params: 2,257,984
Trainable params: 0
Non-trainable params: 2,257,984
```

### 분류 헤드를 추가하기(Add a classification head)
특징 블록으로 부터 예측기를 생성하기 위해서 5*5 공간 위치를 평균화 한다. 평균화를 위하여 **tf.keras.layers.GlobalAveragePooling2D**를 사용하고 이 레이어는 feature들을 이미지 한장당 1280개의 요소를 가지는 벡터로 변환한다.

```
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
feature_batch_average = global_average_layer(feature_batch)
print(feature_batch_average.shape)
```
```
&gt; (32, 1280)
```
이 feature들을 이미지 하나당 하나의 예측으로 변환하기 위하여 **tf.keras.layers.Dense** 를 적용한다. 여기서 활성화 함수를 사용할 필요는 없다 왜냐하면 이 예측은 logit 또는 원시 예측 값을 가지고 있을 것이기 때문이다. 양수는 class 1을 예측하고 음수는 class 0을 예측한다
```
prediction_layer = tf.keras.layers.Dense(1)
prediction_batch = prediction_layer(feature_batch_average)
print(prediction_batch.shape)
```
```
&gt; (32, 1)
```
이제 특징 추출기와 앞의 2 layer를 **tf.keras.Sequential** 모델을 사용하여 쌓는다.
```
model = tf.keras.Sequential([
  base_model,
  global_average_layer,
  prediction_layer
])
```

### 모델 컴파일하기
너는 트레이닝 작업 전에 반드시 컴파일을 해야한다. 이 모델은 linear output(선형 출력)을 제공하기 때문에 from_logits=True 옵션으로 이진 cross-entropy 손실함수를 사용해라.
```
base_learning_rate = 0.0001
model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
```
```
model.summary()
```
```
Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
mobilenetv2_1.00_160 (Model) (None, 5, 5, 1280)        2257984   
_________________________________________________________________
global_average_pooling2d (Gl (None, 1280)              0         
_________________________________________________________________
dense (Dense)                (None, 1)                 1281      
=================================================================
Total params: 2,259,265
Trainable params: 1,281
Non-trainable params: 2,257,984
________________________________________________________________
```

MobileNet의 250만개의 파라메터는 고정되었지만, 1200개의 학습가능한 파라메터가 Dense 레이어에 남아있다. 또한 이것은 가중치와 편향으로 두개의 tf.Variable 객체로 나눌 수 있다.
```
len(model.trainable_variables)
```
```
&gt; 2
```

### 모델 학습하기
10 에폭을 학습하면 너는 96% 이하의 정확도를 얻을 수 있다.
```
initial_epochs = 10
validation_steps=20

loss0,accuracy0 = model.evaluate(validation_batches, steps = validation_steps)
```
```
20/20 [==============================] - 1s 63ms/step - loss: 0.6461 - accuracy: 0.5469
```
```
print(&quot;initial loss: {:.2f}&quot;.format(loss0))
print(&quot;initial accuracy: {:.2f}&quot;.format(accuracy0))
```
```
initial loss: 0.65
initial accuracy: 0.55
```
```
history = model.fit(train_batches,
                    epochs=initial_epochs,
                    validation_data=validation_batches)
```
```
Epoch 1/10
582/582 [==============================] - 16s 27ms/step - loss: 0.3501 - accuracy: 0.8310 - val_loss: 0.1772 - val_accuracy: 0.8964
Epoch 2/10
582/582 [==============================] - 14s 24ms/step - loss: 0.1947 - accuracy: 0.9200 - val_loss: 0.1358 - val_accuracy: 0.9243
Epoch 3/10
582/582 [==============================] - 13s 23ms/step - loss: 0.1623 - accuracy: 0.9320 - val_loss: 0.1295 - val_accuracy: 0.9304
Epoch 4/10
582/582 [==============================] - 13s 22ms/step - loss: 0.1457 - accuracy: 0.9393 - val_loss: 0.1162 - val_accuracy: 0.9368
Epoch 5/10
582/582 [==============================] - 13s 22ms/step - loss: 0.1366 - accuracy: 0.9428 - val_loss: 0.1105 - val_accuracy: 0.9424
Epoch 6/10
582/582 [==============================] - 13s 23ms/step - loss: 0.1343 - accuracy: 0.9429 - val_loss: 0.1065 - val_accuracy: 0.9445
Epoch 7/10
582/582 [==============================] - 14s 24ms/step - loss: 0.1256 - accuracy: 0.9471 - val_loss: 0.1034 - val_accuracy: 0.9475
Epoch 8/10
582/582 [==============================] - 13s 23ms/step - loss: 0.1236 - accuracy: 0.9506 - val_loss: 0.1101 - val_accuracy: 0.9458
Epoch 9/10
582/582 [==============================] - 13s 22ms/step - loss: 0.1227 - accuracy: 0.9474 - val_loss: 0.0998 - val_accuracy: 0.9523
Epoch 10/10
582/582 [==============================] - 13s 23ms/step - loss: 0.1211 - accuracy: 0.9483 - val_loss: 0.1031 - val_accuracy: 0.9514
```

## 미세 조정 ( Fine tuning )
특징 추출 실험에서 너는 MobileNet V2 base model의 상단 레이어 몇개만 학습시켰다. 사전 훈련된 네트워크의 가중치는 학습되지 않았다.

성능을 더욱 향상시키는 한 가지 방법은 우리가 추가 한 분류기의 훈련과 함께 사전 훈련 된 모델의 최상위 레이어 가중치를 훈련 (또는 &quot;미세 조정&quot;)하는 것입니다. 훈련 과정을 통해 가중치는 일반적인(generic) 특징을 포함한 맵에서 훈련하려는 데이터 집합(cats and dogs)과 관련된 기능으로 강제 조정됩니다.

* Note
사전 훈련 된 모델을 훈련 불가능으로 설정하여 최상위 분류기(classifier)를 훈련 한 뒤에만 ​시도해야합니다. 사전 훈련 된 모델 위에 무작위로 초기화 된 분류기를 추가하고 모든 레이어를 공동으로 훈련하려고하면 경사하강법 업데이트의 크기가 너무 클 수 있으며 (분류기-classifier 의 임의 가중치로 인해) 사전 훈련 된 모델은 배운 것을 잊어 버릴 수 있습니다.

또한 전체 MobileNet 모델이 아닌 소수의 최상위 계층을 미세 조정해야합니다. 대부분의 convolution 네트워크에서 계층이 높을수록 계층이 더 전문화됩니다. 처음 몇 층은 거의 모든 유형의 이미지로 일반화되는 매우 간단하고 일반적인 기능을 학습합니다. 더 높은 수준으로 올라가면 기능이 모델이 훈련 된 데이터 세트에 점점 더 구체적이됩니다. 미세 조정(fine tuning)의 목표는 이러한 특수 기능을 일반 학습을 덮어 쓰지 않고 새 데이터 세트와 함께 사용할 수 있도록 조정하는 것입니다.

### 모델의 상단 레이어 고정 해제
base_model 고정을 해제하고 bottom 레이어를 훈련 할 수 없도록 설정하기 만하면됩니다. 그런 다음 모델을 다시 컴파일하고 훈련을 다시 시작해야합니다.
```
base_model.trainable = True
```
```
# base model에 얼마나 많은 layer가 존재하는지 확인한다
print(&quot;Number of layers in the base model: &quot;, len(base_model.layers))

# 미세조정을 시작할 레이어 위치를 정한다
fine_tune_at = 100

# 미세조정을 시작하기 전 레이어들은 다 고정한다
for layer in base_model.layers[:fine_tune_at]:
  layer.trainable =  False
```
```
&gt; Number of layers in the base model:  155
```

### 모델 컴파일하기
더 낮은 학습률(learning rate)를 적용하여 모델을 컴파일한다
```
model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate/10),
              metrics=['accuracy'])
```
```
model.summary()
```
```
Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
mobilenetv2_1.00_160 (Model) (None, 5, 5, 1280)        2257984   
_________________________________________________________________
global_average_pooling2d (Gl (None, 1280)              0         
_________________________________________________________________
dense (Dense)                (None, 1)                 1281      
=================================================================
Total params: 2,259,265
Trainable params: 1,863,873
Non-trainable params: 395,392
_________________________________________________________________
```
```
len(model.trainable_variables)
```
```
58
```

### 계속해서 모델 학습시키기
```
fine_tune_epochs = 10
total_epochs =  initial_epochs + fine_tune_epochs

history_fine = model.fit(train_batches,
                         epochs=total_epochs,
                         initial_epoch =  history.epoch[-1],
                         validation_data=validation_batches)
```

## 요약
* 특징 추출을 위해 사전 훈련 된 모델 사용 : 작은 데이터 세트로 작업 할 때 동일한 도메인에서 더 큰 데이터 세트에 대해 훈련 된 모델에서 학습 한 기능을 활용하는 것이 일반적입니다. 사전 훈련 된 모델을 인스턴스화하고 완전히 연결된 분류기를 맨 위에 추가하면됩니다. 사전 훈련 된 모델은 &quot;동결&quot;되고 분류기의 가중치 만 훈련 중에 업데이트됩니다. 이 경우 컨벌루션베이스는 각 이미지와 관련된 모든 기능을 추출했으며 추출 된 기능 세트가 제공된 이미지 클래스를 결정하는 분류기를 훈련했습니다.

* 사전 훈련 된 모델 미세 조정 : 성능을 더욱 향상시키기 위해 사전 훈련 된 모델의 최상위 계층을 미세 조정을 통해 새로운 데이터 세트로 재사용 할 수 있습니다. 이 경우 모델이 데이터 세트와 관련된 고급 기능을 학습 할 수 있도록 가중치를 조정했습니다. 이 기술은 일반적으로 훈련 데이터 세트가 크고 사전 훈련 된 모델이 훈련 된 원래 데이터 세트와 매우 유사한 경우에 권장됩니다.</description>
            <pubDate></pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/image_transfer_learning_preTrained.html</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/image_transfer_learning_preTrained.html</guid>
            
            
        </item>
        
        <item>
            <title>About</title>
            <description># 차곡차곡 쌓아가는 기술블로그입니다
&lt;hr/&gt;

## 귀여운 반려묘와 함께 지내고 있습니다
&lt;img src=&quot;/assets/images/java.jpg&quot; width=&quot;40%&quot; height=&quot;30%&quot;/&gt;

### 한쪽만 파기보단 다양한 기술에 관심이 많고 도큐먼트를 좋아합니다
### email : kdwooa@gmail.com
</description>
            <pubDate></pubDate>
            <link>https://east-rain.github.io/</link>
            <guid isPermaLink="true">https://east-rain.github.io/</guid>
            
            
        </item>
        
        <item>
            <title>딥러닝 기초</title>
            <description></description>
            <pubDate></pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/basic%20deeplearning/</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/basic%20deeplearning/</guid>
            
            
        </item>
        
        <item>
            <title>텐서플로우 튜토리얼</title>
            <description></description>
            <pubDate></pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/</guid>
            
            
        </item>
        
        <item>
            <title>Programming</title>
            <description></description>
            <pubDate></pubDate>
            <link>https://east-rain.github.io/docs/Programming/</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Programming/</guid>
            
            
        </item>
        
        <item>
            <title>Deep Learning</title>
            <description></description>
            <pubDate></pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/</guid>
            
            
        </item>
        
        <item>
            <title>텐서플로우 설치 - windows</title>
            <description># 텐서플로우 설치 - windows

1. Python 3 설치 - anaconda 사용&lt;br&gt;
https://www.anaconda.com/distribution/ 에서 anaconda installer 다운로드 및 설치

2. NVIDIA GPU driver 설치&lt;br&gt;
https://www.nvidia.co.kr/Download/index.aspx?lang=kr

3. NVIDIA CUDA Toolkit 설치&lt;br&gt;
https://developer.nvidia.com/cuda-downloads

4. cuDDN 설치&lt;br&gt;
https://developer.nvidia.com/cudnn&lt;br&gt;
다운받은 압축파일을 적당한 위치에 푼다.
여기서는 C:\tools/cuda 에 풀겠다.

5. 환경변수 추가&lt;br&gt;
C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\bin;&lt;br&gt;
C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\v10.2\\extras\\CUPTI\\libx64;&lt;br&gt;
C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\include;&lt;br&gt;
C:\\tools\\cuda\\bin;&lt;br&gt;
해당 경로들을 환경변수에 추가한다.

6. 텐서플로우 설치 - pip&lt;br&gt;
```
$ pip install tensorflow
```

텐서플로우2 버전부터는 해당 명령어로 CPU와 GPU를 모두 설치해준다

7. 설치 확인
```
$ python3 -c &quot;import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))&quot;

&gt; 2020-04-21 20:36:09.675162: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
  tf.Tensor(-342.69507, shape=(), dtype=float32)
```
결과가 나오면 설치가 잘 된것이다.
위에 I 메시지는 내 cpu는 더 많은 명령어를 지원하지만, pip로 받은 텐서플로우 빌드 버전에서는 해당 명령어를 사용하지 못한다는 의미이다. 만약 해당 메시지가 싫거나 좀 더 빠른 동작을 원한다면 텐서플로우 소스를 받아서 빌드하면 된다.
</description>
            <pubDate></pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/install_tensorflow.html</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/install_tensorflow.html</guid>
            
            
        </item>
        
        <item>
            <title>최적화 함수들(optimization)</title>
            <description># 최적화 함수들(optimization)
신경망 학습의 목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것이며, 이러한 문제를 푸는 것을 최적화(optimization)이라고 한다.

## 확률적 경사 하강법 - SGD(Stochastic Gradient Descent)
최적의 매개변수 값을 찾는 단서로 매개변수의 기울기(미분)을 이용. 매개변수의 기울기를 구해, 기울어진 방향으로 매개변수 값을 갱신하는 일을 계속 반복한다.

```
W &lt;- W - ( learning rate * dL / dW )

W : 가중치
L : 손실 함수 
```

#### SGD의 단점
&lt;img src=&quot;optimization/sgd-1.png&quot;/&gt;
&lt;img src=&quot;optimization/sgd-2.png&quot;/&gt;

심하게 굽이진 움직임을 보여준다. 따라서 이러한 경우에는 조금 비효율 적이다.

## 모멘텀(Momentum)
```
v &lt;- av - ( learning rate * dL / dW )
W &lt;- W + v

W : 가중치
L : 손실함수
```

SGD와 차이점을 보면 av 값을 더해준게 눈에 띈다. 여기서 a는 고정된 상수값이고(ex 0.9) v는 물체의 속도라고 생각하면 된다.
해당 방향으로 진행할 수록 공이 기울기를 따라 구르듯 힘을 받는다.





</description>
            <pubDate></pubDate>
            <link>https://east-rain.github.io/docs/Deep%20Learning/basic%20deeplearning/optimization.html</link>
            <guid isPermaLink="true">https://east-rain.github.io/docs/Deep%20Learning/basic%20deeplearning/optimization.html</guid>
            
            
        </item>
        
    </channel>
</rss>