<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <title>최적화 함수들(optimization) - 차곡차곡 쌓자</title> <link rel="shortcut icon" href="https://east-rain.github.io/favicon.ico" type="image/x-icon"> <link rel="stylesheet" href="https://east-rain.github.io/assets/css/just-the-docs.css"> <link rel="alternate" type="application/rss+xml" href="https://east-rain.github.io/feed.xml" title="차곡차곡 쌓자 Feed"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-163902491-1"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', "UA-163902491-1"); </script> <script type="text/javascript" src="https://east-rain.github.io/assets/js/vendor/lunr.min.js" charset="utf-8"></script> <script type="text/javascript" src="https://east-rain.github.io/assets/js/just-the-docs.js" charset="utf-8"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <meta name="naver-site-verification" content="0ec4078b2caa70b3c1bd0053083cb7b3ed281f42" /> <!-- Begin Jekyll SEO tag v2.6.1 --> <title>최적화 함수들(optimization) | 차곡차곡 쌓자</title> <meta name="generator" content="Jekyll v3.8.6" /> <meta property="og:title" content="최적화 함수들(optimization)" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="차곡차곡 쌓아가는 기술블로그입니다. 되도록 정확한 자료만을 정리하려고 노력합니다." /> <meta property="og:description" content="차곡차곡 쌓아가는 기술블로그입니다. 되도록 정확한 자료만을 정리하려고 노력합니다." /> <link rel="canonical" href="https://east-rain.github.io/docs/Deep%20Learning/basic%20deeplearning/optimization.html" /> <meta property="og:url" content="https://east-rain.github.io/docs/Deep%20Learning/basic%20deeplearning/optimization.html" /> <meta property="og:site_name" content="차곡차곡 쌓자" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2020-04-23T00:00:00+09:00" /> <script type="application/ld+json"> {"headline":"최적화 함수들(optimization)","dateModified":"2020-04-23T00:00:00+09:00","datePublished":"2020-04-23T00:00:00+09:00","description":"차곡차곡 쌓아가는 기술블로그입니다. 되도록 정확한 자료만을 정리하려고 노력합니다.","url":"https://east-rain.github.io/docs/Deep%20Learning/basic%20deeplearning/optimization.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://east-rain.github.io/docs/Deep%20Learning/basic%20deeplearning/optimization.html"},"@context":"https://schema.org"}</script> <!-- End Jekyll SEO tag --> </head> <body> <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> <symbol id="link" viewBox="0 0 16 16"> <title>Link</title> <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path> </symbol> </svg> <div class="page-wrap"> <div class="side-bar"> <div class="site-header"> <a href="https://east-rain.github.io" class="site-title lh-tight"> 차곡차곡 쌓자 </a> <button class="menu-button fs-3 js-main-nav-trigger" data-text-toggle="Hide" type="button">Menu</button> </div> <div class="navigation main-nav js-main-nav"> <nav role="navigation" aria-label="Main navigation"> <ul class="navigation-list"><li class="navigation-list-item"><a href="https://east-rain.github.io/404.html" class="navigation-list-link"></a></li><li class="navigation-list-item"><a href="https://east-rain.github.io/" class="navigation-list-link">About</a></li><li class="navigation-list-item active"><a href="https://east-rain.github.io/docs/Deep%20Learning/" class="navigation-list-link">Deep Learning</a><ul class="navigation-list-child-list "><li class="navigation-list-item active"><a href="https://east-rain.github.io/docs/Deep%20Learning/basic%20deeplearning/" class="navigation-list-link">딥러닝 기초</a><ul class="navigation-list-child-list"><li class="navigation-list-item active"> <a href="https://east-rain.github.io/docs/Deep%20Learning/basic%20deeplearning/optimization.html" class="navigation-list-link active">최적화 함수들(optimization)</a> </li></ul></li><li class="navigation-list-item "><a href="https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/" class="navigation-list-link">텐서플로우 튜토리얼</a><ul class="navigation-list-child-list"><li class="navigation-list-item "> <a href="https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/install_tensorflow.html" class="navigation-list-link">텐서플로우 설치 - windows</a> </li><li class="navigation-list-item "> <a href="https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/image_classification.html" class="navigation-list-link">텐서플로우 이미지 분류하기</a> </li><li class="navigation-list-item "> <a href="https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/image_transfer_learning_TFHub.html" class="navigation-list-link">텐서플로우 허브 전이학습</a> </li><li class="navigation-list-item "> <a href="https://east-rain.github.io/docs/Deep%20Learning/tensorflow%20tutorial/image_transfer_learning_preTrained.html" class="navigation-list-link">텐서플로우 전이학습</a> </li></ul></li><li class="navigation-list-item "><a href="https://east-rain.github.io/docs/Deep%20Learning/Yolo(darknet)/" class="navigation-list-link">Yolo(darknet)</a><ul class="navigation-list-child-list"><li class="navigation-list-item "> <a href="https://east-rain.github.io/docs/Deep%20Learning/Yolo(darknet)/introduction.html" class="navigation-list-link">개요</a> </li><li class="navigation-list-item "> <a href="https://east-rain.github.io/docs/Deep%20Learning/Yolo(darknet)/environment.html" class="navigation-list-link">환경설정</a> </li><li class="navigation-list-item "> <a href="https://east-rain.github.io/docs/Deep%20Learning/Yolo(darknet)/training.html" class="navigation-list-link">트레이닝</a> </li></ul></li><li class="navigation-list-item "><a href="https://east-rain.github.io/docs/Deep%20Learning/vector%20search/" class="navigation-list-link">이미지 유사도 검색</a></li><li class="navigation-list-item "><a href="https://east-rain.github.io/docs/Deep%20Learning/posenet/" class="navigation-list-link">PoseNet</a></li><li class="navigation-list-item "><a href="https://east-rain.github.io/docs/Deep%20Learning/deeplabcut/" class="navigation-list-link">DeepLabCut</a><ul class="navigation-list-child-list"><li class="navigation-list-item "> <a href="https://east-rain.github.io/docs/Deep%20Learning/deeplabcut/introduction.html" class="navigation-list-link">개요 및 사용법</a> </li><li class="navigation-list-item "> <a href="https://east-rain.github.io/docs/Deep%20Learning/deeplabcut/deepercut.html" class="navigation-list-link">deepercut(base model)</a> </li></ul></li><li class="navigation-list-item "><a href="https://east-rain.github.io/docs/Deep%20Learning/Openpose/" class="navigation-list-link">Openpose</a><ul class="navigation-list-child-list"><li class="navigation-list-item "> <a href="https://east-rain.github.io/docs/Deep%20Learning/Openpose/openpose.html" class="navigation-list-link">개요 및 사용법</a> </li></ul></li></ul></li><li class="navigation-list-item"><a href="https://east-rain.github.io/docs/Programming/" class="navigation-list-link">Programming</a><ul class="navigation-list-child-list "></ul></li></ul> </nav> </div> <footer class="site-footer"> <p class="text-small text-grey-dk-000 mb-4">This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</p> </footer> </div> <div class="main-content-wrap js-main-content" tabindex="0"> <div class="main-content"> <div class="page-header js-page-header"> <div class="search"> <div class="search-input-wrap"> <input type="text" class="js-search-input search-input" tabindex="0" placeholder="Search" aria-label="Search" autocomplete="off"> <svg width="14" height="14" viewBox="0 0 28 28" xmlns="http://www.w3.org/2000/svg" class="search-icon"><title>Search</title><g fill-rule="nonzero"><path d="M17.332 20.735c-5.537 0-10-4.6-10-10.247 0-5.646 4.463-10.247 10-10.247 5.536 0 10 4.601 10 10.247s-4.464 10.247-10 10.247zm0-4c3.3 0 6-2.783 6-6.247 0-3.463-2.7-6.247-6-6.247s-6 2.784-6 6.247c0 3.464 2.7 6.247 6 6.247z"/><path d="M11.672 13.791L.192 25.271 3.02 28.1 14.5 16.62z"/></g></svg> </div> <div class="js-search-results search-results-wrap"></div> </div> </div> <div class="page"> <nav class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="https://east-rain.github.io/docs/Deep%20Learning/">Deep Learning</a></li> <li class="breadcrumb-nav-list-item"><a href="https://east-rain.github.io/docs/Deep%20Learning/basic%20deeplearning/">딥러닝 기초</a></li> <li class="breadcrumb-nav-list-item"><span>최적화 함수들(optimization)</span></li> </ol> </nav> <div id="main-content" class="page-content" role="main"> <!-- <h1 id="최적화-함수들optimization">최적화 함수들(optimization)</h1> <p>신경망 학습의 목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것이며, 이러한 문제를 푸는 것을 최적화(optimization)이라고 한다.</p> <h2 id="확률적-경사-하강법---sgdstochastic-gradient-descent"> <a href="#확률적-경사-하강법---sgdstochastic-gradient-descent" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> 확률적 경사 하강법 - SGD(Stochastic Gradient Descent) </h2> <p>최적의 매개변수 값을 찾는 단서로 매개변수의 기울기(미분)을 이용. 매개변수의 기울기를 구해, 기울어진 방향으로 매개변수 값을 갱신하는 일을 계속 반복한다.</p> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>W &lt;- W - ( learning rate * dL / dW )

W : 가중치
L : 손실 함수 
</code></pre></div></div> <h4 id="sgd의-단점"> <a href="#sgd의-단점" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> SGD의 단점 </h4> <p><img src="optimization/sgd-1.png" /> <img src="optimization/sgd-2.png" /></p> <p>심하게 굽이진 움직임을 보여준다. 따라서 이러한 경우에는 조금 비효율 적이다.</p> <h2 id="모멘텀momentum"> <a href="#모멘텀momentum" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> 모멘텀(Momentum) </h2> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>v &lt;- av - ( learning rate * dL / dW )
W &lt;- W + v

W : 가중치
L : 손실함수
</code></pre></div></div> <p>SGD와 차이점을 보면 av 값을 더해준게 눈에 띈다. 여기서 a는 고정된 상수값이고(ex 0.9) v는 물체의 속도라고 생각하면 된다. 해당 방향으로 진행할 수록 공이 기울기를 따라 구르듯 힘을 받는다.</p> <p><img src="optimization/momentum-1.png" width="350" /></p> <p>모멘텀의 갱신 경로는 공이 그릇 바닥을 구르듯 움직인다. SGD와 비교했을 때 지그재그 정도가 덜한 것을 알 수 있다.</p> <p><img src="optimization/momentum-2.gif" width="350" /></p> <p>또한 Momentum 방식을 이용할 경우 위의 그림과 같이 local minima를 빠져나오는 효과가 있을 것이라고도 기대할 수 있다. 기존의 SGD를 이용할 경우 좌측의 local minima에 빠지면 gradient가 0이 되어 이동할 수가 없지만, momentum 방식의 경우 기존에 이동했던 방향에 관성이 있어 이 local minima를 빠져나오고 더 좋은 minima로 이동할 것을 기대할 수 있게 된다.</p> <h2 id="adagrad"> <a href="#adagrad" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> AdaGrad </h2> <p>학습에서는 Learning Rate가 중요하다. 이 값이 너무 작으면 학습 시간이 너무 길어지고, 반대로 너무 크면 발산하여 학습이 제대로 이뤄지지 않는다.</p> <p>이러한 학습률을 정하는 효과적 기술로 학습률 감소(learning rate decay)가 있다. 이는 학습을 진행하면서 학습률을 점차 줄여나가는 방법이다.</p> <p>학습률을 서서히 낮추는 가장 간단한 방법은 전체 학습률 값을 일괄적으로 낮추는 것이지만, 이를 더 발전시킨 것이 AdaGrad이다. AdaGrad는 ‘각각의’ 매개변수에 ‘맞춤형’값을 만들어준다.</p> <p>AdaGrad는 개별 매개변수에 각각 다른 학습률을 적용해준다. <img src="optimization/adaGrad.png" width="350" /></p> <p>이 수식을 해석하면 매개변수의 원수 중에서 크게 갱신된 원소는 학습률이 낮아진다.</p> <p><img src="optimization/adaGrad-1.png" width="350" /></p> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RMSProp
AdaGrad는 과거의 기울기를 제곱하여 계속 더해간다. 그래서 학습을 진행할수록 갱신 강도가 
약해지고 계속해서 학습하면 어느 순간 갱신량이 0이 되어 전혀 갱신되지가 않는다. 
이를 개선한 기법이 RMSProp이다. RMSProp은 과거의 모든 기울기를 균일하게 
더해가는 것이 아니라, 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영한다.
</code></pre></div></div> <h2 id="adam"> <a href="#adam" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Adam </h2> <p>모멘텀 + AdaGrad 정확히 파고들면 다르지만, 직관적으로 해석하면 모멘텀과 AdaGrad를 융합한 듯한 방법이다.</p> <p><img src="optimization/adam.png" width="350" /></p> <h2 id="optimizer-한눈으로-보기"> <a href="#optimizer-한눈으로-보기" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Optimizer 한눈으로 보기 </h2> <p><img src="optimization/optimizer.jpg" width="550" /><br /> <em>출처: 하용호, 자습해도 모르겠던 딥러닝, 머리속에 인스톨 시켜드립니다(https://www.slideshare.net/yongho/ss-79607172)</em></p> <h2 id="어느-갱신-방법을-이용할-것인가"> <a href="#어느-갱신-방법을-이용할-것인가" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> 어느 갱신 방법을 이용할 것인가? </h2> <p>상황마다 다르다. 풀어야 할 문제가 무엇이냐에 따라 달라지고, 하이퍼파라미터를 어떻게 설정하느냐에 따라서 적절한 optimizer가 다르다.</p> --> <h1 id="최적화-함수들optimization">최적화 함수들(optimization)</h1> <p>신경망 학습의 목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것이며, 이러한 문제를 푸는 것을 최적화(optimization)이라고 한다.</p> <h2 id="확률적-경사-하강법---sgdstochastic-gradient-descent">확률적 경사 하강법 - SGD(Stochastic Gradient Descent)</h2> <p>최적의 매개변수 값을 찾는 단서로 매개변수의 기울기(미분)을 이용. 매개변수의 기울기를 구해, 기울어진 방향으로 매개변수 값을 갱신하는 일을 계속 반복한다.</p> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>W &lt;- W - ( learning rate * dL / dW )

W : 가중치
L : 손실 함수 
</code></pre></div></div> <h4 id="sgd의-단점">SGD의 단점</h4> <p><img src="optimization/sgd-1.png" /> <img src="optimization/sgd-2.png" /></p> <p>심하게 굽이진 움직임을 보여준다. 따라서 이러한 경우에는 조금 비효율 적이다.</p> <h2 id="모멘텀momentum">모멘텀(Momentum)</h2> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>v &lt;- av - ( learning rate * dL / dW )
W &lt;- W + v

W : 가중치
L : 손실함수
</code></pre></div></div> <p>SGD와 차이점을 보면 av 값을 더해준게 눈에 띈다. 여기서 a는 고정된 상수값이고(ex 0.9) v는 물체의 속도라고 생각하면 된다. 해당 방향으로 진행할 수록 공이 기울기를 따라 구르듯 힘을 받는다.</p> <p><img src="optimization/momentum-1.png" width="350" /></p> <p>모멘텀의 갱신 경로는 공이 그릇 바닥을 구르듯 움직인다. SGD와 비교했을 때 지그재그 정도가 덜한 것을 알 수 있다.</p> <p><img src="optimization/momentum-2.gif" width="350" /></p> <p>또한 Momentum 방식을 이용할 경우 위의 그림과 같이 local minima를 빠져나오는 효과가 있을 것이라고도 기대할 수 있다. 기존의 SGD를 이용할 경우 좌측의 local minima에 빠지면 gradient가 0이 되어 이동할 수가 없지만, momentum 방식의 경우 기존에 이동했던 방향에 관성이 있어 이 local minima를 빠져나오고 더 좋은 minima로 이동할 것을 기대할 수 있게 된다.</p> <h2 id="adagrad">AdaGrad</h2> <p>학습에서는 Learning Rate가 중요하다. 이 값이 너무 작으면 학습 시간이 너무 길어지고, 반대로 너무 크면 발산하여 학습이 제대로 이뤄지지 않는다.</p> <p>이러한 학습률을 정하는 효과적 기술로 학습률 감소(learning rate decay)가 있다. 이는 학습을 진행하면서 학습률을 점차 줄여나가는 방법이다.</p> <p>학습률을 서서히 낮추는 가장 간단한 방법은 전체 학습률 값을 일괄적으로 낮추는 것이지만, 이를 더 발전시킨 것이 AdaGrad이다. AdaGrad는 ‘각각의’ 매개변수에 ‘맞춤형’값을 만들어준다.</p> <p>AdaGrad는 개별 매개변수에 각각 다른 학습률을 적용해준다. <img src="optimization/adaGrad.png" width="350" /></p> <p>이 수식을 해석하면 매개변수의 원수 중에서 크게 갱신된 원소는 학습률이 낮아진다.</p> <p><img src="optimization/adaGrad-1.png" width="350" /></p> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RMSProp
AdaGrad는 과거의 기울기를 제곱하여 계속 더해간다. 그래서 학습을 진행할수록 갱신 강도가 
약해지고 계속해서 학습하면 어느 순간 갱신량이 0이 되어 전혀 갱신되지가 않는다. 
이를 개선한 기법이 RMSProp이다. RMSProp은 과거의 모든 기울기를 균일하게 
더해가는 것이 아니라, 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영한다.
</code></pre></div></div> <h2 id="adam">Adam</h2> <p>모멘텀 + AdaGrad 정확히 파고들면 다르지만, 직관적으로 해석하면 모멘텀과 AdaGrad를 융합한 듯한 방법이다.</p> <p><img src="optimization/adam.png" width="350" /></p> <h2 id="optimizer-한눈으로-보기">Optimizer 한눈으로 보기</h2> <p><img src="optimization/optimizer.jpg" width="550" /><br /> <em>출처: 하용호, 자습해도 모르겠던 딥러닝, 머리속에 인스톨 시켜드립니다(https://www.slideshare.net/yongho/ss-79607172)</em></p> <h2 id="어느-갱신-방법을-이용할-것인가">어느 갱신 방법을 이용할 것인가?</h2> <p>상황마다 다르다. 풀어야 할 문제가 무엇이냐에 따라 달라지고, 하이퍼파라미터를 어떻게 설정하느냐에 따라서 적절한 optimizer가 다르다.</p> </div> </div> </div> </div> </body> </html>
